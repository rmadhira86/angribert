{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  7 06:41:32 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   21C    P0    36W / 300W |     11MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.8.0+cu111)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.9.0+cu111)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.8.0+cu111 in /usr/local/lib/python3.6/dist-packages (1.8.0+cu111)\n",
      "Requirement already satisfied: torchvision==0.9.0+cu111 in /usr/local/lib/python3.6/dist-packages (0.9.0+cu111)\n",
      "Requirement already satisfied: torchaudio==0.8.0 in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.8.0+cu111) (0.8)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.8.0+cu111) (1.17.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.8.0+cu111) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.9.0+cu111) (7.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.48.2)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (1.17.41)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.24.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2021.3.17)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.95)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (0.0.43)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.3.6)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.41 in /usr/local/lib/python3.6/dist-packages (from boto3) (1.20.41)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.16.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.21.0,>=1.20.41->boto3) (2.8.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers\n",
    "!pip3 install -q ipywidgets --user\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import pprint\n",
    "# Importing stock ml libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch import cuda\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, BertConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MM - TODO: Uncomment for Manohar's run\n",
    "# INPUT_PATH = '/kaggle/input/emo-ekman'\n",
    "# INPUT_MODELS = '/kaggle/input/emo-semfr/models'\n",
    "# MODEL_PATH = '/kaggle/working/models'\n",
    "# OUTPUT_PATH = '/kaggle/working/results'\n",
    "#DK - TODO: Uncomment for Devesh's run\n",
    "INPUT_PATH = 'Data/'\n",
    "INPUT_MODELS = './models'\n",
    "MODEL_PATH = './models'\n",
    "OUTPUT_PATH = './training_results/clean_runs'\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.mkdir(MODEL_PATH) \n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.mkdir(OUTPUT_PATH) \n",
    "\n",
    "#TODO - Change pretrained model\n",
    "PRE_TRAINED_MODEL = 'distilbert-base-cased'\n",
    "device = torch.device('cuda' if cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivation from https://github.com/DhavalTaunk08/NLP_scripts/blob/master/Transformers_multilabel_distilbert.ipynb  \n",
    "https://mccormickml.com/2019/07/22/BERT-fine-tuning/#32-required-formatting  \n",
    "https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209  \n",
    "\n",
    "https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb#scrollTo=DegHNyIEQxB2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def format_time(seconds):\n",
    "    return str(datetime.timedelta(seconds=seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file,sep='\\t'):\n",
    "    '''Load a given file, combine the labels into 1 field called labels\n",
    "       and return a dataframe\n",
    "       Args: file contains a sep delimited data. Expected format of the file is \n",
    "             Text followed by One Hot Encoded list of columns for each label\n",
    "       Returns: dataframe containing 2 columns\n",
    "                First column of Text \n",
    "                Second column - list of all labels \n",
    "    '''\n",
    "\n",
    "    df = pd.read_csv(file,sep=sep)\n",
    "    df['labels'] = df[df.columns[1:]].values.tolist()\n",
    "    df = df[[df.columns[0],'labels']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpusdata(path,corpus,samples=None,type='train', ext='tsv',sep='\\t'):\n",
    "    ''' Loads data for multiple corpus, samples from each corpus would be loaded \n",
    "        and returned as one dataframe.\n",
    "        Files are expected to be in path + '/' + corpus + '_' + type + '.' + sep\n",
    "        Args: path : starting path for all corpus files\n",
    "              corpus : list of corpus to load data from\n",
    "              samples : Either None or list of sizes to select from each corpus\n",
    "                        If list specified, len should be same as length of corpus\n",
    "                        If None - all records from corpus files are included\n",
    "              type : Expected use is for training data from multiple corpus to be combined\n",
    "              ext  : Extension\n",
    "        Returns: Dataframe from all the corpus in samples specified      \n",
    "    '''\n",
    "    df_list = []\n",
    "    if not samples:\n",
    "        samples = [1000000] * len(corpus)\n",
    "    for f,s in zip(corpus,samples):\n",
    "        df = load_file(path +'/' + f + '_' + type + '.' + ext,sep)\n",
    "        if s < df.shape[0]:\n",
    "            df = df.sample(s)\n",
    "        df_list.append(df)\n",
    "    df_all = pd.concat(df_list).reset_index(drop=True)\n",
    "    colnames = {k:k.lower() for k in df_all.columns}\n",
    "    df_all.rename(columns=colnames,inplace=True)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    '''Subclass f torch.util.data.Dataset abstract class to provide access to our data'''\n",
    "    def __init__(self, dataframe, tokenizer, max_length=64):\n",
    "        '''\n",
    "            Args: dataframe - Dataframe containing 2 columns. First column should contain text\n",
    "                              and second column should contain list of 1 hot-encoded labels\n",
    "                  tokenizer - Tokenizer to use. Tokenizer must correspond to the BERT model used\n",
    "        '''\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = self.data.iloc[:,0]\n",
    "        self.targets = self.data.iloc[:,1]\n",
    "        self.max_length = max_length\n",
    "        self.labels_len = self.targets.str.len().max()\n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ''' '''\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split()) # Force convert different whitespaces into a single space\n",
    "\n",
    "        # encode_plus referred to in most documents on web is now deprecated. \n",
    "        # Using __call__ instead\n",
    "        inputs = self.tokenizer(\n",
    "            text=text,\n",
    "            text_pair=None,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length', \n",
    "            # While Max_Length is known for BERT depending on Model, \n",
    "            # total length impacts training time\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        # Inputs contains three keys\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']  \n",
    "        token_ids = inputs['token_type_ids']\n",
    "\n",
    "        return {\n",
    "            'ids'            : torch.tensor(ids,dtype=torch.long),\n",
    "            'mask'           : torch.tensor(mask,dtype=torch.long),\n",
    "            'token_type_ids' : torch.tensor(token_ids,dtype=torch.long),\n",
    "            'targets'        : torch.tensor(self.targets[index],dtype=torch.float)\n",
    "        } \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionBERTModel(torch.nn.Module):\n",
    "    def __init__(self,classes,model,drop_out=False):\n",
    "        super(EmotionBERTModel,self).__init__()\n",
    "        self.model = model \n",
    "        self.hidden_size = model.config.hidden_size\n",
    "        self.classifier_pre = torch.nn.Linear(self.hidden_size,self.hidden_size)\n",
    "        if drop_out:\n",
    "            self.dropout = torch.nn.Dropout(drop_out)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "        self.classifier = torch.nn.Linear(self.hidden_size,classes)\n",
    "\n",
    "    def forward(self,input_ids,attention_mask,token_type_ids):\n",
    "        output_1 = self.model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.classifier_pre(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer():\n",
    "    return DistilBertTokenizer.from_pretrained(PRE_TRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basemodel(labels_len,drop_out):\n",
    "    model = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL)\n",
    "    train_model = EmotionBERTModel(labels_len,model,drop_out=drop_out)\n",
    "    return train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader,optimizer,model,epoch):\n",
    "    '''Train the data for each epoch'''\n",
    "    epoch_start = time.time()\n",
    "    #Reset total loss\n",
    "    total_train_loss = 0\n",
    "\n",
    "    #Set the model to train mode. \n",
    "    # This only changes the mode, so dropout and batchnorm perform correctlt \n",
    "    model.train()\n",
    " #   device = get_device()\n",
    "    for step,data in enumerate(loader):\n",
    "        # Progress update every 40 batches.\n",
    "        #Unpack the data\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device)\n",
    "        \n",
    "\n",
    "        # Zero out the previously calculated gradients before passing backward pass \n",
    "        model.zero_grad()\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "#        optimizer.zero_grad()\n",
    "        # Calculated the loss for this batch\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. \n",
    "        # `loss` is a Tensor containing a single value; \n",
    "        #  the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Report progress every 100 batches\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            print('Epoch {:>2}  Batch {:>5,}  of  {:>5,}.  Loss {:0.4f}  Elapsed: {:}.'.format(\n",
    "                 epoch,\n",
    "                 step, \n",
    "                 len(loader),\n",
    "                 loss.item(),\n",
    "                 format_time(time.time() - epoch_start)))        \n",
    "\n",
    "#        optimizer.zero_grad()\n",
    "        #Perform backward pass to calculate gradients\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(loader) \n",
    "    elapsed_time = time.time() - epoch_start\n",
    "    print (\"Avg Training Loss {:0.4f}, Completed in {:} \".format(\n",
    "        avg_train_loss,\n",
    "        format_time(elapsed_time)\n",
    "          ))\n",
    "    return model,avg_train_loss,elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_output(loader,model):\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "#    device = get_device()\n",
    "    t0 = time.time()\n",
    "    total_val_loss = 0\n",
    "    # Put the model in evaluation mode. Dropout layers behave differently in evaluation mode\n",
    "    model.eval()\n",
    "    # Tell pytorch not to construct the compute graph during the forward pass, \n",
    "    # since this is only needed for backprop (training).\n",
    "    with torch.no_grad():\n",
    "        for step, data in enumerate(loader):\n",
    "            #Unpack the data\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "            # Perform forward pass and predict output\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            # Calculated the loss for this batch\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            # Accumulate the validation loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. \n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                print('  Batch {:>5,}  of  {:>5,}.  Loss {:0.4f}  Elapsed: {:}.'.format(\n",
    "                    step, \n",
    "                    len(loader),\n",
    "                    loss.item(),\n",
    "                    format_time(time.time() - t0)))        \n",
    "\n",
    "    avg_val_loss = total_val_loss / len(loader)\n",
    "    elapsed_time = time.time() - t0\n",
    "    print (\"Avg Validation Loss {:0.4f}, Completed in {:} \".format(\n",
    "        avg_val_loss,\n",
    "        format_time(elapsed_time)\n",
    "          ))\n",
    "    return fin_outputs, fin_targets, avg_val_loss,elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the score on the val set:\n",
    "\n",
    "def generate_score(loader,model,batch_size=1):\n",
    "    results = {\"weighted\":0,\"micro\":0,\"macro\":0,\"samples\":0}\n",
    "    \n",
    "    \n",
    "    print(\"**** Loss ******\")\n",
    "    outputs, targets,val_loss,val_time = calc_output(loader,model)\n",
    "    outputs = (np.array(outputs) >= 0.5).astype(float)\n",
    "    print(\"**** Results ******\")\n",
    "    for k in results.keys():\n",
    "        score = metrics.f1_score(targets, outputs,average = k) \n",
    "        print(\"F1 Score ({}) = {:10.6f}\".format(k,score))\n",
    "        results[k] = score\n",
    "    results['outputs'] = outputs.tolist()\n",
    "    results['targets'] = targets\n",
    "    results['val_loss'] = val_loss\n",
    "    results['val_time'] = val_time\n",
    "    return results\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datareaders(train_data,batch_size=16,max_length=64):\n",
    "    tokenizer = get_tokenizer()\n",
    "    \n",
    "    dataset = EmotionDataset(train_data,tokenizer,max_length=max_length)\n",
    "    dl_params = {'batch_size': batch_size,'shuffle': True,'num_workers': 0 }\n",
    "    loader = DataLoader(dataset,**dl_params)  \n",
    "    return loader, dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data,val_data,trained_model:None, freeze_layers=None,\n",
    "                epochs=4,batch_size=16,drop_out=0.3,lr=1e-05,max_length=64):\n",
    "\n",
    "    train_stats = []\n",
    "    train_loader, train_dataset = get_datareaders(train_data,batch_size,max_length=max_length)\n",
    "    val_loader,val_dataset = get_datareaders(val_data,batch_size,max_length=max_length)\n",
    "\n",
    "    if trained_model is None:\n",
    "        trained_model = get_basemodel(train_dataset.labels_len,drop_out=drop_out)\n",
    "\n",
    "    if freeze_layers:\n",
    "        for name,param in trained_model.named_parameters():\n",
    "            if any([fl in name for fl in freeze_layers]):\n",
    "                param.requires_grad = False\n",
    "\n",
    "    trained_model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = trained_model.parameters(), lr=lr)\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        print ('*****  Epoch {} *****'.format(epoch))\n",
    "        print ('-----  Training -----')\n",
    "        trained_model,train_loss,train_time = train(train_loader,optimizer,trained_model,epoch)\n",
    "        print ('-----  Validation -----')\n",
    "        results = generate_score(val_loader,trained_model)\n",
    "        results['epoch'] = str(epoch)\n",
    "        results['train_loss'] = train_loss\n",
    "        results['train_time'] = train_time\n",
    "        results['train_count'] = train_data.shape[0]\n",
    "        results['val_count'] = val_data.shape[0]\n",
    "        train_stats.append(results)\n",
    "    \n",
    "        \n",
    "    return trained_model, train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_data,model,epoch,max_length=64):\n",
    "    test_loader,test_dataset = get_datareaders(test_data,batch_size=1,max_length=64)\n",
    "    print ('-----  Testing -----')\n",
    "    results = generate_score(test_loader,model)\n",
    "    results['epoch'] = epoch\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "#TODO: Certainly Change TRIAL_RUN\n",
    "# Setup models for experiments for the production run\n",
    "# To ensure that model is setup correctly\n",
    "# Set Trial Run to true. This will ensure all models to be run\n",
    "#   Trial run would run on CPU typically in less than 10 minutes\n",
    "# Then change TRIAL_RUN to False and run on GPU\n",
    "metadata = {\n",
    "    'TRIAL_RUN':False,\n",
    "    'TRIAL_SIZE': 5,\n",
    "    'TRIAL_EPOCHS': 1,\n",
    "    'run_type':'ekman_final_positive',\n",
    "    'description':'Run Ekman classification with Transfer learning from GoEmotions, SemEval and Friends datasets',\n",
    "    'model_ext':'.pt',\n",
    "    'dict_ext':'_inf.txt'\n",
    "}\n",
    "model_config = {\n",
    "    'epochs'      :   4,\n",
    "    'batch_size'  :  16,\n",
    "    'max_length'  : 512,\n",
    "    'lr'          : 4e-05,\n",
    "    'drop_out'    : 0.2,\n",
    "    'dataset_path':INPUT_PATH,\n",
    "    'input_model_path': INPUT_MODELS,\n",
    "    'model_path'  :MODEL_PATH,\n",
    "    'output_path' :OUTPUT_PATH\n",
    "}\n",
    "experiments = {\n",
    "    'GOEMO' :{\n",
    "        'run'          :False,\n",
    "        'description'  :'GoEmotions ',\n",
    "        'train_corpus' : ['goemotions'],\n",
    "        'test_corpus'  : ['tales'],\n",
    "        'save_model'   :'emo_goemo',\n",
    "        'save_stats'   :'stats_goemo'\n",
    "    },\n",
    "    'GOEMO_EMB_FR' :{\n",
    "        'run'          :False,\n",
    "        'description'  :'GoEmotions Freeze Embeddings',\n",
    "        'load_model'   :None,\n",
    "        'freeze_layers':['embeddings'],\n",
    "        'train_corpus' : ['goemotions'],\n",
    "        'train_samples': None,\n",
    "        'test_corpus'  : ['tales'],\n",
    "        'test_samples' : None,\n",
    "        'save_model'   :'emo_goemo_emb_fr',\n",
    "        'save_stats'   :'stats_goemo'\n",
    "    },    \n",
    "    'GOEMO_FR' :{\n",
    "        'run'          :False,\n",
    "        'description'  :'GoEmotions and Friends for training',\n",
    "        'train_corpus' : ['goemotions','friends'],\n",
    "        'test_corpus'  : ['tales'],\n",
    "        'save_model'   :'emo_goemo_fr',\n",
    "        'save_stats'   :'stats_goemo'\n",
    "    },\n",
    "    'GOEMO_SEM' :{\n",
    "        'run'          :False,\n",
    "        'description'  :'GoEmotions,SemEval for training',\n",
    "        'train_corpus' : ['goemotions','semeval'],\n",
    "        'test_corpus'  : ['tales'],\n",
    "        'save_model'   :'emo_goemo_sem',\n",
    "        'save_stats'   :'stats_goemo_sem'\n",
    "    },\n",
    "    'GOEMO_SEM_FR' :{\n",
    "        'run'          :False,\n",
    "        'description'  :'GoEmotions,SemEval and Friends for training',\n",
    "        'train_corpus' : ['goemotions','semeval','friends'],\n",
    "        'test_corpus'  : ['tales'],\n",
    "        'save_model'   :'emo_goemo_sem_fr',\n",
    "        'save_stats'   :'stats_goemo_sem_fr'\n",
    "    },\n",
    "    \n",
    "    'TALES_100': {\n",
    "        'run': True,\n",
    "        'description': \"TALES with ['tales'] 100 samples\",\n",
    "      'load_model': None,\n",
    "      'train_corpus': ['tales'],\n",
    "      'train_samples': [100],\n",
    "      'test_corpus': ['tales'],\n",
    "      'save_stats': 'stats_tales_100'},\n",
    "    \n",
    "    'TALES_200': {\n",
    "        'run': True,\n",
    "        'description': \"TALES with ['tales'] 200 samples\",\n",
    "      'load_model': None,\n",
    "      'train_corpus': ['tales'],\n",
    "      'train_samples': [200],\n",
    "      'test_corpus': ['tales'],\n",
    "      'save_stats': 'stats_tales_200'},\n",
    "    \n",
    "    'TALES_300': {\n",
    "        'run': True,\n",
    "        'description': \"TALES with ['tales'] 300 samples\",\n",
    "      'load_model': None,\n",
    "      'train_corpus': ['tales'],\n",
    "      'train_samples': [300],\n",
    "      'test_corpus': ['tales'],\n",
    "      'save_stats': 'stats_tales_300'},\n",
    "    \n",
    "     'TALES_400': {\n",
    "        'run': True,\n",
    "        'description': \"TALES with ['tales'] 400 samples\",\n",
    "      'load_model': None,\n",
    "      'train_corpus': ['tales'],\n",
    "      'train_samples': [400],\n",
    "      'test_corpus': ['tales'],\n",
    "      'save_stats': 'stats_tales_400'},\n",
    "            \n",
    "    'TARGET_500': {\n",
    "        'run': True,\n",
    "        'description': \"TARGET with ['tales'] 500 samples\",\n",
    "      'load_model': None,\n",
    "      'train_corpus': ['tales'],\n",
    "      'train_samples': [500],\n",
    "      'test_corpus': ['tales'],\n",
    "      'save_stats': 'stats_target_500'},\n",
    "     \n",
    "    'TALES_600': {\n",
    "        'run': True,\n",
    "        'description': \"TALES with ['tales'] 600 samples\",\n",
    "      'load_model': None,\n",
    "      'train_corpus': ['tales'],\n",
    "      'train_samples': [600],\n",
    "      'test_corpus': ['tales'],\n",
    "      'save_stats': 'stats_tales_600'},\n",
    "    \n",
    "     'TALES_700': {\n",
    "        'run': True,\n",
    "        'description': \"TALES with ['tales'] 700 samples\",\n",
    "      'load_model': None,\n",
    "      'train_corpus': ['tales'],\n",
    "      'train_samples': [700],\n",
    "      'test_corpus': ['tales'],\n",
    "      'save_stats': 'stats_tales_700'},\n",
    "    \n",
    "    'TALES_800': {\n",
    "        'run': True,\n",
    "        'description': \"TALES with ['tales'] 800 samples\",\n",
    "      'load_model': None,\n",
    "      'train_corpus': ['tales'],\n",
    "      'train_samples': [800],\n",
    "      'test_corpus': ['tales'],\n",
    "      'save_stats': 'stats_tales_800'},\n",
    "    \n",
    "    'TALES_900': {\n",
    "        'run': True,\n",
    "        'description': \"TALES with ['tales'] 900 samples\",\n",
    "      'load_model': None,\n",
    "      'train_corpus': ['tales'],\n",
    "      'train_samples': [900],\n",
    "      'test_corpus': ['tales'],\n",
    "      'save_stats': 'stats_tales_900'},\n",
    "    \n",
    " 'TALES_1000': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 1000 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_1000'},\n",
    "    \n",
    " 'TALES_1100': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 1100 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1100],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_1100'},\n",
    " \n",
    "    'TALES_1200': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 1200 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1200],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_1200'},\n",
    "    \n",
    "    'TALES_1300': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 1300 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1300],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_1300'},\n",
    "    \n",
    "    'TALES_1400': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 1400 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1400],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_1400'},\n",
    "    \n",
    "    \n",
    " 'TALES_1500': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 1500 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_1500'},\n",
    "    \n",
    "    \n",
    "    'TALES_1600': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 1600 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1600],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_1600'},\n",
    "    \n",
    "      'TALES_1700': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 1700 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1700],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_1700'},\n",
    "    \n",
    "    'TALES_1800': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 1800 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1800],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_1800'},\n",
    "    \n",
    "    'TALES_1900': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 1900 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1900],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_1900'},\n",
    "    \n",
    "    \n",
    " 'TALES_2000': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 2000 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_2000'},\n",
    "    \n",
    "    'TALES_2100': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 2100 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2100],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_2100'},\n",
    "    \n",
    "    \n",
    "    'TALES_2200': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 2200 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2200],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_2200'},\n",
    "    \n",
    "    'TALES_2300': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 2300 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2300],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_2300'},\n",
    "    \n",
    "    'TALES_2400': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 2400 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2400],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_2400'},\n",
    "    \n",
    "    'TALES_2500': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 2500 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_2500'},\n",
    "    \n",
    "    'TALES_2600': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 2600 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2600],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_2600'},\n",
    "    \n",
    "    'TALES_2700': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 2700 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2700],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_2700'},\n",
    "    \n",
    "    'TALES_2800': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 2800 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2800],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_2800'},\n",
    "    \n",
    "      'TALES_2900': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 2900 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2900],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_2900'},\n",
    "       \n",
    " 'TALES_3000': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 3000 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_3000'},\n",
    "    \n",
    "    'TALES_3100': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 3100 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3100],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_3100'},\n",
    "    \n",
    "    'TALES_3200': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 3200 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3200],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_3200'},\n",
    "    \n",
    "    'TALES_3300': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 3300 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3300],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_3300'},\n",
    "    \n",
    "     'TALES_3400': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 3400 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3400],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_3400'},\n",
    "    \n",
    "       'TALES_3500': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 3400 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_3500'},\n",
    "    \n",
    "     'TALES_3600': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 3600 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3600],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_3600'},\n",
    "    \n",
    "     'TALES_3700': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 3700 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3700],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_3700'},\n",
    "    \n",
    "     'TALES_3800': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 3800 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3800],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_3800'},\n",
    "    \n",
    "       'TALES_3900': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 3900 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3900],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_3900'},\n",
    "    \n",
    "    'TALES_4000': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Target with ['tales'] 4000 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [4000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_4000'},\n",
    "    \n",
    " 'TALES_5000': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Tales with ['tales'] 5000 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [5000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_5000'},\n",
    " 'TALES_8000': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Tales with ['tales'] 8000 samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [8000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_8000'},\n",
    " 'TALES_ALL': {'run': True,\n",
    "  'name' : 'TALES',                 \n",
    "  'description': \"Tales with ['tales'] ALL samples\",\n",
    "  'load_model': None,\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': None,\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_tales_all'},\n",
    "    \n",
    "    'GEMB_FR_3_5_100': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 100 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [100],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_100'},\n",
    "    \n",
    "      'GEMB_FR_3_5_200': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 200 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [200],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_200'},\n",
    "    \n",
    "    'GEMB_FR_3_5_300': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 300 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [300],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_300'},\n",
    "    \n",
    "    'GEMB_FR_3_5_400': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 400 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [400],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_400'},\n",
    "    \n",
    "    \n",
    " 'GEMB_FR_3_5_500': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 500 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_500'},\n",
    " \n",
    "    'GEMB_FR_3_5_600': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 600 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [600],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_600'},\n",
    "    \n",
    "    'GEMB_FR_3_5_700': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 700 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [700],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_700'},\n",
    "    \n",
    "      'GEMB_FR_3_5_800': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 800 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [800],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_800'},\n",
    "    \n",
    "     'GEMB_FR_3_5_900': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 900 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [900],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_900'},\n",
    " \n",
    "   'GEMB_FR_3_5_1000': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 1000 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_1000'},\n",
    "    \n",
    "    'GEMB_FR_3_5_1100': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 1100 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1100],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_1100'},\n",
    "    \n",
    "     'GEMB_FR_3_5_1200': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 1200 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1200],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_1200'},\n",
    "    \n",
    "    'GEMB_FR_3_5_1300': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 1300 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1300],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_1300'},\n",
    "    \n",
    "      'GEMB_FR_3_5_1400': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 1400 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1400],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_1400'},\n",
    "    \n",
    " 'GEMB_FR_3_5_1500': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 1500 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_1500'},\n",
    "    \n",
    "    'GEMB_FR_3_5_1600': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 1600 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1600],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_1600'},\n",
    "     \n",
    "    'GEMB_FR_3_5_1700': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 1700 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1700],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_1700'},\n",
    "    \n",
    "     'GEMB_FR_3_5_1800': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 1800 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1800],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_1800'},\n",
    "    \n",
    "    'GEMB_FR_3_5_1900': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 1900 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1900],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_1900'},\n",
    "    \n",
    " 'GEMB_FR_3_5_2000': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 2000 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_2000'},\n",
    "    \n",
    "    'GEMB_FR_3_5_2100': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 2100 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2100],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_2100'},\n",
    "    \n",
    "    'GEMB_FR_3_5_2200': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 2200 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2200],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_2200'},\n",
    "    \n",
    "    'GEMB_FR_3_5_2300': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 2300 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2300],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_2300'},\n",
    "    \n",
    "    'GEMB_FR_3_5_2400': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 2400 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2400],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_2400'},\n",
    "    \n",
    "      'GEMB_FR_3_5_2500': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 2500 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_2500'},\n",
    "    \n",
    "    'GEMB_FR_3_5_2600': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 2600 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2600],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_2600'},\n",
    "    \n",
    "    'GEMB_FR_3_5_2700': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 2700 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2700],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_2700'},\n",
    "    \n",
    "    'GEMB_FR_3_5_2800': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 2800 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2800],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_2800'},\n",
    "    \n",
    "    'GEMB_FR_3_5_2900': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 2900 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2900],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_2900'},\n",
    "    \n",
    " 'GEMB_FR_3_5_3000': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 3000 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_3000'},\n",
    " 'GEMB_FR_3_5_3100': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 3100 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3100],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_3100'},\n",
    "     'GEMB_FR_3_5_3200': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 3200 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3200],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_3200'},\n",
    "    'GEMB_FR_3_5_3300': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 3300 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3300],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_3300'},\n",
    "      'GEMB_FR_3_5_3400': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 3300 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3400],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_3400'},\n",
    "        'GEMB_FR_3_5_3500': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 3500 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_3500'},\n",
    "    'GEMB_FR_3_5_3600': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 3600 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3600],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_3600'},\n",
    "       'GEMB_FR_3_5_3700': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 3700 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3700],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_3700'},\n",
    "        'GEMB_FR_3_5_3800': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 3800 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3800],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_3800'},\n",
    "        'GEMB_FR_3_5_3900': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 3900 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3900],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_3900'},\n",
    "     'GEMB_FR_3_5_4000': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 4000 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [4000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_4000'},\n",
    "    \n",
    " 'GEMB_FR_3_5_5000': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 5000 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [5000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_5000'},\n",
    " 'GEMB_FR_3_5_8000': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] 8000 samplesand transfer learning from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [8000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_8000'},\n",
    " 'GEMB_FR_3_5_ALL': {'run': True,\n",
    "  'name' : 'GEMB_FR_3_5',                 \n",
    "  'description': \"GoEmo with embeddings frozen followed by Friends with layers 3 to 5 frozen with ['tales'] ALL samples and xfer learn from goemb_frall_tr3_5\",\n",
    "  'load_model': 'goemb_frall_tr3_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': None,\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_fr_3_5_all'},\n",
    " \n",
    "    'GO_SEM_FR_100': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 100 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [100],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_100'},\n",
    "    \n",
    "      'GO_SEM_FR_200': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 200 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [200],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_200'},\n",
    "    'GO_SEM_FR_300': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 300 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [300],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_300'},\n",
    "    'GO_SEM_FR_400': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 400 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [400],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_400'},\n",
    "        \n",
    "    'GO_SEM_FR_500': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 500 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_500'},\n",
    "    \n",
    "    'GO_SEM_FR_600': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 600 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [600],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_600'},\n",
    "    'GO_SEM_FR_700': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 700 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [700],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_700'},\n",
    "    'GO_SEM_FR_800': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 800 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [800],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_800'},\n",
    "    'GO_SEM_FR_900': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 900 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [900],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_900'},\n",
    "    \n",
    " 'GO_SEM_FR_1000': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 1000 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_1000'},\n",
    "    \n",
    "  'GO_SEM_FR_1100': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 1100 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1100],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_1100'}, \n",
    "    'GO_SEM_FR_1200': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 1200 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1200],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_1200'}, \n",
    "    'GO_SEM_FR_1300': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 1300 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1300],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_1300'}, \n",
    "     'GO_SEM_FR_1400': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 1400 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1400],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_1400'}, \n",
    " 'GO_SEM_FR_1500': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 1500 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_1500'},\n",
    "    \n",
    "'GO_SEM_FR_1600': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 1600 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1600],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_1600'},\n",
    "    \n",
    "'GO_SEM_FR_1700': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 1600 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1700],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_1700'},\n",
    "    \n",
    "    'GO_SEM_FR_1800': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 1800 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1800],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_1800'},\n",
    "    'GO_SEM_FR_1900': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 1900 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1900],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_1900'},\n",
    "    'GO_SEM_FR_2000': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 2000 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_2000'},\n",
    "    \n",
    "    'GO_SEM_FR_2100': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 2100 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2100],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_2100'},\n",
    "    \n",
    "      'GO_SEM_FR_2200': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 2200 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2200],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_2200'},\n",
    "      'GO_SEM_FR_2300': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 2300 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2300],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_2300'},\n",
    "    'GO_SEM_FR_2400': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 2500 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2400],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_2400'},\n",
    "    \n",
    "    'GO_SEM_FR_2500': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 2500 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_2500'},\n",
    "    'GO_SEM_FR_2600': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 2600 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2600],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_2600'},\n",
    "    \n",
    "    'GO_SEM_FR_2700': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 2700 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2700],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_2700'},\n",
    "    \n",
    "    'GO_SEM_FR_2800': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 2800 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2800],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_2800'},\n",
    "    \n",
    "    'GO_SEM_FR_2900': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 2900 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2900],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_2900'},\n",
    "    \n",
    " 'GO_SEM_FR_3000': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 3000 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_3000'},\n",
    "    \n",
    "    'GO_SEM_FR_3100': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 3100 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3100],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_3100'},\n",
    "     'GO_SEM_FR_3200': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 3200 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3200],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_3200'},\n",
    "    'GO_SEM_FR_3300': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 3300 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3300],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_3300'},\n",
    "      'GO_SEM_FR_3400': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 3400 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3400],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_3400'},\n",
    "       'GO_SEM_FR_3500': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 3500 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_3500'},\n",
    "       'GO_SEM_FR_3600': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 3600 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3600],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_3600'},\n",
    "      'GO_SEM_FR_3700': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 3700 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3700],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_3700'},\n",
    "       'GO_SEM_FR_3800': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 3800 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3800],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_3800'},\n",
    "      'GO_SEM_FR_3900': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 3900 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3900],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_3800'},\n",
    "     'GO_SEM_FR_4000': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 4000 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [4000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_4000'},\n",
    "    \n",
    "    \n",
    " 'GO_SEM_FR_5000': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 5000 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [5000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_5000'},\n",
    " 'GO_SEM_FR_8000': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] 8000 samplesand transfer learning from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [8000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_8000'},\n",
    " 'GO_SEM_FR_ALL': {'run': True,\n",
    "  'name' : 'GO_SEM_FR',                 \n",
    "  'description': \"GoEmo, SemEval and Friends  with ['tales'] ALL samples and xfer learn from emo_goemo_sem_fr\",\n",
    "  'load_model': 'emo_goemo_sem_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': None,\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_fr_all'},\n",
    " \n",
    "    'GO_FR_100': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 100 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [100],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_100'},\n",
    "     'GO_FR_200': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 200 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [200],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_200'},\n",
    "         'GO_FR_300': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 300 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [300],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_300'},\n",
    "    'GO_FR_400': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 400 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [400],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_400'},\n",
    "    \n",
    "    'GO_FR_500': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 500 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_500'},\n",
    "    \n",
    "    'GO_FR_600': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 600 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [600],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_600'},\n",
    "    'GO_FR_700': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 700 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [700],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_700'},\n",
    "     'GO_FR_800': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 800 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [800],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_800'},\n",
    "     'GO_FR_900': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 900 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [900],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_900'},\n",
    "        \n",
    " 'GO_FR_1000': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 1000 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_1000'},\n",
    "    \n",
    "    'GO_FR_1100': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 1100 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1100],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_1100'},\n",
    "    \n",
    "       'GO_FR_1200': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 1200 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1200],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_1200'},\n",
    "    \n",
    "      'GO_FR_1300': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 1300 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1300],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_1300'},\n",
    "    \n",
    "    'GO_FR_1400': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 1400 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1400],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_1400'},\n",
    "    \n",
    "     'GO_FR_1500': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 1500 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_1500'},\n",
    "    \n",
    " 'GO_FR_1600': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 1600 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1600],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_1600'},\n",
    "    'GO_FR_1700': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 1700 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1700],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_1700'},\n",
    "    \n",
    "       'GO_FR_1800': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 1800 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1800],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_1800'},\n",
    "    \n",
    " 'GO_FR_1900': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 1800 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1900],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_1900'},\n",
    " 'GO_FR_2000': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 2000 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_2000'},\n",
    "    \n",
    "    'GO_FR_2100': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 2100 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2100],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_2100'},\n",
    "    \n",
    "     'GO_FR_2200': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 2200 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2200],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_2200'},\n",
    "     'GO_FR_2300': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 2300 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2300],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_2300'},\n",
    "     'GO_FR_2400': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 2400 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2400],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_2400'},\n",
    "      'GO_FR_2500': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 2500 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_2500'},\n",
    "    \n",
    "    'GO_FR_2600': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 2500 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2600],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_2600'},\n",
    "     'GO_FR_2700': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 2700 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2700],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_2700'},\n",
    "     'GO_FR_2800': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 2800 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2800],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_2800'},\n",
    "      'GO_FR_2900': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 2900 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2900],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_2900'},\n",
    "    \n",
    "    'GO_FR_3000': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 3000 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_3000'},\n",
    "     'GO_FR_3100': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 3100 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3100],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_3100'}, \n",
    "       'GO_FR_3200': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 3200 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3200],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_3200'}, \n",
    "    \n",
    "    'GO_FR_3300': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 3300 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3300],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_3300'}, \n",
    "    'GO_FR_3400': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 3400 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3400],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_3400'}, \n",
    "     'GO_FR_3500': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 3500 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_3500'},\n",
    "    'GO_FR_3600': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 3600 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3600],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_3600'},\n",
    "    'GO_FR_3700': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 3700 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3700],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_3700'},\n",
    "    \n",
    "    'GO_FR_3800': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 3700 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3800],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_3800'},\n",
    "    'GO_FR_3900': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 3900 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3900],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_3900'},\n",
    "    'GO_FR_4000': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 4000 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [4000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_4000'},\n",
    "    \n",
    " 'GO_FR_5000': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 5000 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [5000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_5000'},\n",
    " 'GO_FR_8000': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] 8000 samplesand transfer learning from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [8000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_8000'},\n",
    " 'GO_FR_ALL': {'run': True,\n",
    "  'name' : 'GO_FR',                 \n",
    "  'description': \"GoEmo, Friends  with ['tales'] ALL samples and xfer learn from emo_goemo_fr\",\n",
    "  'load_model': 'emo_goemo_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': None,\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_fr_all'},\n",
    "\n",
    "    ######COPY 2 will start from HERE########\n",
    "    'GO_SEM_500': {'run': False,\n",
    "  'name' : 'GO_SEM',                 \n",
    "  'description': \"GoEmo, SemEval  with ['tales'] 500 samplesand transfer learning from emo_goemo_sem\",\n",
    "  'load_model': 'emo_goemo_sem',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_500'},\n",
    " 'GO_SEM_1000': {'run': True,\n",
    "  'name' : 'GO_SEM',                 \n",
    "  'description': \"GoEmo, SemEval  with ['tales'] 1000 samplesand transfer learning from emo_goemo_sem\",\n",
    "  'load_model': 'emo_goemo_sem',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_1000'},\n",
    " 'GO_SEM_1500': {'run': False,\n",
    "  'name' : 'GO_SEM',                 \n",
    "  'description': \"GoEmo, SemEval  with ['tales'] 1500 samplesand transfer learning from emo_goemo_sem\",\n",
    "  'load_model': 'emo_goemo_sem',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_1500'},\n",
    " 'GO_SEM_2000': {'run': False,\n",
    "  'name' : 'GO_SEM',                 \n",
    "  'description': \"GoEmo, SemEval  with ['tales'] 2000 samplesand transfer learning from emo_goemo_sem\",\n",
    "  'load_model': 'emo_goemo_sem',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_2000'},\n",
    " 'GO_SEM_3000': {'run': False,\n",
    "  'name' : 'GO_SEM',                 \n",
    "  'description': \"GoEmo, SemEval  with ['tales'] 3000 samplesand transfer learning from emo_goemo_sem\",\n",
    "  'load_model': 'emo_goemo_sem',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_3000'},\n",
    " 'GO_SEM_5000': {'run': False,\n",
    "  'name' : 'GO_SEM',                 \n",
    "  'description': \"GoEmo, SemEval  with ['tales'] 5000 samplesand transfer learning from emo_goemo_sem\",\n",
    "  'load_model': 'emo_goemo_sem',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [5000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_5000'},\n",
    " 'GO_SEM_8000': {'run': False,\n",
    "  'name' : 'GO_SEM',                 \n",
    "  'description': \"GoEmo, SemEval  with ['tales'] 8000 samplesand transfer learning from emo_goemo_sem\",\n",
    "  'load_model': 'emo_goemo_sem',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [8000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_8000'},\n",
    " 'GO_SEM_ALL': {'run': False,\n",
    "  'name' : 'GO_SEM',                 \n",
    "  'description': \"GoEmo, SemEval  with ['tales'] ALL samples and xfer learn from emo_goemo_sem\",\n",
    "  'load_model': 'emo_goemo_sem',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': False,\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_sem_all'},\n",
    " 'GO_500': {'run': False,\n",
    "  'name' : 'GO',                 \n",
    "  'description': \"GoEmo  with ['tales'] 500 samplesand transfer learning from emo_goemo\",\n",
    "  'load_model': 'emo_goemo',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_500'},\n",
    " 'GO_1000': {'run': False,\n",
    "  'name' : 'GO',                 \n",
    "  'description': \"GoEmo  with ['tales'] 1000 samplesand transfer learning from emo_goemo\",\n",
    "  'load_model': 'emo_goemo',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_1000'},\n",
    " 'GO_1500': {'run': False,\n",
    "  'name' : 'GO',                 \n",
    "  'description': \"GoEmo  with ['tales'] 1500 samplesand transfer learning from emo_goemo\",\n",
    "  'load_model': 'emo_goemo',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_1500'},\n",
    " 'GO_2000': {'run': False,\n",
    "  'name' : 'GO',                 \n",
    "  'description': \"GoEmo  with ['tales'] 2000 samplesand transfer learning from emo_goemo\",\n",
    "  'load_model': 'emo_goemo',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_2000'},\n",
    " 'GO_3000': {'run': False,\n",
    "  'name' : 'GO',                 \n",
    "  'description': \"GoEmo  with ['tales'] 3000 samplesand transfer learning from emo_goemo\",\n",
    "  'load_model': 'emo_goemo',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_3000'},\n",
    " 'GO_5000': {'run': False,\n",
    "  'name' : 'GO',                 \n",
    "  'description': \"GoEmo  with ['tales'] 5000 samplesand transfer learning from emo_goemo\",\n",
    "  'load_model': 'emo_goemo',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [5000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_5000'},\n",
    " 'GO_8000': {'run': False,\n",
    "  'name' : 'GO',                 \n",
    "  'description': \"GoEmo  with ['tales'] 8000 samplesand transfer learning from emo_goemo\",\n",
    "  'load_model': 'emo_goemo',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [8000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_8000'},\n",
    " 'GO_ALL': {'run': False,\n",
    "  'name' : 'GO',                 \n",
    "  'description': \"GoEmo  with ['tales'] ALL samples and xfer learn from emo_goemo\",\n",
    "  'load_model': 'emo_goemo',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': None,\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_go_all'},\n",
    "'GEMB_500': {'run': True,\n",
    "  'name': 'GEMB',\n",
    "  'description': \"GoEmo with embeddings frozen with ['tales'] 500 samplesand transfer learning from emo_goemo_emb_fr\",\n",
    "  'load_model': 'emo_goemo_emb_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_500'},\n",
    " 'GEMB_1000': {'run': False,\n",
    "  'name': 'GEMB',\n",
    "  'description': \"GoEmo with embeddings frozen with ['tales'] 1000 samplesand transfer learning from emo_goemo_emb_fr\",\n",
    "  'load_model': 'emo_goemo_emb_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_1000'},\n",
    " 'GEMB_1500': {'run': False,\n",
    "  'name': 'GEMB',\n",
    "  'description': \"GoEmo with embeddings frozen with ['tales'] 1500 samplesand transfer learning from emo_goemo_emb_fr\",\n",
    "  'load_model': 'emo_goemo_emb_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_1500'},\n",
    " 'GEMB_2000': {'run': False,\n",
    "  'name': 'GEMB',\n",
    "  'description': \"GoEmo with embeddings frozen with ['tales'] 2000 samplesand transfer learning from emo_goemo_emb_fr\",\n",
    "  'load_model': 'emo_goemo_emb_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_2000'},\n",
    " 'GEMB_3000': {'run': False,\n",
    "  'name': 'GEMB',\n",
    "  'description': \"GoEmo with embeddings frozen with ['tales'] 3000 samplesand transfer learning from emo_goemo_emb_fr\",\n",
    "  'load_model': 'emo_goemo_emb_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_3000'},\n",
    " 'GEMB_5000': {'run': False,\n",
    "  'name': 'GEMB',\n",
    "  'description': \"GoEmo with embeddings frozen with ['tales'] 5000 samplesand transfer learning from emo_goemo_emb_fr\",\n",
    "  'load_model': 'emo_goemo_emb_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [5000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_5000'},\n",
    " 'GEMB_8000': {'run': False,\n",
    "  'name': 'GEMB',\n",
    "  'description': \"GoEmo with embeddings frozen with ['tales'] 8000 samplesand transfer learning from emo_goemo_emb_fr\",\n",
    "  'load_model': 'emo_goemo_emb_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [8000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_8000'},\n",
    " 'GEMB_ALL': {'run': False,\n",
    "  'name': 'GEMB',\n",
    "  'description': \"GoEmo with embeddings frozen with ['tales'] ALL samples and xfer learn from emo_goemo_emb_fr\",\n",
    "  'load_model': 'emo_goemo_emb_fr',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': False,\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_all'},\n",
    " 'GEMB_SEMFR_0_5_500': {'run': False,\n",
    "  'name': 'GEMB_SEMFR_0_5',\n",
    "  'description': \"GoEmo, SemEval, Friends with embeddings and all layers frozen with ['tales'] 500 samplesand transfer learning from goemb_semfrall_tr0_5\",\n",
    "  'load_model': 'goemb_semfrall_tr0_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_semfr_0_5_500'},\n",
    " 'GEMB_SEMFR_0_5_1000': {'run': False,\n",
    "  'name': 'GEMB_SEMFR_0_5',\n",
    "  'description': \"GoEmo, SemEval, Friends with embeddings and all layers frozen with ['tales'] 1000 samplesand transfer learning from goemb_semfrall_tr0_5\",\n",
    "  'load_model': 'goemb_semfrall_tr0_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_semfr_0_5_1000'},\n",
    " 'GEMB_SEMFR_0_5_1500': {'run': False,\n",
    "  'name': 'GEMB_SEMFR_0_5',\n",
    "  'description': \"GoEmo, SemEval, Friends with embeddings and all layers frozen with ['tales'] 1500 samplesand transfer learning from goemb_semfrall_tr0_5\",\n",
    "  'load_model': 'goemb_semfrall_tr0_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [1500],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_semfr_0_5_1500'},\n",
    " 'GEMB_SEMFR_0_5_2000': {'run': False,\n",
    "  'name': 'GEMB_SEMFR_0_5',\n",
    "  'description': \"GoEmo, SemEval, Friends with embeddings and all layers frozen with ['tales'] 2000 samplesand transfer learning from goemb_semfrall_tr0_5\",\n",
    "  'load_model': 'goemb_semfrall_tr0_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [2000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_semfr_0_5_2000'},\n",
    " 'GEMB_SEMFR_0_5_3000': {'run': False,\n",
    "  'name': 'GEMB_SEMFR_0_5',\n",
    "  'description': \"GoEmo, SemEval, Friends with embeddings and all layers frozen with ['tales'] 3000 samplesand transfer learning from goemb_semfrall_tr0_5\",\n",
    "  'load_model': 'goemb_semfrall_tr0_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [3000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_semfr_0_5_3000'},\n",
    " 'GEMB_SEMFR_0_5_5000': {'run': False,\n",
    "  'name': 'GEMB_SEMFR_0_5',\n",
    "  'description': \"GoEmo, SemEval, Friends with embeddings and all layers frozen with ['tales'] 5000 samplesand transfer learning from goemb_semfrall_tr0_5\",\n",
    "  'load_model': 'goemb_semfrall_tr0_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [5000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_semfr_0_5_5000'},\n",
    " 'GEMB_SEMFR_0_5_8000': {'run': False,\n",
    "  'name': 'GEMB_SEMFR_0_5',\n",
    "  'description': \"GoEmo, SemEval, Friends with embeddings and all layers frozen with ['tales'] 8000 samplesand transfer learning from goemb_semfrall_tr0_5\",\n",
    "  'load_model': 'goemb_semfrall_tr0_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': [8000],\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_semfr_0_5_8000'},\n",
    " 'GEMB_SEMFR_0_5_ALL': {'run': False,\n",
    "  'name': 'GEMB_SEMFR_0_5',\n",
    "  'description': \"GoEmo, SemEval, Friends with embeddings and all layers frozen with ['tales'] ALL samples and xfer learn from goemb_semfrall_tr0_5\",\n",
    "  'load_model': 'goemb_semfrall_tr0_5',\n",
    "  'train_corpus': ['tales'],\n",
    "  'train_samples': None,\n",
    "  'test_corpus': ['tales'],\n",
    "  'save_stats': 'stats_gemb_semfr_0_5_all'}    \n",
    "}        \n",
    "\n",
    "run_exp = {k:v for k,v in experiments.items() if v['run']}\n",
    "for i,k in enumerate(run_exp.keys()):\n",
    "    run_exp[k]['run_step'] = i\n",
    "#We don't expect actual job to be run within an hour. \n",
    "# So re-write config, if being run within hour\n",
    "with open('config_{}0000.json'.format(time.strftime('%Y%m%d_%H')),'w') as f:\n",
    "    config = {**metadata,**model_config,**run_exp}\n",
    "    json.dump(config,f)\n",
    "print (len(experiments.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Errors and 0 Warning found\n",
      "ERRORS:\n",
      "[]\n",
      "\n",
      "WARNINGS:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def validate_config():\n",
    "    def getIssue(issue_type,config,key,problem,desc):\n",
    "        issue = {}\n",
    "        issue['type'] = issue_type\n",
    "        issue['config']=config\n",
    "        issue['key'] =key\n",
    "        issue['issue'] =problem\n",
    "        issue['desc'] =desc\n",
    "        return issue\n",
    "\n",
    "    # Metadata runtype should not have spaces\n",
    "    issues = []\n",
    "    if re.findall('\\s+',metadata['run_type']):\n",
    "        iss = getIssue('WARN','metadata','run_type',\n",
    "                'White space found','this may create issues in file creation')\n",
    "        issues.append(iss)\n",
    "    if model_config['epochs'] < 4:\n",
    "        iss = getIssue('WARN','model_config','epochs',\n",
    "                'Very few epochs','Epochs specified is < normal 4. Use Trial Run feature to test the run.')\n",
    "        issues.append(iss)\n",
    "\n",
    "    exp_keys = list(run_exp.keys())\n",
    "    required_keys = ['dataset_path',\n",
    "                'input_model_path',\n",
    "                'model_path',\n",
    "                'output_path',\n",
    "                'test_corpus']\n",
    "    train_req_keys = ['epochs',\n",
    "                    'batch_size',\n",
    "                    'max_length',\n",
    "                    'lr',\n",
    "                    'drop_out']\n",
    "\n",
    "    for i,e in enumerate(exp_keys):\n",
    "        ec = {}\n",
    "\n",
    "        ec.update(model_config)\n",
    "        ec.update(run_exp[e])\n",
    "\n",
    "        #All required keys MUST be present in config\n",
    "        missing = [key for key in required_keys if key not in ec]\n",
    "        if missing:\n",
    "            iss = getIssue('ERROR', e, ','.join(missing),\n",
    "                'Required keys missing','Provide missing keys')\n",
    "            issues.append(iss)\n",
    "\n",
    "        load_model = ec.get('load_model',None)\n",
    "        if load_model:\n",
    "            #If load model is specified, check if a prior step is creating the model\n",
    "            found = False \n",
    "            for j in range(i):\n",
    "                sm = run_exp[exp_keys[j]].get('save_model',None)\n",
    "                if sm == load_model:\n",
    "                    found = True \n",
    "            if not found:\n",
    "                load_model_f = ec['input_model_path'] + '/' + load_model + metadata['model_ext']\n",
    "                found = os.path.exists(load_model_f)\n",
    "            if not found:\n",
    "                load_model_f = ec['dataset_path'] + '/' + load_model + metadata['model_ext']\n",
    "                found = os.path.exists(load_model_f)\n",
    "            if not found:\n",
    "                iss = getIssue('ERROR',e,'load_model',\n",
    "                      'Load Model not found',\n",
    "                      '{} not found in previous experiments or as file {}'.format(\n",
    "                          load_model,load_model_f))\n",
    "                issues.append(iss)\n",
    "        if ec.get('train_corpus',None):\n",
    "            missing = [key for key in train_req_keys if key not in ec]\n",
    "            if missing:\n",
    "                iss = getIssue('ERROR', e, ','.join(missing),\n",
    "                'Required keys missing','These keys required when training data is specified')\n",
    "                issues.append(iss)\n",
    "            tc = ec['train_corpus']\n",
    "            ts = ec.get('train_samples',None)\n",
    "            if not isinstance(tc,list):\n",
    "                iss = getIssue('ERROR',e,'train_corpus',\n",
    "                        'Training corpus must be a list',\n",
    "                        'Training corpus {} is not a list'.format(tc)) \n",
    "                issues.append(iss)\n",
    "\n",
    "            if ts:\n",
    "                if not isinstance(ts,list):\n",
    "                    iss = getIssue('ERROR',e,'train_samples',\n",
    "                            'Training samples must be a list',\n",
    "                            'Training samples {} is not a list'.format(ts)) \n",
    "                    issues.append(iss)\n",
    "                if isinstance(tc,list) and isinstance(ts,list):\n",
    "                    if len(tc) != len(ec['train_samples']):\n",
    "                        iss = getIssue('ERROR',e,'train_corpus,train_samples',\n",
    "                                'Training corpus, samples length mismatch',\n",
    "                                'Training corpus {}, samples {}'.format(\n",
    "                                    ','.join(tc),','.join(ec['train_samples']))) \n",
    "                        issues.append(iss)\n",
    "            for fc in tc:\n",
    "                fl = [ec['dataset_path'] +'/' + fc + '_train.tsv',ec['dataset_path'] +'/' + fc + '_dev.tsv']\n",
    "                for f in fl:\n",
    "                    if not os.path.exists(f):\n",
    "                        iss = getIssue('ERROR',e,'train_corpus',\n",
    "                                'Train file not found',\n",
    "                                'Training corpus {}. File {} not found'.format(\n",
    "                                    ','.join(tc),f))\n",
    "                        issues.append(iss)\n",
    "\n",
    "        if ec.get('test_corpus',None):\n",
    "            tc = ec['test_corpus']\n",
    "            ts = ec.get('test_samples',None)\n",
    "            if not isinstance(tc,list):\n",
    "                iss = getIssue('ERROR',e,'test_corpus',\n",
    "                        'Test corpus must be a list',\n",
    "                        'Test corpus {} is not a list'.format(tc)) \n",
    "                issues.append(iss)\n",
    "            if ts:\n",
    "                if not isinstance(ts,list):\n",
    "                    iss = getIssue('ERROR',e,'train_samples',\n",
    "                            'Training samples must be a list',\n",
    "                            'Training samples {} is not a list'.format(ts)) \n",
    "                    issues.append(iss)\n",
    "                if isinstance(tc,list) and isinstance(ts,list):\n",
    "                    if len(tc) != len(ec['test_samples']):\n",
    "                        iss = getIssue('ERROR',e,'test_corpus,test_samples',\n",
    "                                'Test corpus, samples length mismatch',\n",
    "                                'Test corpus {}, samples {}'.format(\n",
    "                                    ','.join(tc),','.join(ec['test_samples'])))\n",
    "                        issues.append(iss)\n",
    "            if i > 0:\n",
    "                ps = run_exp[exp_keys[i-1]].get('test_samples',None)\n",
    "                if ts != ps :\n",
    "                    iss = getIssue('WARN',e,'test_samples',\n",
    "                        'Test sample does not match previous',\n",
    "                        'Test sample {} for {} does not match {} s sample {}'.format(e,ts,exp_keys[i-1],ps))\n",
    "                    issues.append(iss)\n",
    "            for fc in tc:\n",
    "                f = ec['dataset_path'] +'/' + fc + '_test.tsv'\n",
    "                if not os.path.exists(f):\n",
    "                    iss = getIssue('ERROR',e,'test_corpus',\n",
    "                          'Test file not found',\n",
    "                          'Test corpus {}. File {} not found'.format(','.join(tc),f))\n",
    "                    issues.append(iss)\n",
    "        else:\n",
    "            iss = getIssue('ERROR',e,'test_corpus',\n",
    "                'Test corpus is empty',\n",
    "                'Test corpus cannot be None. Must be specified')\n",
    "            issues.append(iss) \n",
    "    return issues                                       \n",
    "\n",
    "issues = validate_config()\n",
    "errors = [iss for iss in issues if iss['type'] == 'ERROR']\n",
    "warnings = [iss for iss in issues if iss['type'] == 'WARN']\n",
    "print ('{} Errors and {} Warning found'.format(len(errors),len(warnings)))\n",
    "print ('ERRORS:')\n",
    "pprint.pprint(errors)\n",
    "print ('')\n",
    "print ('WARNINGS:')\n",
    "pprint.pprint(warnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([k['desc'] for k in errors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production 174 experiments\n",
      "\n",
      "**** Model_Config ****\n",
      "{'batch_size': 16,\n",
      " 'dataset_path': 'Data/',\n",
      " 'drop_out': 0.2,\n",
      " 'epochs': 4,\n",
      " 'input_model_path': './models',\n",
      " 'lr': 4e-05,\n",
      " 'max_length': 512,\n",
      " 'model_path': './models',\n",
      " 'output_path': './training_results/clean_runs'}\n",
      "\n",
      "**** Run Experiments ****\n",
      "['TALES_100',\n",
      " 'TALES_200',\n",
      " 'TALES_300',\n",
      " 'TALES_400',\n",
      " 'TARGET_500',\n",
      " 'TALES_600',\n",
      " 'TALES_700',\n",
      " 'TALES_800',\n",
      " 'TALES_900',\n",
      " 'TALES_1000',\n",
      " 'TALES_1100',\n",
      " 'TALES_1200',\n",
      " 'TALES_1300',\n",
      " 'TALES_1400',\n",
      " 'TALES_1500',\n",
      " 'TALES_1600',\n",
      " 'TALES_1700',\n",
      " 'TALES_1800',\n",
      " 'TALES_1900',\n",
      " 'TALES_2000',\n",
      " 'TALES_2100',\n",
      " 'TALES_2200',\n",
      " 'TALES_2300',\n",
      " 'TALES_2400',\n",
      " 'TALES_2500',\n",
      " 'TALES_2600',\n",
      " 'TALES_2700',\n",
      " 'TALES_2800',\n",
      " 'TALES_2900',\n",
      " 'TALES_3000',\n",
      " 'TALES_3100',\n",
      " 'TALES_3200',\n",
      " 'TALES_3300',\n",
      " 'TALES_3400',\n",
      " 'TALES_3500',\n",
      " 'TALES_3600',\n",
      " 'TALES_3700',\n",
      " 'TALES_3800',\n",
      " 'TALES_3900',\n",
      " 'TALES_4000',\n",
      " 'TALES_5000',\n",
      " 'TALES_8000',\n",
      " 'TALES_ALL',\n",
      " 'GEMB_FR_3_5_100',\n",
      " 'GEMB_FR_3_5_200',\n",
      " 'GEMB_FR_3_5_300',\n",
      " 'GEMB_FR_3_5_400',\n",
      " 'GEMB_FR_3_5_500',\n",
      " 'GEMB_FR_3_5_600',\n",
      " 'GEMB_FR_3_5_700',\n",
      " 'GEMB_FR_3_5_800',\n",
      " 'GEMB_FR_3_5_900',\n",
      " 'GEMB_FR_3_5_1000',\n",
      " 'GEMB_FR_3_5_1100',\n",
      " 'GEMB_FR_3_5_1200',\n",
      " 'GEMB_FR_3_5_1300',\n",
      " 'GEMB_FR_3_5_1400',\n",
      " 'GEMB_FR_3_5_1500',\n",
      " 'GEMB_FR_3_5_1600',\n",
      " 'GEMB_FR_3_5_1700',\n",
      " 'GEMB_FR_3_5_1800',\n",
      " 'GEMB_FR_3_5_1900',\n",
      " 'GEMB_FR_3_5_2000',\n",
      " 'GEMB_FR_3_5_2100',\n",
      " 'GEMB_FR_3_5_2200',\n",
      " 'GEMB_FR_3_5_2300',\n",
      " 'GEMB_FR_3_5_2400',\n",
      " 'GEMB_FR_3_5_2500',\n",
      " 'GEMB_FR_3_5_2600',\n",
      " 'GEMB_FR_3_5_2700',\n",
      " 'GEMB_FR_3_5_2800',\n",
      " 'GEMB_FR_3_5_2900',\n",
      " 'GEMB_FR_3_5_3000',\n",
      " 'GEMB_FR_3_5_3100',\n",
      " 'GEMB_FR_3_5_3200',\n",
      " 'GEMB_FR_3_5_3300',\n",
      " 'GEMB_FR_3_5_3400',\n",
      " 'GEMB_FR_3_5_3500',\n",
      " 'GEMB_FR_3_5_3600',\n",
      " 'GEMB_FR_3_5_3700',\n",
      " 'GEMB_FR_3_5_3800',\n",
      " 'GEMB_FR_3_5_3900',\n",
      " 'GEMB_FR_3_5_4000',\n",
      " 'GEMB_FR_3_5_5000',\n",
      " 'GEMB_FR_3_5_8000',\n",
      " 'GEMB_FR_3_5_ALL',\n",
      " 'GO_SEM_FR_100',\n",
      " 'GO_SEM_FR_200',\n",
      " 'GO_SEM_FR_300',\n",
      " 'GO_SEM_FR_400',\n",
      " 'GO_SEM_FR_500',\n",
      " 'GO_SEM_FR_600',\n",
      " 'GO_SEM_FR_700',\n",
      " 'GO_SEM_FR_800',\n",
      " 'GO_SEM_FR_900',\n",
      " 'GO_SEM_FR_1000',\n",
      " 'GO_SEM_FR_1100',\n",
      " 'GO_SEM_FR_1200',\n",
      " 'GO_SEM_FR_1300',\n",
      " 'GO_SEM_FR_1400',\n",
      " 'GO_SEM_FR_1500',\n",
      " 'GO_SEM_FR_1600',\n",
      " 'GO_SEM_FR_1700',\n",
      " 'GO_SEM_FR_1800',\n",
      " 'GO_SEM_FR_1900',\n",
      " 'GO_SEM_FR_2000',\n",
      " 'GO_SEM_FR_2100',\n",
      " 'GO_SEM_FR_2200',\n",
      " 'GO_SEM_FR_2300',\n",
      " 'GO_SEM_FR_2400',\n",
      " 'GO_SEM_FR_2500',\n",
      " 'GO_SEM_FR_2600',\n",
      " 'GO_SEM_FR_2700',\n",
      " 'GO_SEM_FR_2800',\n",
      " 'GO_SEM_FR_2900',\n",
      " 'GO_SEM_FR_3000',\n",
      " 'GO_SEM_FR_3100',\n",
      " 'GO_SEM_FR_3200',\n",
      " 'GO_SEM_FR_3300',\n",
      " 'GO_SEM_FR_3400',\n",
      " 'GO_SEM_FR_3500',\n",
      " 'GO_SEM_FR_3600',\n",
      " 'GO_SEM_FR_3700',\n",
      " 'GO_SEM_FR_3800',\n",
      " 'GO_SEM_FR_3900',\n",
      " 'GO_SEM_FR_4000',\n",
      " 'GO_SEM_FR_5000',\n",
      " 'GO_SEM_FR_8000',\n",
      " 'GO_SEM_FR_ALL',\n",
      " 'GO_FR_100',\n",
      " 'GO_FR_200',\n",
      " 'GO_FR_300',\n",
      " 'GO_FR_400',\n",
      " 'GO_FR_500',\n",
      " 'GO_FR_600',\n",
      " 'GO_FR_700',\n",
      " 'GO_FR_800',\n",
      " 'GO_FR_900',\n",
      " 'GO_FR_1000',\n",
      " 'GO_FR_1100',\n",
      " 'GO_FR_1200',\n",
      " 'GO_FR_1300',\n",
      " 'GO_FR_1400',\n",
      " 'GO_FR_1500',\n",
      " 'GO_FR_1600',\n",
      " 'GO_FR_1700',\n",
      " 'GO_FR_1800',\n",
      " 'GO_FR_1900',\n",
      " 'GO_FR_2000',\n",
      " 'GO_FR_2100',\n",
      " 'GO_FR_2200',\n",
      " 'GO_FR_2300',\n",
      " 'GO_FR_2400',\n",
      " 'GO_FR_2500',\n",
      " 'GO_FR_2600',\n",
      " 'GO_FR_2700',\n",
      " 'GO_FR_2800',\n",
      " 'GO_FR_2900',\n",
      " 'GO_FR_3000',\n",
      " 'GO_FR_3100',\n",
      " 'GO_FR_3200',\n",
      " 'GO_FR_3300',\n",
      " 'GO_FR_3400',\n",
      " 'GO_FR_3500',\n",
      " 'GO_FR_3600',\n",
      " 'GO_FR_3700',\n",
      " 'GO_FR_3800',\n",
      " 'GO_FR_3900',\n",
      " 'GO_FR_4000',\n",
      " 'GO_FR_5000',\n",
      " 'GO_FR_8000',\n",
      " 'GO_FR_ALL',\n",
      " 'GO_SEM_1000',\n",
      " 'GEMB_500']\n",
      "{'GEMB_500': {'description': \"GoEmo with embeddings frozen with ['tales'] 500 \"\n",
      "                             'samplesand transfer learning from '\n",
      "                             'emo_goemo_emb_fr',\n",
      "              'load_model': 'emo_goemo_emb_fr',\n",
      "              'name': 'GEMB',\n",
      "              'run': True,\n",
      "              'run_step': 173,\n",
      "              'save_stats': 'stats_gemb_500',\n",
      "              'test_corpus': ['tales'],\n",
      "              'train_corpus': ['tales'],\n",
      "              'train_samples': [500]},\n",
      " 'GEMB_FR_3_5_100': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                    'Friends with layers 3 to 5 frozen with '\n",
      "                                    \"['tales'] 100 samplesand transfer \"\n",
      "                                    'learning from goemb_frall_tr3_5',\n",
      "                     'load_model': 'goemb_frall_tr3_5',\n",
      "                     'name': 'GEMB_FR_3_5',\n",
      "                     'run': True,\n",
      "                     'run_step': 43,\n",
      "                     'save_stats': 'stats_gemb_fr_3_5_100',\n",
      "                     'test_corpus': ['tales'],\n",
      "                     'train_corpus': ['tales'],\n",
      "                     'train_samples': [100]},\n",
      " 'GEMB_FR_3_5_1000': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 1000 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 52,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_1000',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [1000]},\n",
      " 'GEMB_FR_3_5_1100': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 1100 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 53,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_1100',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [1100]},\n",
      " 'GEMB_FR_3_5_1200': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 1200 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 54,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_1200',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [1200]},\n",
      " 'GEMB_FR_3_5_1300': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 1300 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 55,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_1300',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [1300]},\n",
      " 'GEMB_FR_3_5_1400': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 1400 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 56,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_1400',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [1400]},\n",
      " 'GEMB_FR_3_5_1500': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 1500 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 57,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_1500',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [1500]},\n",
      " 'GEMB_FR_3_5_1600': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 1600 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 58,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_1600',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [1600]},\n",
      " 'GEMB_FR_3_5_1700': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 1700 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 59,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_1700',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [1700]},\n",
      " 'GEMB_FR_3_5_1800': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 1800 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 60,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_1800',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [1800]},\n",
      " 'GEMB_FR_3_5_1900': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 1900 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 61,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_1900',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [1900]},\n",
      " 'GEMB_FR_3_5_200': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                    'Friends with layers 3 to 5 frozen with '\n",
      "                                    \"['tales'] 200 samplesand transfer \"\n",
      "                                    'learning from goemb_frall_tr3_5',\n",
      "                     'load_model': 'goemb_frall_tr3_5',\n",
      "                     'name': 'GEMB_FR_3_5',\n",
      "                     'run': True,\n",
      "                     'run_step': 44,\n",
      "                     'save_stats': 'stats_gemb_fr_3_5_200',\n",
      "                     'test_corpus': ['tales'],\n",
      "                     'train_corpus': ['tales'],\n",
      "                     'train_samples': [200]},\n",
      " 'GEMB_FR_3_5_2000': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 2000 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 62,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_2000',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [2000]},\n",
      " 'GEMB_FR_3_5_2100': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 2100 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 63,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_2100',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [2100]},\n",
      " 'GEMB_FR_3_5_2200': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 2200 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 64,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_2200',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [2200]},\n",
      " 'GEMB_FR_3_5_2300': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 2300 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 65,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_2300',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [2300]},\n",
      " 'GEMB_FR_3_5_2400': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 2400 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 66,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_2400',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [2400]},\n",
      " 'GEMB_FR_3_5_2500': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 2500 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 67,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_2500',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [2500]},\n",
      " 'GEMB_FR_3_5_2600': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 2600 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 68,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_2600',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [2600]},\n",
      " 'GEMB_FR_3_5_2700': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 2700 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 69,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_2700',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [2700]},\n",
      " 'GEMB_FR_3_5_2800': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 2800 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 70,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_2800',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [2800]},\n",
      " 'GEMB_FR_3_5_2900': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 2900 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 71,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_2900',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [2900]},\n",
      " 'GEMB_FR_3_5_300': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                    'Friends with layers 3 to 5 frozen with '\n",
      "                                    \"['tales'] 300 samplesand transfer \"\n",
      "                                    'learning from goemb_frall_tr3_5',\n",
      "                     'load_model': 'goemb_frall_tr3_5',\n",
      "                     'name': 'GEMB_FR_3_5',\n",
      "                     'run': True,\n",
      "                     'run_step': 45,\n",
      "                     'save_stats': 'stats_gemb_fr_3_5_300',\n",
      "                     'test_corpus': ['tales'],\n",
      "                     'train_corpus': ['tales'],\n",
      "                     'train_samples': [300]},\n",
      " 'GEMB_FR_3_5_3000': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 3000 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 72,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_3000',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [3000]},\n",
      " 'GEMB_FR_3_5_3100': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 3100 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 73,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_3100',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [3100]},\n",
      " 'GEMB_FR_3_5_3200': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 3200 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 74,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_3200',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [3200]},\n",
      " 'GEMB_FR_3_5_3300': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 3300 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 75,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_3300',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [3300]},\n",
      " 'GEMB_FR_3_5_3400': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 3300 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 76,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_3400',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [3400]},\n",
      " 'GEMB_FR_3_5_3500': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 3500 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 77,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_3500',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [3500]},\n",
      " 'GEMB_FR_3_5_3600': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 3600 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 78,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_3600',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [3600]},\n",
      " 'GEMB_FR_3_5_3700': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 3700 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 79,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_3700',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [3700]},\n",
      " 'GEMB_FR_3_5_3800': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 3800 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 80,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_3800',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [3800]},\n",
      " 'GEMB_FR_3_5_3900': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 3900 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 81,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_3900',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [3900]},\n",
      " 'GEMB_FR_3_5_400': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                    'Friends with layers 3 to 5 frozen with '\n",
      "                                    \"['tales'] 400 samplesand transfer \"\n",
      "                                    'learning from goemb_frall_tr3_5',\n",
      "                     'load_model': 'goemb_frall_tr3_5',\n",
      "                     'name': 'GEMB_FR_3_5',\n",
      "                     'run': True,\n",
      "                     'run_step': 46,\n",
      "                     'save_stats': 'stats_gemb_fr_3_5_400',\n",
      "                     'test_corpus': ['tales'],\n",
      "                     'train_corpus': ['tales'],\n",
      "                     'train_samples': [400]},\n",
      " 'GEMB_FR_3_5_4000': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 4000 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 82,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_4000',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [4000]},\n",
      " 'GEMB_FR_3_5_500': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                    'Friends with layers 3 to 5 frozen with '\n",
      "                                    \"['tales'] 500 samplesand transfer \"\n",
      "                                    'learning from goemb_frall_tr3_5',\n",
      "                     'load_model': 'goemb_frall_tr3_5',\n",
      "                     'name': 'GEMB_FR_3_5',\n",
      "                     'run': True,\n",
      "                     'run_step': 47,\n",
      "                     'save_stats': 'stats_gemb_fr_3_5_500',\n",
      "                     'test_corpus': ['tales'],\n",
      "                     'train_corpus': ['tales'],\n",
      "                     'train_samples': [500]},\n",
      " 'GEMB_FR_3_5_5000': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 5000 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 83,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_5000',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [5000]},\n",
      " 'GEMB_FR_3_5_600': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                    'Friends with layers 3 to 5 frozen with '\n",
      "                                    \"['tales'] 600 samplesand transfer \"\n",
      "                                    'learning from goemb_frall_tr3_5',\n",
      "                     'load_model': 'goemb_frall_tr3_5',\n",
      "                     'name': 'GEMB_FR_3_5',\n",
      "                     'run': True,\n",
      "                     'run_step': 48,\n",
      "                     'save_stats': 'stats_gemb_fr_3_5_600',\n",
      "                     'test_corpus': ['tales'],\n",
      "                     'train_corpus': ['tales'],\n",
      "                     'train_samples': [600]},\n",
      " 'GEMB_FR_3_5_700': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                    'Friends with layers 3 to 5 frozen with '\n",
      "                                    \"['tales'] 700 samplesand transfer \"\n",
      "                                    'learning from goemb_frall_tr3_5',\n",
      "                     'load_model': 'goemb_frall_tr3_5',\n",
      "                     'name': 'GEMB_FR_3_5',\n",
      "                     'run': True,\n",
      "                     'run_step': 49,\n",
      "                     'save_stats': 'stats_gemb_fr_3_5_700',\n",
      "                     'test_corpus': ['tales'],\n",
      "                     'train_corpus': ['tales'],\n",
      "                     'train_samples': [700]},\n",
      " 'GEMB_FR_3_5_800': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                    'Friends with layers 3 to 5 frozen with '\n",
      "                                    \"['tales'] 800 samplesand transfer \"\n",
      "                                    'learning from goemb_frall_tr3_5',\n",
      "                     'load_model': 'goemb_frall_tr3_5',\n",
      "                     'name': 'GEMB_FR_3_5',\n",
      "                     'run': True,\n",
      "                     'run_step': 50,\n",
      "                     'save_stats': 'stats_gemb_fr_3_5_800',\n",
      "                     'test_corpus': ['tales'],\n",
      "                     'train_corpus': ['tales'],\n",
      "                     'train_samples': [800]},\n",
      " 'GEMB_FR_3_5_8000': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                     'Friends with layers 3 to 5 frozen with '\n",
      "                                     \"['tales'] 8000 samplesand transfer \"\n",
      "                                     'learning from goemb_frall_tr3_5',\n",
      "                      'load_model': 'goemb_frall_tr3_5',\n",
      "                      'name': 'GEMB_FR_3_5',\n",
      "                      'run': True,\n",
      "                      'run_step': 84,\n",
      "                      'save_stats': 'stats_gemb_fr_3_5_8000',\n",
      "                      'test_corpus': ['tales'],\n",
      "                      'train_corpus': ['tales'],\n",
      "                      'train_samples': [8000]},\n",
      " 'GEMB_FR_3_5_900': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                    'Friends with layers 3 to 5 frozen with '\n",
      "                                    \"['tales'] 900 samplesand transfer \"\n",
      "                                    'learning from goemb_frall_tr3_5',\n",
      "                     'load_model': 'goemb_frall_tr3_5',\n",
      "                     'name': 'GEMB_FR_3_5',\n",
      "                     'run': True,\n",
      "                     'run_step': 51,\n",
      "                     'save_stats': 'stats_gemb_fr_3_5_900',\n",
      "                     'test_corpus': ['tales'],\n",
      "                     'train_corpus': ['tales'],\n",
      "                     'train_samples': [900]},\n",
      " 'GEMB_FR_3_5_ALL': {'description': 'GoEmo with embeddings frozen followed by '\n",
      "                                    'Friends with layers 3 to 5 frozen with '\n",
      "                                    \"['tales'] ALL samples and xfer learn from \"\n",
      "                                    'goemb_frall_tr3_5',\n",
      "                     'load_model': 'goemb_frall_tr3_5',\n",
      "                     'name': 'GEMB_FR_3_5',\n",
      "                     'run': True,\n",
      "                     'run_step': 85,\n",
      "                     'save_stats': 'stats_gemb_fr_3_5_all',\n",
      "                     'test_corpus': ['tales'],\n",
      "                     'train_corpus': ['tales'],\n",
      "                     'train_samples': None},\n",
      " 'GO_FR_100': {'description': \"GoEmo, Friends  with ['tales'] 100 samplesand \"\n",
      "                              'transfer learning from emo_goemo_fr',\n",
      "               'load_model': 'emo_goemo_fr',\n",
      "               'name': 'GO_FR',\n",
      "               'run': True,\n",
      "               'run_step': 129,\n",
      "               'save_stats': 'stats_go_fr_100',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [100]},\n",
      " 'GO_FR_1000': {'description': \"GoEmo, Friends  with ['tales'] 1000 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 138,\n",
      "                'save_stats': 'stats_go_fr_1000',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1000]},\n",
      " 'GO_FR_1100': {'description': \"GoEmo, Friends  with ['tales'] 1100 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 139,\n",
      "                'save_stats': 'stats_go_fr_1100',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1100]},\n",
      " 'GO_FR_1200': {'description': \"GoEmo, Friends  with ['tales'] 1200 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 140,\n",
      "                'save_stats': 'stats_go_fr_1200',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1200]},\n",
      " 'GO_FR_1300': {'description': \"GoEmo, Friends  with ['tales'] 1300 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 141,\n",
      "                'save_stats': 'stats_go_fr_1300',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1300]},\n",
      " 'GO_FR_1400': {'description': \"GoEmo, Friends  with ['tales'] 1400 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 142,\n",
      "                'save_stats': 'stats_go_fr_1400',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1400]},\n",
      " 'GO_FR_1500': {'description': \"GoEmo, Friends  with ['tales'] 1500 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 143,\n",
      "                'save_stats': 'stats_go_fr_1500',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1500]},\n",
      " 'GO_FR_1600': {'description': \"GoEmo, Friends  with ['tales'] 1600 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 144,\n",
      "                'save_stats': 'stats_go_fr_1600',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1600]},\n",
      " 'GO_FR_1700': {'description': \"GoEmo, Friends  with ['tales'] 1700 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 145,\n",
      "                'save_stats': 'stats_go_fr_1700',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1700]},\n",
      " 'GO_FR_1800': {'description': \"GoEmo, Friends  with ['tales'] 1800 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 146,\n",
      "                'save_stats': 'stats_go_fr_1800',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1800]},\n",
      " 'GO_FR_1900': {'description': \"GoEmo, Friends  with ['tales'] 1800 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 147,\n",
      "                'save_stats': 'stats_go_fr_1900',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1900]},\n",
      " 'GO_FR_200': {'description': \"GoEmo, Friends  with ['tales'] 200 samplesand \"\n",
      "                              'transfer learning from emo_goemo_fr',\n",
      "               'load_model': 'emo_goemo_fr',\n",
      "               'name': 'GO_FR',\n",
      "               'run': True,\n",
      "               'run_step': 130,\n",
      "               'save_stats': 'stats_go_fr_200',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [200]},\n",
      " 'GO_FR_2000': {'description': \"GoEmo, Friends  with ['tales'] 2000 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 148,\n",
      "                'save_stats': 'stats_go_fr_2000',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2000]},\n",
      " 'GO_FR_2100': {'description': \"GoEmo, Friends  with ['tales'] 2100 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 149,\n",
      "                'save_stats': 'stats_go_fr_2100',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2100]},\n",
      " 'GO_FR_2200': {'description': \"GoEmo, Friends  with ['tales'] 2200 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 150,\n",
      "                'save_stats': 'stats_go_fr_2200',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2200]},\n",
      " 'GO_FR_2300': {'description': \"GoEmo, Friends  with ['tales'] 2300 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 151,\n",
      "                'save_stats': 'stats_go_fr_2300',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2300]},\n",
      " 'GO_FR_2400': {'description': \"GoEmo, Friends  with ['tales'] 2400 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 152,\n",
      "                'save_stats': 'stats_go_fr_2400',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2400]},\n",
      " 'GO_FR_2500': {'description': \"GoEmo, Friends  with ['tales'] 2500 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 153,\n",
      "                'save_stats': 'stats_go_fr_2500',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2500]},\n",
      " 'GO_FR_2600': {'description': \"GoEmo, Friends  with ['tales'] 2500 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 154,\n",
      "                'save_stats': 'stats_go_fr_2600',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2600]},\n",
      " 'GO_FR_2700': {'description': \"GoEmo, Friends  with ['tales'] 2700 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 155,\n",
      "                'save_stats': 'stats_go_fr_2700',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2700]},\n",
      " 'GO_FR_2800': {'description': \"GoEmo, Friends  with ['tales'] 2800 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 156,\n",
      "                'save_stats': 'stats_go_fr_2800',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2800]},\n",
      " 'GO_FR_2900': {'description': \"GoEmo, Friends  with ['tales'] 2900 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 157,\n",
      "                'save_stats': 'stats_go_fr_2900',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2900]},\n",
      " 'GO_FR_300': {'description': \"GoEmo, Friends  with ['tales'] 300 samplesand \"\n",
      "                              'transfer learning from emo_goemo_fr',\n",
      "               'load_model': 'emo_goemo_fr',\n",
      "               'name': 'GO_FR',\n",
      "               'run': True,\n",
      "               'run_step': 131,\n",
      "               'save_stats': 'stats_go_fr_300',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [300]},\n",
      " 'GO_FR_3000': {'description': \"GoEmo, Friends  with ['tales'] 3000 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 158,\n",
      "                'save_stats': 'stats_go_fr_3000',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3000]},\n",
      " 'GO_FR_3100': {'description': \"GoEmo, Friends  with ['tales'] 3100 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 159,\n",
      "                'save_stats': 'stats_go_fr_3100',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3100]},\n",
      " 'GO_FR_3200': {'description': \"GoEmo, Friends  with ['tales'] 3200 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 160,\n",
      "                'save_stats': 'stats_go_fr_3200',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3200]},\n",
      " 'GO_FR_3300': {'description': \"GoEmo, Friends  with ['tales'] 3300 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 161,\n",
      "                'save_stats': 'stats_go_fr_3300',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3300]},\n",
      " 'GO_FR_3400': {'description': \"GoEmo, Friends  with ['tales'] 3400 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 162,\n",
      "                'save_stats': 'stats_go_fr_3400',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3400]},\n",
      " 'GO_FR_3500': {'description': \"GoEmo, Friends  with ['tales'] 3500 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 163,\n",
      "                'save_stats': 'stats_go_fr_3500',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3500]},\n",
      " 'GO_FR_3600': {'description': \"GoEmo, Friends  with ['tales'] 3600 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 164,\n",
      "                'save_stats': 'stats_go_fr_3600',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3600]},\n",
      " 'GO_FR_3700': {'description': \"GoEmo, Friends  with ['tales'] 3700 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 165,\n",
      "                'save_stats': 'stats_go_fr_3700',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3700]},\n",
      " 'GO_FR_3800': {'description': \"GoEmo, Friends  with ['tales'] 3700 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 166,\n",
      "                'save_stats': 'stats_go_fr_3800',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3800]},\n",
      " 'GO_FR_3900': {'description': \"GoEmo, Friends  with ['tales'] 3900 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 167,\n",
      "                'save_stats': 'stats_go_fr_3900',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3900]},\n",
      " 'GO_FR_400': {'description': \"GoEmo, Friends  with ['tales'] 400 samplesand \"\n",
      "                              'transfer learning from emo_goemo_fr',\n",
      "               'load_model': 'emo_goemo_fr',\n",
      "               'name': 'GO_FR',\n",
      "               'run': True,\n",
      "               'run_step': 132,\n",
      "               'save_stats': 'stats_go_fr_400',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [400]},\n",
      " 'GO_FR_4000': {'description': \"GoEmo, Friends  with ['tales'] 4000 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 168,\n",
      "                'save_stats': 'stats_go_fr_4000',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [4000]},\n",
      " 'GO_FR_500': {'description': \"GoEmo, Friends  with ['tales'] 500 samplesand \"\n",
      "                              'transfer learning from emo_goemo_fr',\n",
      "               'load_model': 'emo_goemo_fr',\n",
      "               'name': 'GO_FR',\n",
      "               'run': True,\n",
      "               'run_step': 133,\n",
      "               'save_stats': 'stats_go_fr_500',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [500]},\n",
      " 'GO_FR_5000': {'description': \"GoEmo, Friends  with ['tales'] 5000 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 169,\n",
      "                'save_stats': 'stats_go_fr_5000',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [5000]},\n",
      " 'GO_FR_600': {'description': \"GoEmo, Friends  with ['tales'] 600 samplesand \"\n",
      "                              'transfer learning from emo_goemo_fr',\n",
      "               'load_model': 'emo_goemo_fr',\n",
      "               'name': 'GO_FR',\n",
      "               'run': True,\n",
      "               'run_step': 134,\n",
      "               'save_stats': 'stats_go_fr_600',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [600]},\n",
      " 'GO_FR_700': {'description': \"GoEmo, Friends  with ['tales'] 700 samplesand \"\n",
      "                              'transfer learning from emo_goemo_fr',\n",
      "               'load_model': 'emo_goemo_fr',\n",
      "               'name': 'GO_FR',\n",
      "               'run': True,\n",
      "               'run_step': 135,\n",
      "               'save_stats': 'stats_go_fr_700',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [700]},\n",
      " 'GO_FR_800': {'description': \"GoEmo, Friends  with ['tales'] 800 samplesand \"\n",
      "                              'transfer learning from emo_goemo_fr',\n",
      "               'load_model': 'emo_goemo_fr',\n",
      "               'name': 'GO_FR',\n",
      "               'run': True,\n",
      "               'run_step': 136,\n",
      "               'save_stats': 'stats_go_fr_800',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [800]},\n",
      " 'GO_FR_8000': {'description': \"GoEmo, Friends  with ['tales'] 8000 samplesand \"\n",
      "                               'transfer learning from emo_goemo_fr',\n",
      "                'load_model': 'emo_goemo_fr',\n",
      "                'name': 'GO_FR',\n",
      "                'run': True,\n",
      "                'run_step': 170,\n",
      "                'save_stats': 'stats_go_fr_8000',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [8000]},\n",
      " 'GO_FR_900': {'description': \"GoEmo, Friends  with ['tales'] 900 samplesand \"\n",
      "                              'transfer learning from emo_goemo_fr',\n",
      "               'load_model': 'emo_goemo_fr',\n",
      "               'name': 'GO_FR',\n",
      "               'run': True,\n",
      "               'run_step': 137,\n",
      "               'save_stats': 'stats_go_fr_900',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [900]},\n",
      " 'GO_FR_ALL': {'description': \"GoEmo, Friends  with ['tales'] ALL samples and \"\n",
      "                              'xfer learn from emo_goemo_fr',\n",
      "               'load_model': 'emo_goemo_fr',\n",
      "               'name': 'GO_FR',\n",
      "               'run': True,\n",
      "               'run_step': 171,\n",
      "               'save_stats': 'stats_go_fr_all',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': None},\n",
      " 'GO_SEM_1000': {'description': \"GoEmo, SemEval  with ['tales'] 1000 \"\n",
      "                                'samplesand transfer learning from '\n",
      "                                'emo_goemo_sem',\n",
      "                 'load_model': 'emo_goemo_sem',\n",
      "                 'name': 'GO_SEM',\n",
      "                 'run': True,\n",
      "                 'run_step': 172,\n",
      "                 'save_stats': 'stats_go_sem_1000',\n",
      "                 'test_corpus': ['tales'],\n",
      "                 'train_corpus': ['tales'],\n",
      "                 'train_samples': [1000]},\n",
      " 'GO_SEM_FR_100': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                  '100 samplesand transfer learning from '\n",
      "                                  'emo_goemo_sem_fr',\n",
      "                   'load_model': 'emo_goemo_sem_fr',\n",
      "                   'name': 'GO_SEM_FR',\n",
      "                   'run': True,\n",
      "                   'run_step': 86,\n",
      "                   'save_stats': 'stats_go_sem_fr_100',\n",
      "                   'test_corpus': ['tales'],\n",
      "                   'train_corpus': ['tales'],\n",
      "                   'train_samples': [100]},\n",
      " 'GO_SEM_FR_1000': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '1000 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 95,\n",
      "                    'save_stats': 'stats_go_sem_fr_1000',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [1000]},\n",
      " 'GO_SEM_FR_1100': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '1100 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 96,\n",
      "                    'save_stats': 'stats_go_sem_fr_1100',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [1100]},\n",
      " 'GO_SEM_FR_1200': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '1200 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 97,\n",
      "                    'save_stats': 'stats_go_sem_fr_1200',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [1200]},\n",
      " 'GO_SEM_FR_1300': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '1300 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 98,\n",
      "                    'save_stats': 'stats_go_sem_fr_1300',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [1300]},\n",
      " 'GO_SEM_FR_1400': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '1400 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 99,\n",
      "                    'save_stats': 'stats_go_sem_fr_1400',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [1400]},\n",
      " 'GO_SEM_FR_1500': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '1500 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 100,\n",
      "                    'save_stats': 'stats_go_sem_fr_1500',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [1500]},\n",
      " 'GO_SEM_FR_1600': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '1600 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 101,\n",
      "                    'save_stats': 'stats_go_sem_fr_1600',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [1600]},\n",
      " 'GO_SEM_FR_1700': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '1600 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 102,\n",
      "                    'save_stats': 'stats_go_sem_fr_1700',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [1700]},\n",
      " 'GO_SEM_FR_1800': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '1800 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 103,\n",
      "                    'save_stats': 'stats_go_sem_fr_1800',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [1800]},\n",
      " 'GO_SEM_FR_1900': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '1900 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 104,\n",
      "                    'save_stats': 'stats_go_sem_fr_1900',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [1900]},\n",
      " 'GO_SEM_FR_200': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                  '200 samplesand transfer learning from '\n",
      "                                  'emo_goemo_sem_fr',\n",
      "                   'load_model': 'emo_goemo_sem_fr',\n",
      "                   'name': 'GO_SEM_FR',\n",
      "                   'run': True,\n",
      "                   'run_step': 87,\n",
      "                   'save_stats': 'stats_go_sem_fr_200',\n",
      "                   'test_corpus': ['tales'],\n",
      "                   'train_corpus': ['tales'],\n",
      "                   'train_samples': [200]},\n",
      " 'GO_SEM_FR_2000': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '2000 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 105,\n",
      "                    'save_stats': 'stats_go_sem_fr_2000',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [2000]},\n",
      " 'GO_SEM_FR_2100': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '2100 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 106,\n",
      "                    'save_stats': 'stats_go_sem_fr_2100',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [2100]},\n",
      " 'GO_SEM_FR_2200': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '2200 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 107,\n",
      "                    'save_stats': 'stats_go_sem_fr_2200',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [2200]},\n",
      " 'GO_SEM_FR_2300': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '2300 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 108,\n",
      "                    'save_stats': 'stats_go_sem_fr_2300',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [2300]},\n",
      " 'GO_SEM_FR_2400': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '2500 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 109,\n",
      "                    'save_stats': 'stats_go_sem_fr_2400',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [2400]},\n",
      " 'GO_SEM_FR_2500': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '2500 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 110,\n",
      "                    'save_stats': 'stats_go_sem_fr_2500',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [2500]},\n",
      " 'GO_SEM_FR_2600': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '2600 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 111,\n",
      "                    'save_stats': 'stats_go_sem_fr_2600',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [2600]},\n",
      " 'GO_SEM_FR_2700': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '2700 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 112,\n",
      "                    'save_stats': 'stats_go_sem_fr_2700',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [2700]},\n",
      " 'GO_SEM_FR_2800': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '2800 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 113,\n",
      "                    'save_stats': 'stats_go_sem_fr_2800',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [2800]},\n",
      " 'GO_SEM_FR_2900': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '2900 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 114,\n",
      "                    'save_stats': 'stats_go_sem_fr_2900',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [2900]},\n",
      " 'GO_SEM_FR_300': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                  '300 samplesand transfer learning from '\n",
      "                                  'emo_goemo_sem_fr',\n",
      "                   'load_model': 'emo_goemo_sem_fr',\n",
      "                   'name': 'GO_SEM_FR',\n",
      "                   'run': True,\n",
      "                   'run_step': 88,\n",
      "                   'save_stats': 'stats_go_sem_fr_300',\n",
      "                   'test_corpus': ['tales'],\n",
      "                   'train_corpus': ['tales'],\n",
      "                   'train_samples': [300]},\n",
      " 'GO_SEM_FR_3000': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '3000 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 115,\n",
      "                    'save_stats': 'stats_go_sem_fr_3000',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [3000]},\n",
      " 'GO_SEM_FR_3100': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '3100 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 116,\n",
      "                    'save_stats': 'stats_go_sem_fr_3100',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [3100]},\n",
      " 'GO_SEM_FR_3200': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '3200 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 117,\n",
      "                    'save_stats': 'stats_go_sem_fr_3200',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [3200]},\n",
      " 'GO_SEM_FR_3300': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '3300 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 118,\n",
      "                    'save_stats': 'stats_go_sem_fr_3300',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [3300]},\n",
      " 'GO_SEM_FR_3400': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '3400 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 119,\n",
      "                    'save_stats': 'stats_go_sem_fr_3400',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [3400]},\n",
      " 'GO_SEM_FR_3500': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '3500 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 120,\n",
      "                    'save_stats': 'stats_go_sem_fr_3500',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [3500]},\n",
      " 'GO_SEM_FR_3600': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '3600 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 121,\n",
      "                    'save_stats': 'stats_go_sem_fr_3600',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [3600]},\n",
      " 'GO_SEM_FR_3700': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '3700 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 122,\n",
      "                    'save_stats': 'stats_go_sem_fr_3700',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [3700]},\n",
      " 'GO_SEM_FR_3800': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '3800 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 123,\n",
      "                    'save_stats': 'stats_go_sem_fr_3800',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [3800]},\n",
      " 'GO_SEM_FR_3900': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '3900 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 124,\n",
      "                    'save_stats': 'stats_go_sem_fr_3800',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [3900]},\n",
      " 'GO_SEM_FR_400': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                  '400 samplesand transfer learning from '\n",
      "                                  'emo_goemo_sem_fr',\n",
      "                   'load_model': 'emo_goemo_sem_fr',\n",
      "                   'name': 'GO_SEM_FR',\n",
      "                   'run': True,\n",
      "                   'run_step': 89,\n",
      "                   'save_stats': 'stats_go_sem_fr_400',\n",
      "                   'test_corpus': ['tales'],\n",
      "                   'train_corpus': ['tales'],\n",
      "                   'train_samples': [400]},\n",
      " 'GO_SEM_FR_4000': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '4000 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 125,\n",
      "                    'save_stats': 'stats_go_sem_fr_4000',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [4000]},\n",
      " 'GO_SEM_FR_500': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                  '500 samplesand transfer learning from '\n",
      "                                  'emo_goemo_sem_fr',\n",
      "                   'load_model': 'emo_goemo_sem_fr',\n",
      "                   'name': 'GO_SEM_FR',\n",
      "                   'run': True,\n",
      "                   'run_step': 90,\n",
      "                   'save_stats': 'stats_go_sem_fr_500',\n",
      "                   'test_corpus': ['tales'],\n",
      "                   'train_corpus': ['tales'],\n",
      "                   'train_samples': [500]},\n",
      " 'GO_SEM_FR_5000': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '5000 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 126,\n",
      "                    'save_stats': 'stats_go_sem_fr_5000',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [5000]},\n",
      " 'GO_SEM_FR_600': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                  '600 samplesand transfer learning from '\n",
      "                                  'emo_goemo_sem_fr',\n",
      "                   'load_model': 'emo_goemo_sem_fr',\n",
      "                   'name': 'GO_SEM_FR',\n",
      "                   'run': True,\n",
      "                   'run_step': 91,\n",
      "                   'save_stats': 'stats_go_sem_fr_600',\n",
      "                   'test_corpus': ['tales'],\n",
      "                   'train_corpus': ['tales'],\n",
      "                   'train_samples': [600]},\n",
      " 'GO_SEM_FR_700': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                  '700 samplesand transfer learning from '\n",
      "                                  'emo_goemo_sem_fr',\n",
      "                   'load_model': 'emo_goemo_sem_fr',\n",
      "                   'name': 'GO_SEM_FR',\n",
      "                   'run': True,\n",
      "                   'run_step': 92,\n",
      "                   'save_stats': 'stats_go_sem_fr_700',\n",
      "                   'test_corpus': ['tales'],\n",
      "                   'train_corpus': ['tales'],\n",
      "                   'train_samples': [700]},\n",
      " 'GO_SEM_FR_800': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                  '800 samplesand transfer learning from '\n",
      "                                  'emo_goemo_sem_fr',\n",
      "                   'load_model': 'emo_goemo_sem_fr',\n",
      "                   'name': 'GO_SEM_FR',\n",
      "                   'run': True,\n",
      "                   'run_step': 93,\n",
      "                   'save_stats': 'stats_go_sem_fr_800',\n",
      "                   'test_corpus': ['tales'],\n",
      "                   'train_corpus': ['tales'],\n",
      "                   'train_samples': [800]},\n",
      " 'GO_SEM_FR_8000': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                   '8000 samplesand transfer learning from '\n",
      "                                   'emo_goemo_sem_fr',\n",
      "                    'load_model': 'emo_goemo_sem_fr',\n",
      "                    'name': 'GO_SEM_FR',\n",
      "                    'run': True,\n",
      "                    'run_step': 127,\n",
      "                    'save_stats': 'stats_go_sem_fr_8000',\n",
      "                    'test_corpus': ['tales'],\n",
      "                    'train_corpus': ['tales'],\n",
      "                    'train_samples': [8000]},\n",
      " 'GO_SEM_FR_900': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                  '900 samplesand transfer learning from '\n",
      "                                  'emo_goemo_sem_fr',\n",
      "                   'load_model': 'emo_goemo_sem_fr',\n",
      "                   'name': 'GO_SEM_FR',\n",
      "                   'run': True,\n",
      "                   'run_step': 94,\n",
      "                   'save_stats': 'stats_go_sem_fr_900',\n",
      "                   'test_corpus': ['tales'],\n",
      "                   'train_corpus': ['tales'],\n",
      "                   'train_samples': [900]},\n",
      " 'GO_SEM_FR_ALL': {'description': \"GoEmo, SemEval and Friends  with ['tales'] \"\n",
      "                                  'ALL samples and xfer learn from '\n",
      "                                  'emo_goemo_sem_fr',\n",
      "                   'load_model': 'emo_goemo_sem_fr',\n",
      "                   'name': 'GO_SEM_FR',\n",
      "                   'run': True,\n",
      "                   'run_step': 128,\n",
      "                   'save_stats': 'stats_go_sem_fr_all',\n",
      "                   'test_corpus': ['tales'],\n",
      "                   'train_corpus': ['tales'],\n",
      "                   'train_samples': None},\n",
      " 'TALES_100': {'description': \"TALES with ['tales'] 100 samples\",\n",
      "               'load_model': None,\n",
      "               'run': True,\n",
      "               'run_step': 0,\n",
      "               'save_stats': 'stats_tales_100',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [100]},\n",
      " 'TALES_1000': {'description': \"Target with ['tales'] 1000 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 9,\n",
      "                'save_stats': 'stats_tales_1000',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1000]},\n",
      " 'TALES_1100': {'description': \"Target with ['tales'] 1100 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 10,\n",
      "                'save_stats': 'stats_tales_1100',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1100]},\n",
      " 'TALES_1200': {'description': \"Target with ['tales'] 1200 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 11,\n",
      "                'save_stats': 'stats_tales_1200',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1200]},\n",
      " 'TALES_1300': {'description': \"Target with ['tales'] 1300 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 12,\n",
      "                'save_stats': 'stats_tales_1300',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1300]},\n",
      " 'TALES_1400': {'description': \"Target with ['tales'] 1400 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 13,\n",
      "                'save_stats': 'stats_tales_1400',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1400]},\n",
      " 'TALES_1500': {'description': \"Target with ['tales'] 1500 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 14,\n",
      "                'save_stats': 'stats_tales_1500',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1500]},\n",
      " 'TALES_1600': {'description': \"Target with ['tales'] 1600 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 15,\n",
      "                'save_stats': 'stats_tales_1600',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1600]},\n",
      " 'TALES_1700': {'description': \"Target with ['tales'] 1700 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 16,\n",
      "                'save_stats': 'stats_tales_1700',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1700]},\n",
      " 'TALES_1800': {'description': \"Target with ['tales'] 1800 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 17,\n",
      "                'save_stats': 'stats_tales_1800',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1800]},\n",
      " 'TALES_1900': {'description': \"Target with ['tales'] 1900 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 18,\n",
      "                'save_stats': 'stats_tales_1900',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [1900]},\n",
      " 'TALES_200': {'description': \"TALES with ['tales'] 200 samples\",\n",
      "               'load_model': None,\n",
      "               'run': True,\n",
      "               'run_step': 1,\n",
      "               'save_stats': 'stats_tales_200',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [200]},\n",
      " 'TALES_2000': {'description': \"Target with ['tales'] 2000 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 19,\n",
      "                'save_stats': 'stats_tales_2000',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2000]},\n",
      " 'TALES_2100': {'description': \"Target with ['tales'] 2100 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 20,\n",
      "                'save_stats': 'stats_tales_2100',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2100]},\n",
      " 'TALES_2200': {'description': \"Target with ['tales'] 2200 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 21,\n",
      "                'save_stats': 'stats_tales_2200',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2200]},\n",
      " 'TALES_2300': {'description': \"Target with ['tales'] 2300 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 22,\n",
      "                'save_stats': 'stats_tales_2300',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2300]},\n",
      " 'TALES_2400': {'description': \"Target with ['tales'] 2400 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 23,\n",
      "                'save_stats': 'stats_tales_2400',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2400]},\n",
      " 'TALES_2500': {'description': \"Target with ['tales'] 2500 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 24,\n",
      "                'save_stats': 'stats_tales_2500',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2500]},\n",
      " 'TALES_2600': {'description': \"Target with ['tales'] 2600 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 25,\n",
      "                'save_stats': 'stats_tales_2600',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2600]},\n",
      " 'TALES_2700': {'description': \"Target with ['tales'] 2700 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 26,\n",
      "                'save_stats': 'stats_tales_2700',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2700]},\n",
      " 'TALES_2800': {'description': \"Target with ['tales'] 2800 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 27,\n",
      "                'save_stats': 'stats_tales_2800',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2800]},\n",
      " 'TALES_2900': {'description': \"Target with ['tales'] 2900 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 28,\n",
      "                'save_stats': 'stats_tales_2900',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [2900]},\n",
      " 'TALES_300': {'description': \"TALES with ['tales'] 300 samples\",\n",
      "               'load_model': None,\n",
      "               'run': True,\n",
      "               'run_step': 2,\n",
      "               'save_stats': 'stats_tales_300',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [300]},\n",
      " 'TALES_3000': {'description': \"Target with ['tales'] 3000 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 29,\n",
      "                'save_stats': 'stats_tales_3000',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3000]},\n",
      " 'TALES_3100': {'description': \"Target with ['tales'] 3100 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 30,\n",
      "                'save_stats': 'stats_tales_3100',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3100]},\n",
      " 'TALES_3200': {'description': \"Target with ['tales'] 3200 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 31,\n",
      "                'save_stats': 'stats_tales_3200',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3200]},\n",
      " 'TALES_3300': {'description': \"Target with ['tales'] 3300 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 32,\n",
      "                'save_stats': 'stats_tales_3300',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3300]},\n",
      " 'TALES_3400': {'description': \"Target with ['tales'] 3400 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 33,\n",
      "                'save_stats': 'stats_tales_3400',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3400]},\n",
      " 'TALES_3500': {'description': \"Target with ['tales'] 3400 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 34,\n",
      "                'save_stats': 'stats_tales_3500',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3500]},\n",
      " 'TALES_3600': {'description': \"Target with ['tales'] 3600 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 35,\n",
      "                'save_stats': 'stats_tales_3600',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3600]},\n",
      " 'TALES_3700': {'description': \"Target with ['tales'] 3700 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 36,\n",
      "                'save_stats': 'stats_tales_3700',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3700]},\n",
      " 'TALES_3800': {'description': \"Target with ['tales'] 3800 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 37,\n",
      "                'save_stats': 'stats_tales_3800',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3800]},\n",
      " 'TALES_3900': {'description': \"Target with ['tales'] 3900 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 38,\n",
      "                'save_stats': 'stats_tales_3900',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [3900]},\n",
      " 'TALES_400': {'description': \"TALES with ['tales'] 400 samples\",\n",
      "               'load_model': None,\n",
      "               'run': True,\n",
      "               'run_step': 3,\n",
      "               'save_stats': 'stats_tales_400',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [400]},\n",
      " 'TALES_4000': {'description': \"Target with ['tales'] 4000 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 39,\n",
      "                'save_stats': 'stats_tales_4000',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [4000]},\n",
      " 'TALES_5000': {'description': \"Tales with ['tales'] 5000 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 40,\n",
      "                'save_stats': 'stats_tales_5000',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [5000]},\n",
      " 'TALES_600': {'description': \"TALES with ['tales'] 600 samples\",\n",
      "               'load_model': None,\n",
      "               'run': True,\n",
      "               'run_step': 5,\n",
      "               'save_stats': 'stats_tales_600',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [600]},\n",
      " 'TALES_700': {'description': \"TALES with ['tales'] 700 samples\",\n",
      "               'load_model': None,\n",
      "               'run': True,\n",
      "               'run_step': 6,\n",
      "               'save_stats': 'stats_tales_700',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [700]},\n",
      " 'TALES_800': {'description': \"TALES with ['tales'] 800 samples\",\n",
      "               'load_model': None,\n",
      "               'run': True,\n",
      "               'run_step': 7,\n",
      "               'save_stats': 'stats_tales_800',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [800]},\n",
      " 'TALES_8000': {'description': \"Tales with ['tales'] 8000 samples\",\n",
      "                'load_model': None,\n",
      "                'name': 'TALES',\n",
      "                'run': True,\n",
      "                'run_step': 41,\n",
      "                'save_stats': 'stats_tales_8000',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [8000]},\n",
      " 'TALES_900': {'description': \"TALES with ['tales'] 900 samples\",\n",
      "               'load_model': None,\n",
      "               'run': True,\n",
      "               'run_step': 8,\n",
      "               'save_stats': 'stats_tales_900',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': [900]},\n",
      " 'TALES_ALL': {'description': \"Tales with ['tales'] ALL samples\",\n",
      "               'load_model': None,\n",
      "               'name': 'TALES',\n",
      "               'run': True,\n",
      "               'run_step': 42,\n",
      "               'save_stats': 'stats_tales_all',\n",
      "               'test_corpus': ['tales'],\n",
      "               'train_corpus': ['tales'],\n",
      "               'train_samples': None},\n",
      " 'TARGET_500': {'description': \"TARGET with ['tales'] 500 samples\",\n",
      "                'load_model': None,\n",
      "                'run': True,\n",
      "                'run_step': 4,\n",
      "                'save_stats': 'stats_target_500',\n",
      "                'test_corpus': ['tales'],\n",
      "                'train_corpus': ['tales'],\n",
      "                'train_samples': [500]}}\n"
     ]
    }
   ],
   "source": [
    "if metadata['TRIAL_RUN']:\n",
    "    trial_size = metadata['TRIAL_SIZE']\n",
    "    print ('Trial run {} experiments with Trial Size {}'.format(len(run_exp.keys()),trial_size))\n",
    "    print ('Changing epochs from {} to {} '.format(model_config['epochs'],metadata['TRIAL_EPOCHS']))\n",
    "    model_config['epochs'] = metadata['TRIAL_EPOCHS']\n",
    "    for e,c in run_exp.items():\n",
    "        if c['train_corpus']:\n",
    "            c['train_samples'] = [trial_size] * len(c['train_corpus'])\n",
    "        if c['test_corpus']:\n",
    "            c['test_samples'] = [trial_size] * len(c['test_corpus'])\n",
    "else:\n",
    "    print ('Production {} experiments'.format(len(run_exp.keys())))\n",
    "print (\"\\n**** Model_Config ****\")\n",
    "pprint.pprint(model_config)\n",
    "print (\"\\n**** Run Experiments ****\")\n",
    "pprint.pprint(list(run_exp.keys()))\n",
    "pprint.pprint(run_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL MODE: False DEVICE: cuda Experiments:174 \n"
     ]
    }
   ],
   "source": [
    "print ('TRIAL MODE: {} DEVICE: {} Experiments:{} '.format(\n",
    "       metadata['TRIAL_RUN'],device,len(run_exp.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genexp(name,desc,load_model,sizes=[0,100,200,300],train=['tales'],test=['tales'],gen_all=True):\n",
    "    exp = {}\n",
    "    for size in sizes:\n",
    "        expname = name.upper() + '_' + str(size)\n",
    "        exp[expname] = {}\n",
    "        exp[expname]['run'] = True\n",
    "        exp[expname]['name'] = name\n",
    "        exp[expname]['description'] = desc + ' with ' + str(train) + ' ' + str(size) + ' samples' \n",
    "        if load_model:\n",
    "            exp[expname]['description'] += 'and transfer learning from ' + load_model \n",
    "        exp[expname]['load_model'] = load_model\n",
    "        \n",
    "        if size == 0 :\n",
    "            exp[expname]['train_corpus'] = None\n",
    "            exp[expname]['train_samples'] = None\n",
    "        else:\n",
    "            exp[expname]['train_corpus'] = train\n",
    "            exp[expname]['train_samples'] = [size] * len(train)\n",
    "        exp[expname]['test_corpus'] = test\n",
    "        exp[expname]['save_stats'] = 'stats_' + expname.lower()\n",
    "    if gen_all:\n",
    "        expname = name.upper() + '_ALL'\n",
    "        exp[expname] = {}\n",
    "        exp[expname]['run'] = True\n",
    "        exp[expname]['name'] = name\n",
    "        exp[expname]['description'] = desc + ' with ' + str(train) + ' ALL samples'\n",
    "        if load_model:\n",
    "            exp[expname]['description'] += ' and xfer learn from ' + load_model \n",
    "        exp[expname]['load_model'] = load_model\n",
    "        exp[expname]['train_corpus'] = train\n",
    "        exp[expname]['train_samples'] = None\n",
    "        exp[expname]['test_corpus'] = test\n",
    "        exp[expname]['save_stats'] = 'stats_' + expname.lower()\n",
    "    return exp\n",
    "\n",
    "all_exp = {}\n",
    "sizes = [0,100,200,300,400,500,1000,1500,2000,2500,3000,3500,4000,4500,5000,6000,7000,8000]\n",
    "all_exp.update(genexp('TALES','Tales Only',None,gen_all=False,sizes=sizes[1:]))\n",
    "#These produced positive results\n",
    "# all_exp.update(genexp('GEMB_FR_3_5','GoEmo, Friends with embeddings and layers 3 to 5 frozen','goemb_frall_tr3_5',sizes=sizes))\n",
    "# all_exp.update(genexp('GO_SEM_FR','GoEmo, SemEval and Friends ','emo_goemo_sem_fr',sizes=sizes))\n",
    "# all_exp.update(genexp('GO_FR','GoEmo, Friends ','emo_goemo_fr',sizes=sizes))\n",
    "# all_exp.update(genexp('GO_SEM','GoEmo, SemEval ','emo_goemo_sem',sizes=sizes))\n",
    "# all_exp.update(genexp('GO','GoEmo ','emo_goemo',sizes=sizes))\n",
    "# #These produced negative results\n",
    "all_exp.update(genexp('GEMB','GoEmo with embeddings frozen','emo_goemo_emb_fr',sizes=sizes))\n",
    "all_exp.update(genexp('GEMB_SEM_FR_0_5','GoEmo, SemEval, Friends with embeddings and all layers frozen','goemb_semfrall_tr0_5',sizes=sizes))\n",
    "\n",
    "#all_exp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment TALES_100\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.5981, Completed in 0:00:01.897979 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.4478, Completed in 0:00:00.564987 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.319444\n",
      "F1 Score (micro) =   0.474227\n",
      "F1 Score (macro) =   0.091270\n",
      "F1 Score (samples) =   0.460000\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.3727, Completed in 0:00:01.686528 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3399, Completed in 0:00:00.565560 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.333333\n",
      "F1 Score (micro) =   0.500000\n",
      "F1 Score (macro) =   0.095238\n",
      "F1 Score (samples) =   0.500000\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.3104, Completed in 0:00:01.687071 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3550, Completed in 0:00:00.565189 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.333333\n",
      "F1 Score (micro) =   0.500000\n",
      "F1 Score (macro) =   0.095238\n",
      "F1 Score (samples) =   0.500000\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2778, Completed in 0:00:01.688527 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3386, Completed in 0:00:00.567379 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.333333\n",
      "F1 Score (micro) =   0.500000\n",
      "F1 Score (macro) =   0.095238\n",
      "F1 Score (samples) =   0.500000\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of    100.  Loss 0.1194  Elapsed: 0:00:00.394492.\n",
      "  Batch    80  of    100.  Loss 0.1188  Elapsed: 0:00:00.777159.\n",
      "Avg Validation Loss 0.2798, Completed in 0:00:00.959561 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.474568\n",
      "F1 Score (micro) =   0.620000\n",
      "F1 Score (macro) =   0.109347\n",
      "F1 Score (samples) =   0.620000\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.1218  Elapsed: 0:00:00.392560.\n",
      "  Batch    80  of  1,477.  Loss 0.5184  Elapsed: 0:00:00.776241.\n",
      "  Batch   120  of  1,477.  Loss 0.5101  Elapsed: 0:00:01.159389.\n",
      "  Batch   160  of  1,477.  Loss 0.1189  Elapsed: 0:00:01.546925.\n",
      "  Batch   200  of  1,477.  Loss 0.5882  Elapsed: 0:00:01.931982.\n",
      "  Batch   240  of  1,477.  Loss 0.1214  Elapsed: 0:00:02.315320.\n",
      "  Batch   280  of  1,477.  Loss 0.5133  Elapsed: 0:00:02.697262.\n",
      "  Batch   320  of  1,477.  Loss 0.1211  Elapsed: 0:00:03.079899.\n",
      "  Batch   360  of  1,477.  Loss 0.5785  Elapsed: 0:00:03.463560.\n",
      "  Batch   400  of  1,477.  Loss 0.7034  Elapsed: 0:00:03.848332.\n",
      "  Batch   440  of  1,477.  Loss 0.5044  Elapsed: 0:00:04.230673.\n",
      "  Batch   480  of  1,477.  Loss 0.5882  Elapsed: 0:00:04.615100.\n",
      "  Batch   520  of  1,477.  Loss 0.5162  Elapsed: 0:00:04.997519.\n",
      "  Batch   560  of  1,477.  Loss 0.1168  Elapsed: 0:00:05.381270.\n",
      "  Batch   600  of  1,477.  Loss 0.1230  Elapsed: 0:00:05.764810.\n",
      "  Batch   640  of  1,477.  Loss 0.1191  Elapsed: 0:00:06.147440.\n",
      "  Batch   680  of  1,477.  Loss 0.1213  Elapsed: 0:00:06.531058.\n",
      "  Batch   720  of  1,477.  Loss 0.5519  Elapsed: 0:00:06.913208.\n",
      "  Batch   760  of  1,477.  Loss 0.1228  Elapsed: 0:00:07.296752.\n",
      "  Batch   800  of  1,477.  Loss 0.1165  Elapsed: 0:00:07.679299.\n",
      "  Batch   840  of  1,477.  Loss 0.1211  Elapsed: 0:00:08.060252.\n",
      "  Batch   880  of  1,477.  Loss 0.5742  Elapsed: 0:00:08.444756.\n",
      "  Batch   920  of  1,477.  Loss 0.1142  Elapsed: 0:00:08.830296.\n",
      "  Batch   960  of  1,477.  Loss 0.1246  Elapsed: 0:00:09.214545.\n",
      "  Batch 1,000  of  1,477.  Loss 0.5642  Elapsed: 0:00:09.597523.\n",
      "  Batch 1,040  of  1,477.  Loss 0.5827  Elapsed: 0:00:09.979569.\n",
      "  Batch 1,080  of  1,477.  Loss 0.6056  Elapsed: 0:00:10.363623.\n",
      "  Batch 1,120  of  1,477.  Loss 0.5675  Elapsed: 0:00:10.745467.\n",
      "  Batch 1,160  of  1,477.  Loss 0.4963  Elapsed: 0:00:11.127678.\n",
      "  Batch 1,200  of  1,477.  Loss 0.1211  Elapsed: 0:00:11.511075.\n",
      "  Batch 1,240  of  1,477.  Loss 0.4920  Elapsed: 0:00:11.896259.\n",
      "  Batch 1,280  of  1,477.  Loss 0.7147  Elapsed: 0:00:12.279722.\n",
      "  Batch 1,320  of  1,477.  Loss 0.5980  Elapsed: 0:00:12.663571.\n",
      "  Batch 1,360  of  1,477.  Loss 0.5599  Elapsed: 0:00:13.049212.\n",
      "  Batch 1,400  of  1,477.  Loss 0.6125  Elapsed: 0:00:13.432136.\n",
      "  Batch 1,440  of  1,477.  Loss 0.4898  Elapsed: 0:00:13.814956.\n",
      "Avg Validation Loss 0.3212, Completed in 0:00:14.158968 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.386889\n",
      "F1 Score (micro) =   0.547055\n",
      "F1 Score (macro) =   0.101032\n",
      "F1 Score (samples) =   0.547055\n",
      "Running Experiment TALES_200\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.4763, Completed in 0:00:03.358012 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3702, Completed in 0:00:01.126965 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.295188\n",
      "F1 Score (micro) =   0.465000\n",
      "F1 Score (macro) =   0.090687\n",
      "F1 Score (samples) =   0.465000\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2751, Completed in 0:00:03.358006 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3750, Completed in 0:00:01.127495 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.295188\n",
      "F1 Score (micro) =   0.465000\n",
      "F1 Score (macro) =   0.090687\n",
      "F1 Score (samples) =   0.465000\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2580, Completed in 0:00:03.357854 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3828, Completed in 0:00:01.128015 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.295188\n",
      "F1 Score (micro) =   0.465000\n",
      "F1 Score (macro) =   0.090687\n",
      "F1 Score (samples) =   0.465000\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2432, Completed in 0:00:03.359524 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3574, Completed in 0:00:01.129866 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.302332\n",
      "F1 Score (micro) =   0.471795\n",
      "F1 Score (macro) =   0.092882\n",
      "F1 Score (samples) =   0.460000\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of    200.  Loss 0.0702  Elapsed: 0:00:00.397129.\n",
      "  Batch    80  of    200.  Loss 0.0758  Elapsed: 0:00:00.785398.\n",
      "  Batch   120  of    200.  Loss 0.0757  Elapsed: 0:00:01.172290.\n",
      "  Batch   160  of    200.  Loss 0.7379  Elapsed: 0:00:01.560349.\n",
      "Avg Validation Loss 0.2252, Completed in 0:00:01.938818 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.585325\n",
      "F1 Score (micro) =   0.711688\n",
      "F1 Score (macro) =   0.121185\n",
      "F1 Score (samples) =   0.685000\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.4755  Elapsed: 0:00:00.399719.\n",
      "  Batch    80  of  1,477.  Loss 0.6497  Elapsed: 0:00:00.788563.\n",
      "  Batch   120  of  1,477.  Loss 0.5818  Elapsed: 0:00:01.181392.\n",
      "  Batch   160  of  1,477.  Loss 0.1158  Elapsed: 0:00:01.573055.\n",
      "  Batch   200  of  1,477.  Loss 0.5435  Elapsed: 0:00:01.964190.\n",
      "  Batch   240  of  1,477.  Loss 0.0750  Elapsed: 0:00:02.358212.\n",
      "  Batch   280  of  1,477.  Loss 0.1245  Elapsed: 0:00:02.748863.\n",
      "  Batch   320  of  1,477.  Loss 0.1258  Elapsed: 0:00:03.140822.\n",
      "  Batch   360  of  1,477.  Loss 0.5462  Elapsed: 0:00:03.528394.\n",
      "  Batch   400  of  1,477.  Loss 0.4270  Elapsed: 0:00:03.920325.\n",
      "  Batch   440  of  1,477.  Loss 0.4688  Elapsed: 0:00:04.309932.\n",
      "  Batch   480  of  1,477.  Loss 0.5853  Elapsed: 0:00:04.699019.\n",
      "  Batch   520  of  1,477.  Loss 0.4728  Elapsed: 0:00:05.087055.\n",
      "  Batch   560  of  1,477.  Loss 0.4142  Elapsed: 0:00:05.473267.\n",
      "  Batch   600  of  1,477.  Loss 0.4990  Elapsed: 0:00:05.864329.\n",
      "  Batch   640  of  1,477.  Loss 0.6172  Elapsed: 0:00:06.253593.\n",
      "  Batch   680  of  1,477.  Loss 0.6057  Elapsed: 0:00:06.644918.\n",
      "  Batch   720  of  1,477.  Loss 0.1495  Elapsed: 0:00:07.036530.\n",
      "  Batch   760  of  1,477.  Loss 0.0887  Elapsed: 0:00:07.425728.\n",
      "  Batch   800  of  1,477.  Loss 0.1389  Elapsed: 0:00:07.815296.\n",
      "  Batch   840  of  1,477.  Loss 0.7556  Elapsed: 0:00:08.205418.\n",
      "  Batch   880  of  1,477.  Loss 0.5170  Elapsed: 0:00:08.596549.\n",
      "  Batch   920  of  1,477.  Loss 0.6303  Elapsed: 0:00:08.985644.\n",
      "  Batch   960  of  1,477.  Loss 0.6835  Elapsed: 0:00:09.377267.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0761  Elapsed: 0:00:09.766157.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0849  Elapsed: 0:00:10.153911.\n",
      "  Batch 1,080  of  1,477.  Loss 0.5463  Elapsed: 0:00:10.543018.\n",
      "  Batch 1,120  of  1,477.  Loss 0.7428  Elapsed: 0:00:10.930703.\n",
      "  Batch 1,160  of  1,477.  Loss 0.6641  Elapsed: 0:00:11.320970.\n",
      "  Batch 1,200  of  1,477.  Loss 0.1627  Elapsed: 0:00:11.711256.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0734  Elapsed: 0:00:12.100274.\n",
      "  Batch 1,280  of  1,477.  Loss 0.7053  Elapsed: 0:00:12.489690.\n",
      "  Batch 1,320  of  1,477.  Loss 0.1447  Elapsed: 0:00:12.879802.\n",
      "  Batch 1,360  of  1,477.  Loss 0.5991  Elapsed: 0:00:13.267346.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0829  Elapsed: 0:00:13.654622.\n",
      "  Batch 1,440  of  1,477.  Loss 0.1092  Elapsed: 0:00:14.044836.\n",
      "Avg Validation Loss 0.3264, Completed in 0:00:14.392927 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.390291\n",
      "F1 Score (micro) =   0.546788\n",
      "F1 Score (macro) =   0.101920\n",
      "F1 Score (samples) =   0.530129\n",
      "Running Experiment TALES_300\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.3848, Completed in 0:00:05.029010 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3647, Completed in 0:00:01.688638 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.304133\n",
      "F1 Score (micro) =   0.473333\n",
      "F1 Score (macro) =   0.091791\n",
      "F1 Score (samples) =   0.473333\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2627, Completed in 0:00:05.028846 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3528, Completed in 0:00:01.690382 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.304133\n",
      "F1 Score (micro) =   0.473333\n",
      "F1 Score (macro) =   0.091791\n",
      "F1 Score (samples) =   0.473333\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2524, Completed in 0:00:05.034804 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3637, Completed in 0:00:01.688159 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.304133\n",
      "F1 Score (micro) =   0.473333\n",
      "F1 Score (macro) =   0.091791\n",
      "F1 Score (samples) =   0.473333\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2319, Completed in 0:00:05.031746 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3255, Completed in 0:00:01.688054 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.306851\n",
      "F1 Score (micro) =   0.419643\n",
      "F1 Score (macro) =   0.092611\n",
      "F1 Score (samples) =   0.313333\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of    300.  Loss 0.2887  Elapsed: 0:00:00.401881.\n",
      "  Batch    80  of    300.  Loss 0.2925  Elapsed: 0:00:00.789332.\n",
      "  Batch   120  of    300.  Loss 0.0803  Elapsed: 0:00:01.177478.\n",
      "  Batch   160  of    300.  Loss 0.0588  Elapsed: 0:00:01.568548.\n",
      "  Batch   200  of    300.  Loss 0.0983  Elapsed: 0:00:01.956542.\n",
      "  Batch   240  of    300.  Loss 0.0758  Elapsed: 0:00:02.343299.\n",
      "  Batch   280  of    300.  Loss 0.0683  Elapsed: 0:00:02.732492.\n",
      "Avg Validation Loss 0.1894, Completed in 0:00:02.917817 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.634523\n",
      "F1 Score (micro) =   0.745418\n",
      "F1 Score (macro) =   0.131371\n",
      "F1 Score (samples) =   0.610000\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.1020  Elapsed: 0:00:00.398611.\n",
      "  Batch    80  of  1,477.  Loss 0.3119  Elapsed: 0:00:00.787199.\n",
      "  Batch   120  of  1,477.  Loss 0.2771  Elapsed: 0:00:01.175389.\n",
      "  Batch   160  of  1,477.  Loss 0.5875  Elapsed: 0:00:01.563910.\n",
      "  Batch   200  of  1,477.  Loss 0.0612  Elapsed: 0:00:01.954070.\n",
      "  Batch   240  of  1,477.  Loss 0.4531  Elapsed: 0:00:02.344592.\n",
      "  Batch   280  of  1,477.  Loss 0.4944  Elapsed: 0:00:02.731255.\n",
      "  Batch   320  of  1,477.  Loss 0.4230  Elapsed: 0:00:03.121370.\n",
      "  Batch   360  of  1,477.  Loss 0.1197  Elapsed: 0:00:03.511374.\n",
      "  Batch   400  of  1,477.  Loss 0.0575  Elapsed: 0:00:03.902925.\n",
      "  Batch   440  of  1,477.  Loss 0.0543  Elapsed: 0:00:04.290220.\n",
      "  Batch   480  of  1,477.  Loss 0.0529  Elapsed: 0:00:04.679391.\n",
      "  Batch   520  of  1,477.  Loss 0.1463  Elapsed: 0:00:05.068220.\n",
      "  Batch   560  of  1,477.  Loss 0.1417  Elapsed: 0:00:05.455666.\n",
      "  Batch   600  of  1,477.  Loss 0.1090  Elapsed: 0:00:05.845175.\n",
      "  Batch   640  of  1,477.  Loss 0.0703  Elapsed: 0:00:06.231759.\n",
      "  Batch   680  of  1,477.  Loss 0.0559  Elapsed: 0:00:06.622284.\n",
      "  Batch   720  of  1,477.  Loss 0.7467  Elapsed: 0:00:07.010291.\n",
      "  Batch   760  of  1,477.  Loss 0.1977  Elapsed: 0:00:07.398754.\n",
      "  Batch   800  of  1,477.  Loss 0.1155  Elapsed: 0:00:07.788458.\n",
      "  Batch   840  of  1,477.  Loss 0.4461  Elapsed: 0:00:08.177101.\n",
      "  Batch   880  of  1,477.  Loss 0.3939  Elapsed: 0:00:08.566301.\n",
      "  Batch   920  of  1,477.  Loss 0.3070  Elapsed: 0:00:08.957400.\n",
      "  Batch   960  of  1,477.  Loss 0.2734  Elapsed: 0:00:09.346968.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0974  Elapsed: 0:00:09.736176.\n",
      "  Batch 1,040  of  1,477.  Loss 0.2554  Elapsed: 0:00:10.125373.\n",
      "  Batch 1,080  of  1,477.  Loss 0.4214  Elapsed: 0:00:10.512104.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0733  Elapsed: 0:00:10.900161.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0727  Elapsed: 0:00:11.288316.\n",
      "  Batch 1,200  of  1,477.  Loss 0.4861  Elapsed: 0:00:11.675867.\n",
      "  Batch 1,240  of  1,477.  Loss 0.2759  Elapsed: 0:00:12.062820.\n",
      "  Batch 1,280  of  1,477.  Loss 0.5112  Elapsed: 0:00:12.453533.\n",
      "  Batch 1,320  of  1,477.  Loss 0.6478  Elapsed: 0:00:12.840301.\n",
      "  Batch 1,360  of  1,477.  Loss 0.2219  Elapsed: 0:00:13.229605.\n",
      "  Batch 1,400  of  1,477.  Loss 0.4388  Elapsed: 0:00:13.615058.\n",
      "  Batch 1,440  of  1,477.  Loss 0.1424  Elapsed: 0:00:14.001931.\n",
      "Avg Validation Loss 0.3042, Completed in 0:00:14.352047 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.354020\n",
      "F1 Score (micro) =   0.447720\n",
      "F1 Score (macro) =   0.092448\n",
      "F1 Score (samples) =   0.329045\n",
      "Running Experiment TALES_400\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.3950, Completed in 0:00:06.701606 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3270, Completed in 0:00:02.262250 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.358629\n",
      "F1 Score (micro) =   0.522500\n",
      "F1 Score (macro) =   0.098053\n",
      "F1 Score (samples) =   0.522500\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2772, Completed in 0:00:06.694697 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3408, Completed in 0:00:02.250183 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.358629\n",
      "F1 Score (micro) =   0.522500\n",
      "F1 Score (macro) =   0.098053\n",
      "F1 Score (samples) =   0.522500\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2656, Completed in 0:00:06.692678 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3138, Completed in 0:00:02.252019 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.384560\n",
      "F1 Score (micro) =   0.532562\n",
      "F1 Score (macro) =   0.105143\n",
      "F1 Score (samples) =   0.460000\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2280, Completed in 0:00:06.691573 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3037, Completed in 0:00:02.250305 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.367444\n",
      "F1 Score (micro) =   0.476351\n",
      "F1 Score (macro) =   0.100463\n",
      "F1 Score (samples) =   0.352500\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of    400.  Loss 0.0477  Elapsed: 0:00:00.396943.\n",
      "  Batch    80  of    400.  Loss 0.5359  Elapsed: 0:00:00.782329.\n",
      "  Batch   120  of    400.  Loss 0.0469  Elapsed: 0:00:01.166766.\n",
      "  Batch   160  of    400.  Loss 0.0566  Elapsed: 0:00:01.551596.\n",
      "  Batch   200  of    400.  Loss 0.0651  Elapsed: 0:00:01.938082.\n",
      "  Batch   240  of    400.  Loss 0.4623  Elapsed: 0:00:02.323255.\n",
      "  Batch   280  of    400.  Loss 0.2550  Elapsed: 0:00:02.707101.\n",
      "  Batch   320  of    400.  Loss 0.3850  Elapsed: 0:00:03.092277.\n",
      "  Batch   360  of    400.  Loss 0.0986  Elapsed: 0:00:03.476775.\n",
      "Avg Validation Loss 0.1828, Completed in 0:00:03.851182 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.638290\n",
      "F1 Score (micro) =   0.755224\n",
      "F1 Score (macro) =   0.160810\n",
      "F1 Score (samples) =   0.632500\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0827  Elapsed: 0:00:00.400103.\n",
      "  Batch    80  of  1,477.  Loss 0.2444  Elapsed: 0:00:00.789721.\n",
      "  Batch   120  of  1,477.  Loss 0.4316  Elapsed: 0:00:01.183242.\n",
      "  Batch   160  of  1,477.  Loss 0.0521  Elapsed: 0:00:01.571015.\n",
      "  Batch   200  of  1,477.  Loss 0.0600  Elapsed: 0:00:01.959680.\n",
      "  Batch   240  of  1,477.  Loss 0.4722  Elapsed: 0:00:02.350555.\n",
      "  Batch   280  of  1,477.  Loss 0.0684  Elapsed: 0:00:02.738543.\n",
      "  Batch   320  of  1,477.  Loss 0.2073  Elapsed: 0:00:03.125789.\n",
      "  Batch   360  of  1,477.  Loss 0.7092  Elapsed: 0:00:03.514757.\n",
      "  Batch   400  of  1,477.  Loss 0.1259  Elapsed: 0:00:03.902265.\n",
      "  Batch   440  of  1,477.  Loss 0.3145  Elapsed: 0:00:04.291030.\n",
      "  Batch   480  of  1,477.  Loss 0.1445  Elapsed: 0:00:04.680851.\n",
      "  Batch   520  of  1,477.  Loss 0.5584  Elapsed: 0:00:05.067343.\n",
      "  Batch   560  of  1,477.  Loss 0.6509  Elapsed: 0:00:05.456443.\n",
      "  Batch   600  of  1,477.  Loss 0.0803  Elapsed: 0:00:05.845766.\n",
      "  Batch   640  of  1,477.  Loss 0.5735  Elapsed: 0:00:06.234517.\n",
      "  Batch   680  of  1,477.  Loss 0.6773  Elapsed: 0:00:06.621446.\n",
      "  Batch   720  of  1,477.  Loss 0.2415  Elapsed: 0:00:07.009798.\n",
      "  Batch   760  of  1,477.  Loss 0.1764  Elapsed: 0:00:07.397569.\n",
      "  Batch   800  of  1,477.  Loss 0.6260  Elapsed: 0:00:07.787795.\n",
      "  Batch   840  of  1,477.  Loss 0.0970  Elapsed: 0:00:08.178771.\n",
      "  Batch   880  of  1,477.  Loss 0.3413  Elapsed: 0:00:08.570040.\n",
      "  Batch   920  of  1,477.  Loss 0.1436  Elapsed: 0:00:08.960016.\n",
      "  Batch   960  of  1,477.  Loss 0.0508  Elapsed: 0:00:09.345992.\n",
      "  Batch 1,000  of  1,477.  Loss 0.7688  Elapsed: 0:00:09.733447.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0542  Elapsed: 0:00:10.123458.\n",
      "  Batch 1,080  of  1,477.  Loss 0.3663  Elapsed: 0:00:10.512759.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0843  Elapsed: 0:00:10.900459.\n",
      "  Batch 1,160  of  1,477.  Loss 0.3956  Elapsed: 0:00:11.291044.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0575  Elapsed: 0:00:11.678681.\n",
      "  Batch 1,240  of  1,477.  Loss 0.1063  Elapsed: 0:00:12.069202.\n",
      "  Batch 1,280  of  1,477.  Loss 0.1465  Elapsed: 0:00:12.458372.\n",
      "  Batch 1,320  of  1,477.  Loss 0.7089  Elapsed: 0:00:12.845722.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0496  Elapsed: 0:00:13.234023.\n",
      "  Batch 1,400  of  1,477.  Loss 0.5637  Elapsed: 0:00:13.623250.\n",
      "  Batch 1,440  of  1,477.  Loss 0.3737  Elapsed: 0:00:14.012216.\n",
      "Avg Validation Loss 0.3213, Completed in 0:00:14.361482 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.357223\n",
      "F1 Score (micro) =   0.450252\n",
      "F1 Score (macro) =   0.095233\n",
      "F1 Score (samples) =   0.332431\n",
      "Running Experiment TALES_500\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.3854, Completed in 0:00:08.380755 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3404, Completed in 0:00:02.816556 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.331113\n",
      "F1 Score (micro) =   0.498000\n",
      "F1 Score (macro) =   0.094984\n",
      "F1 Score (samples) =   0.498000\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2872, Completed in 0:00:08.380679 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3186, Completed in 0:00:02.814858 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.308286\n",
      "F1 Score (micro) =   0.401122\n",
      "F1 Score (macro) =   0.088435\n",
      "F1 Score (samples) =   0.286000\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2608, Completed in 0:00:08.377524 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3270, Completed in 0:00:02.812965 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.299960\n",
      "F1 Score (micro) =   0.354839\n",
      "F1 Score (macro) =   0.106366\n",
      "F1 Score (samples) =   0.242000\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2101, Completed in 0:00:08.372683 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3272, Completed in 0:00:02.813488 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.364703\n",
      "F1 Score (micro) =   0.483709\n",
      "F1 Score (macro) =   0.116094\n",
      "F1 Score (samples) =   0.386000\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of    500.  Loss 0.0360  Elapsed: 0:00:00.396787.\n",
      "  Batch    80  of    500.  Loss 0.0363  Elapsed: 0:00:00.782510.\n",
      "  Batch   120  of    500.  Loss 0.2913  Elapsed: 0:00:01.168497.\n",
      "  Batch   160  of    500.  Loss 0.0362  Elapsed: 0:00:01.555124.\n",
      "  Batch   200  of    500.  Loss 0.1655  Elapsed: 0:00:01.940450.\n",
      "  Batch   240  of    500.  Loss 0.1516  Elapsed: 0:00:02.327384.\n",
      "  Batch   280  of    500.  Loss 0.0385  Elapsed: 0:00:02.711777.\n",
      "  Batch   320  of    500.  Loss 0.2166  Elapsed: 0:00:03.098598.\n",
      "  Batch   360  of    500.  Loss 0.0369  Elapsed: 0:00:03.481550.\n",
      "  Batch   400  of    500.  Loss 0.0383  Elapsed: 0:00:03.868277.\n",
      "  Batch   440  of    500.  Loss 0.5930  Elapsed: 0:00:04.252461.\n",
      "  Batch   480  of    500.  Loss 0.0394  Elapsed: 0:00:04.637611.\n",
      "Avg Validation Loss 0.1656, Completed in 0:00:04.820782 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.644784\n",
      "F1 Score (micro) =   0.756303\n",
      "F1 Score (macro) =   0.206131\n",
      "F1 Score (samples) =   0.630000\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.5882  Elapsed: 0:00:00.397179.\n",
      "  Batch    80  of  1,477.  Loss 0.2419  Elapsed: 0:00:00.784812.\n",
      "  Batch   120  of  1,477.  Loss 0.4263  Elapsed: 0:00:01.171642.\n",
      "  Batch   160  of  1,477.  Loss 0.3795  Elapsed: 0:00:01.558456.\n",
      "  Batch   200  of  1,477.  Loss 0.4587  Elapsed: 0:00:01.945463.\n",
      "  Batch   240  of  1,477.  Loss 0.0979  Elapsed: 0:00:02.334372.\n",
      "  Batch   280  of  1,477.  Loss 0.0367  Elapsed: 0:00:02.720767.\n",
      "  Batch   320  of  1,477.  Loss 0.3510  Elapsed: 0:00:03.108677.\n",
      "  Batch   360  of  1,477.  Loss 0.1413  Elapsed: 0:00:03.495348.\n",
      "  Batch   400  of  1,477.  Loss 0.0408  Elapsed: 0:00:03.883346.\n",
      "  Batch   440  of  1,477.  Loss 0.3004  Elapsed: 0:00:04.269244.\n",
      "  Batch   480  of  1,477.  Loss 0.0413  Elapsed: 0:00:04.655714.\n",
      "  Batch   520  of  1,477.  Loss 0.7246  Elapsed: 0:00:05.042352.\n",
      "  Batch   560  of  1,477.  Loss 0.0364  Elapsed: 0:00:05.430698.\n",
      "  Batch   600  of  1,477.  Loss 0.4527  Elapsed: 0:00:05.819320.\n",
      "  Batch   640  of  1,477.  Loss 0.4188  Elapsed: 0:00:06.206116.\n",
      "  Batch   680  of  1,477.  Loss 0.0396  Elapsed: 0:00:06.593697.\n",
      "  Batch   720  of  1,477.  Loss 0.3099  Elapsed: 0:00:06.977604.\n",
      "  Batch   760  of  1,477.  Loss 0.0395  Elapsed: 0:00:07.362477.\n",
      "  Batch   800  of  1,477.  Loss 0.8243  Elapsed: 0:00:07.750104.\n",
      "  Batch   840  of  1,477.  Loss 0.4500  Elapsed: 0:00:08.135447.\n",
      "  Batch   880  of  1,477.  Loss 0.2860  Elapsed: 0:00:08.519693.\n",
      "  Batch   920  of  1,477.  Loss 0.5665  Elapsed: 0:00:08.907013.\n",
      "  Batch   960  of  1,477.  Loss 0.0404  Elapsed: 0:00:09.292385.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0549  Elapsed: 0:00:09.679343.\n",
      "  Batch 1,040  of  1,477.  Loss 0.3249  Elapsed: 0:00:10.063883.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0367  Elapsed: 0:00:10.448921.\n",
      "  Batch 1,120  of  1,477.  Loss 0.9131  Elapsed: 0:00:10.835574.\n",
      "  Batch 1,160  of  1,477.  Loss 0.4048  Elapsed: 0:00:11.218849.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0582  Elapsed: 0:00:11.603859.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0453  Elapsed: 0:00:11.989948.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0436  Elapsed: 0:00:12.376488.\n",
      "  Batch 1,320  of  1,477.  Loss 0.4484  Elapsed: 0:00:12.762374.\n",
      "  Batch 1,360  of  1,477.  Loss 0.1141  Elapsed: 0:00:13.147778.\n",
      "  Batch 1,400  of  1,477.  Loss 0.2023  Elapsed: 0:00:13.533702.\n",
      "  Batch 1,440  of  1,477.  Loss 0.5788  Elapsed: 0:00:13.921620.\n",
      "Avg Validation Loss 0.3069, Completed in 0:00:14.268271 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.402640\n",
      "F1 Score (micro) =   0.511291\n",
      "F1 Score (macro) =   0.117141\n",
      "F1 Score (samples) =   0.406229\n",
      "Running Experiment TALES_1000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.3428, Completed in 0:00:16.731458 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     63.  Loss 0.3778  Elapsed: 0:00:03.693174.\n",
      "Avg Validation Loss 0.3385, Completed in 0:00:05.631630 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.331344\n",
      "F1 Score (micro) =   0.496951\n",
      "F1 Score (macro) =   0.095433\n",
      "F1 Score (samples) =   0.489000\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2749, Completed in 0:00:16.728739 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     63.  Loss 0.2919  Elapsed: 0:00:03.699171.\n",
      "Avg Validation Loss 0.3026, Completed in 0:00:05.631347 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.372343\n",
      "F1 Score (micro) =   0.512195\n",
      "F1 Score (macro) =   0.112419\n",
      "F1 Score (samples) =   0.420000\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2234, Completed in 0:00:16.738061 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     63.  Loss 0.2854  Elapsed: 0:00:03.692265.\n",
      "Avg Validation Loss 0.2972, Completed in 0:00:05.628234 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.456169\n",
      "F1 Score (micro) =   0.536555\n",
      "F1 Score (macro) =   0.224421\n",
      "F1 Score (samples) =   0.433000\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.1554, Completed in 0:00:16.737004 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     63.  Loss 0.3972  Elapsed: 0:00:03.693103.\n",
      "Avg Validation Loss 0.3314, Completed in 0:00:05.697378 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.416212\n",
      "F1 Score (micro) =   0.460630\n",
      "F1 Score (macro) =   0.244382\n",
      "F1 Score (samples) =   0.351000\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,000.  Loss 0.3286  Elapsed: 0:00:00.396699.\n",
      "  Batch    80  of  1,000.  Loss 0.6593  Elapsed: 0:00:00.785738.\n",
      "  Batch   120  of  1,000.  Loss 0.0217  Elapsed: 0:00:01.169371.\n",
      "  Batch   160  of  1,000.  Loss 0.2571  Elapsed: 0:00:01.556780.\n",
      "  Batch   200  of  1,000.  Loss 0.0208  Elapsed: 0:00:01.944565.\n",
      "  Batch   240  of  1,000.  Loss 0.0221  Elapsed: 0:00:02.329758.\n",
      "  Batch   280  of  1,000.  Loss 0.0553  Elapsed: 0:00:02.715001.\n",
      "  Batch   320  of  1,000.  Loss 0.2263  Elapsed: 0:00:03.100025.\n",
      "  Batch   360  of  1,000.  Loss 0.0266  Elapsed: 0:00:03.487946.\n",
      "  Batch   400  of  1,000.  Loss 0.0386  Elapsed: 0:00:03.872558.\n",
      "  Batch   440  of  1,000.  Loss 0.1314  Elapsed: 0:00:04.259628.\n",
      "  Batch   480  of  1,000.  Loss 0.0419  Elapsed: 0:00:04.646033.\n",
      "  Batch   520  of  1,000.  Loss 0.0709  Elapsed: 0:00:05.033080.\n",
      "  Batch   560  of  1,000.  Loss 0.3353  Elapsed: 0:00:05.418280.\n",
      "  Batch   600  of  1,000.  Loss 0.1538  Elapsed: 0:00:05.804539.\n",
      "  Batch   640  of  1,000.  Loss 0.0348  Elapsed: 0:00:06.189047.\n",
      "  Batch   680  of  1,000.  Loss 0.0177  Elapsed: 0:00:06.574837.\n",
      "  Batch   720  of  1,000.  Loss 0.1156  Elapsed: 0:00:06.958232.\n",
      "  Batch   760  of  1,000.  Loss 0.0331  Elapsed: 0:00:07.344307.\n",
      "  Batch   800  of  1,000.  Loss 0.0173  Elapsed: 0:00:07.729171.\n",
      "  Batch   840  of  1,000.  Loss 0.0216  Elapsed: 0:00:08.114233.\n",
      "  Batch   880  of  1,000.  Loss 0.0348  Elapsed: 0:00:08.500512.\n",
      "  Batch   920  of  1,000.  Loss 0.0659  Elapsed: 0:00:08.884865.\n",
      "  Batch   960  of  1,000.  Loss 0.0201  Elapsed: 0:00:09.269946.\n",
      "Avg Validation Loss 0.1153, Completed in 0:00:09.646412 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.812612\n",
      "F1 Score (micro) =   0.863687\n",
      "F1 Score (macro) =   0.494118\n",
      "F1 Score (samples) =   0.772667\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.1091  Elapsed: 0:00:00.399234.\n",
      "  Batch    80  of  1,477.  Loss 0.1391  Elapsed: 0:00:00.784871.\n",
      "  Batch   120  of  1,477.  Loss 0.5369  Elapsed: 0:00:01.172081.\n",
      "  Batch   160  of  1,477.  Loss 0.5943  Elapsed: 0:00:01.560347.\n",
      "  Batch   200  of  1,477.  Loss 0.6923  Elapsed: 0:00:01.947706.\n",
      "  Batch   240  of  1,477.  Loss 0.7526  Elapsed: 0:00:02.335195.\n",
      "  Batch   280  of  1,477.  Loss 0.3065  Elapsed: 0:00:02.721159.\n",
      "  Batch   320  of  1,477.  Loss 0.4502  Elapsed: 0:00:03.106875.\n",
      "  Batch   360  of  1,477.  Loss 0.6888  Elapsed: 0:00:03.491936.\n",
      "  Batch   400  of  1,477.  Loss 0.4500  Elapsed: 0:00:03.876759.\n",
      "  Batch   440  of  1,477.  Loss 0.1538  Elapsed: 0:00:04.261360.\n",
      "  Batch   480  of  1,477.  Loss 0.2628  Elapsed: 0:00:04.649021.\n",
      "  Batch   520  of  1,477.  Loss 0.0274  Elapsed: 0:00:05.034160.\n",
      "  Batch   560  of  1,477.  Loss 0.6995  Elapsed: 0:00:05.417615.\n",
      "  Batch   600  of  1,477.  Loss 0.0270  Elapsed: 0:00:05.802811.\n",
      "  Batch   640  of  1,477.  Loss 0.0701  Elapsed: 0:00:06.187011.\n",
      "  Batch   680  of  1,477.  Loss 0.1033  Elapsed: 0:00:06.575059.\n",
      "  Batch   720  of  1,477.  Loss 0.1899  Elapsed: 0:00:06.960881.\n",
      "  Batch   760  of  1,477.  Loss 0.3244  Elapsed: 0:00:07.345852.\n",
      "  Batch   800  of  1,477.  Loss 0.6180  Elapsed: 0:00:07.734187.\n",
      "  Batch   840  of  1,477.  Loss 0.0544  Elapsed: 0:00:08.119053.\n",
      "  Batch   880  of  1,477.  Loss 0.0666  Elapsed: 0:00:08.505101.\n",
      "  Batch   920  of  1,477.  Loss 0.3032  Elapsed: 0:00:08.891068.\n",
      "  Batch   960  of  1,477.  Loss 0.3340  Elapsed: 0:00:09.274443.\n",
      "  Batch 1,000  of  1,477.  Loss 0.2590  Elapsed: 0:00:09.658401.\n",
      "  Batch 1,040  of  1,477.  Loss 0.7395  Elapsed: 0:00:10.045472.\n",
      "  Batch 1,080  of  1,477.  Loss 0.6650  Elapsed: 0:00:10.430362.\n",
      "  Batch 1,120  of  1,477.  Loss 0.1273  Elapsed: 0:00:10.817828.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0575  Elapsed: 0:00:11.202907.\n",
      "  Batch 1,200  of  1,477.  Loss 0.1870  Elapsed: 0:00:11.587724.\n",
      "  Batch 1,240  of  1,477.  Loss 0.2881  Elapsed: 0:00:11.975283.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0311  Elapsed: 0:00:12.361796.\n",
      "  Batch 1,320  of  1,477.  Loss 0.1012  Elapsed: 0:00:12.746620.\n",
      "  Batch 1,360  of  1,477.  Loss 0.5374  Elapsed: 0:00:13.133654.\n",
      "  Batch 1,400  of  1,477.  Loss 0.2021  Elapsed: 0:00:13.521887.\n",
      "  Batch 1,440  of  1,477.  Loss 0.3700  Elapsed: 0:00:13.908428.\n",
      "Avg Validation Loss 0.3328, Completed in 0:00:14.256678 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.420584\n",
      "F1 Score (micro) =   0.457799\n",
      "F1 Score (macro) =   0.236151\n",
      "F1 Score (samples) =   0.350485\n",
      "Running Experiment TALES_1500\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.3102, Completed in 0:00:25.095426 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3796  Elapsed: 0:00:03.683812.\n",
      "  Batch    80  of     93.  Loss 0.2763  Elapsed: 0:00:07.287912.\n",
      "Avg Validation Loss 0.3240, Completed in 0:00:08.302519 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.335240\n",
      "F1 Score (micro) =   0.454267\n",
      "F1 Score (macro) =   0.095978\n",
      "F1 Score (samples) =   0.351388\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2537, Completed in 0:00:25.115414 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3129  Elapsed: 0:00:03.684874.\n",
      "  Batch    80  of     93.  Loss 0.2880  Elapsed: 0:00:07.288463.\n",
      "Avg Validation Loss 0.3097, Completed in 0:00:08.307419 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.452761\n",
      "F1 Score (micro) =   0.566182\n",
      "F1 Score (macro) =   0.180830\n",
      "F1 Score (samples) =   0.498984\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2014, Completed in 0:00:25.098729 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3295  Elapsed: 0:00:03.692235.\n",
      "  Batch    80  of     93.  Loss 0.3124  Elapsed: 0:00:07.284441.\n",
      "Avg Validation Loss 0.2905, Completed in 0:00:08.304740 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.463458\n",
      "F1 Score (micro) =   0.551188\n",
      "F1 Score (macro) =   0.200635\n",
      "F1 Score (samples) =   0.455428\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.1401, Completed in 0:00:25.120489 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3515  Elapsed: 0:00:03.692370.\n",
      "  Batch    80  of     93.  Loss 0.2797  Elapsed: 0:00:07.294460.\n",
      "Avg Validation Loss 0.3207, Completed in 0:00:08.313068 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.473289\n",
      "F1 Score (micro) =   0.514985\n",
      "F1 Score (macro) =   0.270413\n",
      "F1 Score (samples) =   0.412999\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,500.  Loss 0.3278  Elapsed: 0:00:00.396456.\n",
      "  Batch    80  of  1,500.  Loss 0.1209  Elapsed: 0:00:00.781864.\n",
      "  Batch   120  of  1,500.  Loss 0.0175  Elapsed: 0:00:01.166601.\n",
      "  Batch   160  of  1,500.  Loss 0.0244  Elapsed: 0:00:01.551046.\n",
      "  Batch   200  of  1,500.  Loss 0.0273  Elapsed: 0:00:01.938483.\n",
      "  Batch   240  of  1,500.  Loss 0.0945  Elapsed: 0:00:02.324296.\n",
      "  Batch   280  of  1,500.  Loss 0.3631  Elapsed: 0:00:02.709522.\n",
      "  Batch   320  of  1,500.  Loss 0.0190  Elapsed: 0:00:03.097330.\n",
      "  Batch   360  of  1,500.  Loss 0.0165  Elapsed: 0:00:03.484554.\n",
      "  Batch   400  of  1,500.  Loss 0.0452  Elapsed: 0:00:03.869112.\n",
      "  Batch   440  of  1,500.  Loss 0.0136  Elapsed: 0:00:04.252898.\n",
      "  Batch   480  of  1,500.  Loss 0.0576  Elapsed: 0:00:04.640377.\n",
      "  Batch   520  of  1,500.  Loss 0.3558  Elapsed: 0:00:05.025441.\n",
      "  Batch   560  of  1,500.  Loss 0.0148  Elapsed: 0:00:05.409156.\n",
      "  Batch   600  of  1,500.  Loss 0.3777  Elapsed: 0:00:05.795645.\n",
      "  Batch   640  of  1,500.  Loss 0.3217  Elapsed: 0:00:06.183071.\n",
      "  Batch   680  of  1,500.  Loss 0.0585  Elapsed: 0:00:06.568315.\n",
      "  Batch   720  of  1,500.  Loss 0.0317  Elapsed: 0:00:06.953344.\n",
      "  Batch   760  of  1,500.  Loss 0.0299  Elapsed: 0:00:07.341324.\n",
      "  Batch   800  of  1,500.  Loss 0.0155  Elapsed: 0:00:07.727257.\n",
      "  Batch   840  of  1,500.  Loss 0.0532  Elapsed: 0:00:08.112853.\n",
      "  Batch   880  of  1,500.  Loss 0.0159  Elapsed: 0:00:08.496690.\n",
      "  Batch   920  of  1,500.  Loss 0.0802  Elapsed: 0:00:08.884116.\n",
      "  Batch   960  of  1,500.  Loss 0.0144  Elapsed: 0:00:09.269211.\n",
      "  Batch 1,000  of  1,500.  Loss 0.4480  Elapsed: 0:00:09.655672.\n",
      "  Batch 1,040  of  1,500.  Loss 0.0201  Elapsed: 0:00:10.040264.\n",
      "  Batch 1,080  of  1,500.  Loss 0.0473  Elapsed: 0:00:10.427312.\n",
      "  Batch 1,120  of  1,500.  Loss 0.0401  Elapsed: 0:00:10.813419.\n",
      "  Batch 1,160  of  1,500.  Loss 0.0204  Elapsed: 0:00:11.199615.\n",
      "  Batch 1,200  of  1,500.  Loss 0.0700  Elapsed: 0:00:11.587735.\n",
      "  Batch 1,240  of  1,500.  Loss 0.0172  Elapsed: 0:00:11.974299.\n",
      "  Batch 1,280  of  1,500.  Loss 0.0150  Elapsed: 0:00:12.359351.\n",
      "  Batch 1,320  of  1,500.  Loss 0.0150  Elapsed: 0:00:12.747218.\n",
      "  Batch 1,360  of  1,500.  Loss 0.0178  Elapsed: 0:00:13.134032.\n",
      "  Batch 1,400  of  1,500.  Loss 0.0213  Elapsed: 0:00:13.519960.\n",
      "  Batch 1,440  of  1,500.  Loss 0.0459  Elapsed: 0:00:13.904742.\n",
      "  Batch 1,480  of  1,500.  Loss 0.0595  Elapsed: 0:00:14.289614.\n",
      "Avg Validation Loss 0.1069, Completed in 0:00:14.474314 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.819096\n",
      "F1 Score (micro) =   0.862888\n",
      "F1 Score (macro) =   0.505301\n",
      "F1 Score (samples) =   0.786667\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.2838  Elapsed: 0:00:00.396762.\n",
      "  Batch    80  of  1,477.  Loss 0.7580  Elapsed: 0:00:00.783868.\n",
      "  Batch   120  of  1,477.  Loss 0.3199  Elapsed: 0:00:01.170834.\n",
      "  Batch   160  of  1,477.  Loss 0.0138  Elapsed: 0:00:01.558794.\n",
      "  Batch   200  of  1,477.  Loss 0.2867  Elapsed: 0:00:01.947187.\n",
      "  Batch   240  of  1,477.  Loss 0.3398  Elapsed: 0:00:02.333517.\n",
      "  Batch   280  of  1,477.  Loss 0.7439  Elapsed: 0:00:02.719481.\n",
      "  Batch   320  of  1,477.  Loss 0.0489  Elapsed: 0:00:03.108108.\n",
      "  Batch   360  of  1,477.  Loss 0.3949  Elapsed: 0:00:03.494424.\n",
      "  Batch   400  of  1,477.  Loss 0.7809  Elapsed: 0:00:03.879929.\n",
      "  Batch   440  of  1,477.  Loss 0.9947  Elapsed: 0:00:04.266975.\n",
      "  Batch   480  of  1,477.  Loss 0.5749  Elapsed: 0:00:04.655247.\n",
      "  Batch   520  of  1,477.  Loss 0.0319  Elapsed: 0:00:05.041753.\n",
      "  Batch   560  of  1,477.  Loss 0.1780  Elapsed: 0:00:05.430077.\n",
      "  Batch   600  of  1,477.  Loss 0.0134  Elapsed: 0:00:05.817797.\n",
      "  Batch   640  of  1,477.  Loss 0.7129  Elapsed: 0:00:06.201494.\n",
      "  Batch   680  of  1,477.  Loss 0.0149  Elapsed: 0:00:06.587927.\n",
      "  Batch   720  of  1,477.  Loss 0.6384  Elapsed: 0:00:06.974818.\n",
      "  Batch   760  of  1,477.  Loss 0.6741  Elapsed: 0:00:07.362848.\n",
      "  Batch   800  of  1,477.  Loss 0.1554  Elapsed: 0:00:07.750461.\n",
      "  Batch   840  of  1,477.  Loss 0.0152  Elapsed: 0:00:08.137710.\n",
      "  Batch   880  of  1,477.  Loss 0.0196  Elapsed: 0:00:08.524643.\n",
      "  Batch   920  of  1,477.  Loss 0.1361  Elapsed: 0:00:08.912335.\n",
      "  Batch   960  of  1,477.  Loss 0.0451  Elapsed: 0:00:09.297536.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0426  Elapsed: 0:00:09.685000.\n",
      "  Batch 1,040  of  1,477.  Loss 0.3694  Elapsed: 0:00:10.072862.\n",
      "  Batch 1,080  of  1,477.  Loss 0.2095  Elapsed: 0:00:10.459804.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0148  Elapsed: 0:00:10.844022.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0138  Elapsed: 0:00:11.230676.\n",
      "  Batch 1,200  of  1,477.  Loss 0.5067  Elapsed: 0:00:11.616940.\n",
      "  Batch 1,240  of  1,477.  Loss 0.2799  Elapsed: 0:00:12.006285.\n",
      "  Batch 1,280  of  1,477.  Loss 0.2904  Elapsed: 0:00:12.391228.\n",
      "  Batch 1,320  of  1,477.  Loss 0.4024  Elapsed: 0:00:12.778414.\n",
      "  Batch 1,360  of  1,477.  Loss 0.3850  Elapsed: 0:00:13.164629.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0171  Elapsed: 0:00:13.549971.\n",
      "  Batch 1,440  of  1,477.  Loss 0.3289  Elapsed: 0:00:13.934985.\n",
      "Avg Validation Loss 0.3251, Completed in 0:00:14.280528 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.476453\n",
      "F1 Score (micro) =   0.516239\n",
      "F1 Score (macro) =   0.252271\n",
      "F1 Score (samples) =   0.408937\n",
      "Running Experiment TALES_2000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    125.  Loss 0.3133  Elapsed: 0:00:26.884234.\n",
      "Avg Training Loss 0.2993, Completed in 0:00:33.489057 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3613  Elapsed: 0:00:03.703015.\n",
      "  Batch    80  of     93.  Loss 0.1966  Elapsed: 0:00:07.309949.\n",
      "Avg Validation Loss 0.3191, Completed in 0:00:08.328140 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.333480\n",
      "F1 Score (micro) =   0.434137\n",
      "F1 Score (macro) =   0.098737\n",
      "F1 Score (samples) =   0.316858\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    125.  Loss 0.2736  Elapsed: 0:00:26.913479.\n",
      "Avg Training Loss 0.2420, Completed in 0:00:33.514847 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.4455  Elapsed: 0:00:03.688423.\n",
      "  Batch    80  of     93.  Loss 0.3163  Elapsed: 0:00:07.293434.\n",
      "Avg Validation Loss 0.2878, Completed in 0:00:08.312738 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.470817\n",
      "F1 Score (micro) =   0.572857\n",
      "F1 Score (macro) =   0.213347\n",
      "F1 Score (samples) =   0.504175\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    125.  Loss 0.2134  Elapsed: 0:00:26.878005.\n",
      "Avg Training Loss 0.1831, Completed in 0:00:33.476202 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3124  Elapsed: 0:00:03.692004.\n",
      "  Batch    80  of     93.  Loss 0.1848  Elapsed: 0:00:07.292453.\n",
      "Avg Validation Loss 0.2856, Completed in 0:00:08.311300 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.545628\n",
      "F1 Score (micro) =   0.609831\n",
      "F1 Score (macro) =   0.320776\n",
      "F1 Score (samples) =   0.535771\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    125.  Loss 0.0788  Elapsed: 0:00:26.888091.\n",
      "Avg Training Loss 0.1206, Completed in 0:00:33.487481 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3446  Elapsed: 0:00:03.683325.\n",
      "  Batch    80  of     93.  Loss 0.4074  Elapsed: 0:00:07.292833.\n",
      "Avg Validation Loss 0.3093, Completed in 0:00:08.312561 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.578154\n",
      "F1 Score (micro) =   0.625227\n",
      "F1 Score (macro) =   0.370475\n",
      "F1 Score (samples) =   0.581810\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  2,000.  Loss 0.0298  Elapsed: 0:00:00.400361.\n",
      "  Batch    80  of  2,000.  Loss 0.0118  Elapsed: 0:00:00.787136.\n",
      "  Batch   120  of  2,000.  Loss 0.0306  Elapsed: 0:00:01.176456.\n",
      "  Batch   160  of  2,000.  Loss 0.0132  Elapsed: 0:00:01.565741.\n",
      "  Batch   200  of  2,000.  Loss 0.0121  Elapsed: 0:00:01.954158.\n",
      "  Batch   240  of  2,000.  Loss 0.2380  Elapsed: 0:00:02.344332.\n",
      "  Batch   280  of  2,000.  Loss 0.0839  Elapsed: 0:00:02.732983.\n",
      "  Batch   320  of  2,000.  Loss 0.0298  Elapsed: 0:00:03.122158.\n",
      "  Batch   360  of  2,000.  Loss 0.0122  Elapsed: 0:00:03.513630.\n",
      "  Batch   400  of  2,000.  Loss 0.0128  Elapsed: 0:00:03.900509.\n",
      "  Batch   440  of  2,000.  Loss 1.0001  Elapsed: 0:00:04.288263.\n",
      "  Batch   480  of  2,000.  Loss 0.0172  Elapsed: 0:00:04.676380.\n",
      "  Batch   520  of  2,000.  Loss 0.2019  Elapsed: 0:00:05.064250.\n",
      "  Batch   560  of  2,000.  Loss 0.0309  Elapsed: 0:00:05.452013.\n",
      "  Batch   600  of  2,000.  Loss 0.0358  Elapsed: 0:00:05.841142.\n",
      "  Batch   640  of  2,000.  Loss 0.0160  Elapsed: 0:00:06.232480.\n",
      "  Batch   680  of  2,000.  Loss 0.0911  Elapsed: 0:00:06.618725.\n",
      "  Batch   720  of  2,000.  Loss 0.0599  Elapsed: 0:00:07.005960.\n",
      "  Batch   760  of  2,000.  Loss 0.2105  Elapsed: 0:00:07.395290.\n",
      "  Batch   800  of  2,000.  Loss 0.0230  Elapsed: 0:00:07.783527.\n",
      "  Batch   840  of  2,000.  Loss 0.0580  Elapsed: 0:00:08.170933.\n",
      "  Batch   880  of  2,000.  Loss 0.0111  Elapsed: 0:00:08.561723.\n",
      "  Batch   920  of  2,000.  Loss 0.0110  Elapsed: 0:00:08.948049.\n",
      "  Batch   960  of  2,000.  Loss 0.0123  Elapsed: 0:00:09.336161.\n",
      "  Batch 1,000  of  2,000.  Loss 0.0125  Elapsed: 0:00:09.722758.\n",
      "  Batch 1,040  of  2,000.  Loss 0.0115  Elapsed: 0:00:10.111411.\n",
      "  Batch 1,080  of  2,000.  Loss 0.0204  Elapsed: 0:00:10.498873.\n",
      "  Batch 1,120  of  2,000.  Loss 0.0126  Elapsed: 0:00:10.888537.\n",
      "  Batch 1,160  of  2,000.  Loss 0.4299  Elapsed: 0:00:11.276668.\n",
      "  Batch 1,200  of  2,000.  Loss 0.0104  Elapsed: 0:00:11.663783.\n",
      "  Batch 1,240  of  2,000.  Loss 0.0352  Elapsed: 0:00:12.049120.\n",
      "  Batch 1,280  of  2,000.  Loss 0.0136  Elapsed: 0:00:12.436724.\n",
      "  Batch 1,320  of  2,000.  Loss 0.0109  Elapsed: 0:00:12.826223.\n",
      "  Batch 1,360  of  2,000.  Loss 0.0737  Elapsed: 0:00:13.212283.\n",
      "  Batch 1,400  of  2,000.  Loss 0.1233  Elapsed: 0:00:13.599909.\n",
      "  Batch 1,440  of  2,000.  Loss 0.3150  Elapsed: 0:00:13.988105.\n",
      "  Batch 1,480  of  2,000.  Loss 0.0310  Elapsed: 0:00:14.375658.\n",
      "  Batch 1,520  of  2,000.  Loss 0.0698  Elapsed: 0:00:14.762151.\n",
      "  Batch 1,560  of  2,000.  Loss 0.2110  Elapsed: 0:00:15.151255.\n",
      "  Batch 1,600  of  2,000.  Loss 0.2578  Elapsed: 0:00:15.538629.\n",
      "  Batch 1,640  of  2,000.  Loss 0.0112  Elapsed: 0:00:15.927587.\n",
      "  Batch 1,680  of  2,000.  Loss 0.0130  Elapsed: 0:00:16.315254.\n",
      "  Batch 1,720  of  2,000.  Loss 0.0309  Elapsed: 0:00:16.705493.\n",
      "  Batch 1,760  of  2,000.  Loss 0.0270  Elapsed: 0:00:17.094144.\n",
      "  Batch 1,800  of  2,000.  Loss 0.0318  Elapsed: 0:00:17.483881.\n",
      "  Batch 1,840  of  2,000.  Loss 0.0125  Elapsed: 0:00:17.872094.\n",
      "  Batch 1,880  of  2,000.  Loss 0.0106  Elapsed: 0:00:18.258528.\n",
      "  Batch 1,920  of  2,000.  Loss 0.1403  Elapsed: 0:00:18.647346.\n",
      "  Batch 1,960  of  2,000.  Loss 0.0118  Elapsed: 0:00:19.038970.\n",
      "Avg Validation Loss 0.0757, Completed in 0:00:19.418983 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.892895\n",
      "F1 Score (micro) =   0.917629\n",
      "F1 Score (macro) =   0.669674\n",
      "F1 Score (samples) =   0.874500\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 1.1908  Elapsed: 0:00:00.406466.\n",
      "  Batch    80  of  1,477.  Loss 0.0227  Elapsed: 0:00:00.794999.\n",
      "  Batch   120  of  1,477.  Loss 0.1584  Elapsed: 0:00:01.181961.\n",
      "  Batch   160  of  1,477.  Loss 0.1796  Elapsed: 0:00:01.571675.\n",
      "  Batch   200  of  1,477.  Loss 0.6562  Elapsed: 0:00:01.963223.\n",
      "  Batch   240  of  1,477.  Loss 0.3737  Elapsed: 0:00:02.351390.\n",
      "  Batch   280  of  1,477.  Loss 0.1836  Elapsed: 0:00:02.739003.\n",
      "  Batch   320  of  1,477.  Loss 1.0345  Elapsed: 0:00:03.128607.\n",
      "  Batch   360  of  1,477.  Loss 0.1392  Elapsed: 0:00:03.518001.\n",
      "  Batch   400  of  1,477.  Loss 0.4667  Elapsed: 0:00:03.906983.\n",
      "  Batch   440  of  1,477.  Loss 0.0158  Elapsed: 0:00:04.294231.\n",
      "  Batch   480  of  1,477.  Loss 0.0151  Elapsed: 0:00:04.682035.\n",
      "  Batch   520  of  1,477.  Loss 0.0131  Elapsed: 0:00:05.070374.\n",
      "  Batch   560  of  1,477.  Loss 0.0713  Elapsed: 0:00:05.458250.\n",
      "  Batch   600  of  1,477.  Loss 0.7612  Elapsed: 0:00:05.845838.\n",
      "  Batch   640  of  1,477.  Loss 0.0165  Elapsed: 0:00:06.233804.\n",
      "  Batch   680  of  1,477.  Loss 0.1363  Elapsed: 0:00:06.623216.\n",
      "  Batch   720  of  1,477.  Loss 0.0157  Elapsed: 0:00:07.012364.\n",
      "  Batch   760  of  1,477.  Loss 0.0124  Elapsed: 0:00:07.401567.\n",
      "  Batch   800  of  1,477.  Loss 0.4790  Elapsed: 0:00:07.790619.\n",
      "  Batch   840  of  1,477.  Loss 0.5770  Elapsed: 0:00:08.179546.\n",
      "  Batch   880  of  1,477.  Loss 0.0128  Elapsed: 0:00:08.568236.\n",
      "  Batch   920  of  1,477.  Loss 1.0964  Elapsed: 0:00:08.957474.\n",
      "  Batch   960  of  1,477.  Loss 0.0359  Elapsed: 0:00:09.344956.\n",
      "  Batch 1,000  of  1,477.  Loss 0.1907  Elapsed: 0:00:09.733765.\n",
      "  Batch 1,040  of  1,477.  Loss 0.4154  Elapsed: 0:00:10.123340.\n",
      "  Batch 1,080  of  1,477.  Loss 0.2191  Elapsed: 0:00:10.512030.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0254  Elapsed: 0:00:10.899721.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0389  Elapsed: 0:00:11.290391.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0102  Elapsed: 0:00:11.677573.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0122  Elapsed: 0:00:12.066065.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0120  Elapsed: 0:00:12.453309.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0137  Elapsed: 0:00:12.843275.\n",
      "  Batch 1,360  of  1,477.  Loss 0.2137  Elapsed: 0:00:13.231615.\n",
      "  Batch 1,400  of  1,477.  Loss 0.8844  Elapsed: 0:00:13.618449.\n",
      "  Batch 1,440  of  1,477.  Loss 1.0752  Elapsed: 0:00:14.008021.\n",
      "Avg Validation Loss 0.3008, Completed in 0:00:14.357360 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.584994\n",
      "F1 Score (micro) =   0.631502\n",
      "F1 Score (macro) =   0.340959\n",
      "F1 Score (samples) =   0.583390\n",
      "Running Experiment TALES_2500\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    157.  Loss 0.2216  Elapsed: 0:00:26.870550.\n",
      "Avg Training Loss 0.2895, Completed in 0:00:41.855787 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1820  Elapsed: 0:00:03.685644.\n",
      "  Batch    80  of     93.  Loss 0.3834  Elapsed: 0:00:07.283711.\n",
      "Avg Validation Loss 0.3109, Completed in 0:00:08.303145 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.374946\n",
      "F1 Score (micro) =   0.532061\n",
      "F1 Score (macro) =   0.111425\n",
      "F1 Score (samples) =   0.471903\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    157.  Loss 0.1714  Elapsed: 0:00:26.900757.\n",
      "Avg Training Loss 0.2138, Completed in 0:00:41.874797 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1878  Elapsed: 0:00:03.690434.\n",
      "  Batch    80  of     93.  Loss 0.2351  Elapsed: 0:00:07.290371.\n",
      "Avg Validation Loss 0.2710, Completed in 0:00:08.307124 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.551952\n",
      "F1 Score (micro) =   0.623288\n",
      "F1 Score (macro) =   0.319246\n",
      "F1 Score (samples) =   0.554502\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    157.  Loss 0.0805  Elapsed: 0:00:26.872026.\n",
      "Avg Training Loss 0.1453, Completed in 0:00:41.842923 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3707  Elapsed: 0:00:03.684287.\n",
      "  Batch    80  of     93.  Loss 0.1751  Elapsed: 0:00:07.292013.\n",
      "Avg Validation Loss 0.2968, Completed in 0:00:08.304781 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.585742\n",
      "F1 Score (micro) =   0.635345\n",
      "F1 Score (macro) =   0.392410\n",
      "F1 Score (samples) =   0.594900\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    157.  Loss 0.1022  Elapsed: 0:00:26.864818.\n",
      "Avg Training Loss 0.0867, Completed in 0:00:41.851290 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2467  Elapsed: 0:00:03.689058.\n",
      "  Batch    80  of     93.  Loss 0.2871  Elapsed: 0:00:07.281452.\n",
      "Avg Validation Loss 0.3056, Completed in 0:00:08.304773 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.616572\n",
      "F1 Score (micro) =   0.636298\n",
      "F1 Score (macro) =   0.476403\n",
      "F1 Score (samples) =   0.598962\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  2,500.  Loss 0.0076  Elapsed: 0:00:00.397119.\n",
      "  Batch    80  of  2,500.  Loss 0.0123  Elapsed: 0:00:00.782953.\n",
      "  Batch   120  of  2,500.  Loss 0.0885  Elapsed: 0:00:01.170340.\n",
      "  Batch   160  of  2,500.  Loss 0.0102  Elapsed: 0:00:01.557234.\n",
      "  Batch   200  of  2,500.  Loss 0.0164  Elapsed: 0:00:01.946704.\n",
      "  Batch   240  of  2,500.  Loss 0.0203  Elapsed: 0:00:02.332344.\n",
      "  Batch   280  of  2,500.  Loss 0.0793  Elapsed: 0:00:02.718819.\n",
      "  Batch   320  of  2,500.  Loss 0.1488  Elapsed: 0:00:03.106761.\n",
      "  Batch   360  of  2,500.  Loss 0.2034  Elapsed: 0:00:03.494380.\n",
      "  Batch   400  of  2,500.  Loss 0.0068  Elapsed: 0:00:03.881041.\n",
      "  Batch   440  of  2,500.  Loss 0.0233  Elapsed: 0:00:04.267614.\n",
      "  Batch   480  of  2,500.  Loss 0.0078  Elapsed: 0:00:04.653585.\n",
      "  Batch   520  of  2,500.  Loss 0.0070  Elapsed: 0:00:05.040339.\n",
      "  Batch   560  of  2,500.  Loss 0.1534  Elapsed: 0:00:05.426177.\n",
      "  Batch   600  of  2,500.  Loss 0.1151  Elapsed: 0:00:05.811568.\n",
      "  Batch   640  of  2,500.  Loss 0.0136  Elapsed: 0:00:06.198872.\n",
      "  Batch   680  of  2,500.  Loss 0.0362  Elapsed: 0:00:06.584227.\n",
      "  Batch   720  of  2,500.  Loss 0.0207  Elapsed: 0:00:06.970707.\n",
      "  Batch   760  of  2,500.  Loss 0.0068  Elapsed: 0:00:07.357151.\n",
      "  Batch   800  of  2,500.  Loss 0.0070  Elapsed: 0:00:07.741735.\n",
      "  Batch   840  of  2,500.  Loss 0.0567  Elapsed: 0:00:08.129311.\n",
      "  Batch   880  of  2,500.  Loss 0.2464  Elapsed: 0:00:08.515881.\n",
      "  Batch   920  of  2,500.  Loss 0.0579  Elapsed: 0:00:08.900898.\n",
      "  Batch   960  of  2,500.  Loss 0.0200  Elapsed: 0:00:09.287848.\n",
      "  Batch 1,000  of  2,500.  Loss 0.1248  Elapsed: 0:00:09.672963.\n",
      "  Batch 1,040  of  2,500.  Loss 0.1930  Elapsed: 0:00:10.058151.\n",
      "  Batch 1,080  of  2,500.  Loss 0.0072  Elapsed: 0:00:10.446137.\n",
      "  Batch 1,120  of  2,500.  Loss 0.0074  Elapsed: 0:00:10.833017.\n",
      "  Batch 1,160  of  2,500.  Loss 0.0077  Elapsed: 0:00:11.217051.\n",
      "  Batch 1,200  of  2,500.  Loss 0.0202  Elapsed: 0:00:11.602790.\n",
      "  Batch 1,240  of  2,500.  Loss 0.3306  Elapsed: 0:00:11.991538.\n",
      "  Batch 1,280  of  2,500.  Loss 0.3525  Elapsed: 0:00:12.378149.\n",
      "  Batch 1,320  of  2,500.  Loss 0.0090  Elapsed: 0:00:12.764029.\n",
      "  Batch 1,360  of  2,500.  Loss 0.0090  Elapsed: 0:00:13.151626.\n",
      "  Batch 1,400  of  2,500.  Loss 0.0074  Elapsed: 0:00:13.533757.\n",
      "  Batch 1,440  of  2,500.  Loss 0.0196  Elapsed: 0:00:13.918986.\n",
      "  Batch 1,480  of  2,500.  Loss 0.0100  Elapsed: 0:00:14.304793.\n",
      "  Batch 1,520  of  2,500.  Loss 0.0552  Elapsed: 0:00:14.691289.\n",
      "  Batch 1,560  of  2,500.  Loss 0.0161  Elapsed: 0:00:15.076000.\n",
      "  Batch 1,600  of  2,500.  Loss 0.0629  Elapsed: 0:00:15.461554.\n",
      "  Batch 1,640  of  2,500.  Loss 0.0204  Elapsed: 0:00:15.847566.\n",
      "  Batch 1,680  of  2,500.  Loss 0.0070  Elapsed: 0:00:16.232481.\n",
      "  Batch 1,720  of  2,500.  Loss 0.0073  Elapsed: 0:00:16.618921.\n",
      "  Batch 1,760  of  2,500.  Loss 0.0070  Elapsed: 0:00:17.005157.\n",
      "  Batch 1,800  of  2,500.  Loss 0.0072  Elapsed: 0:00:17.390242.\n",
      "  Batch 1,840  of  2,500.  Loss 0.0085  Elapsed: 0:00:17.776531.\n",
      "  Batch 1,880  of  2,500.  Loss 0.0098  Elapsed: 0:00:18.160932.\n",
      "  Batch 1,920  of  2,500.  Loss 0.0221  Elapsed: 0:00:18.547304.\n",
      "  Batch 1,960  of  2,500.  Loss 0.0087  Elapsed: 0:00:18.932134.\n",
      "  Batch 2,000  of  2,500.  Loss 0.0069  Elapsed: 0:00:19.320237.\n",
      "  Batch 2,040  of  2,500.  Loss 0.0382  Elapsed: 0:00:19.705796.\n",
      "  Batch 2,080  of  2,500.  Loss 0.0143  Elapsed: 0:00:20.091621.\n",
      "  Batch 2,120  of  2,500.  Loss 0.0925  Elapsed: 0:00:20.478418.\n",
      "  Batch 2,160  of  2,500.  Loss 0.1031  Elapsed: 0:00:20.863802.\n",
      "  Batch 2,200  of  2,500.  Loss 0.0076  Elapsed: 0:00:21.249141.\n",
      "  Batch 2,240  of  2,500.  Loss 0.0074  Elapsed: 0:00:21.633344.\n",
      "  Batch 2,280  of  2,500.  Loss 0.0080  Elapsed: 0:00:22.018220.\n",
      "  Batch 2,320  of  2,500.  Loss 0.0734  Elapsed: 0:00:22.401930.\n",
      "  Batch 2,360  of  2,500.  Loss 0.0085  Elapsed: 0:00:22.787487.\n",
      "  Batch 2,400  of  2,500.  Loss 0.0649  Elapsed: 0:00:23.173766.\n",
      "  Batch 2,440  of  2,500.  Loss 0.1320  Elapsed: 0:00:23.563126.\n",
      "  Batch 2,480  of  2,500.  Loss 0.0086  Elapsed: 0:00:23.947714.\n",
      "Avg Validation Loss 0.0461, Completed in 0:00:24.130347 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.959308\n",
      "F1 Score (micro) =   0.962513\n",
      "F1 Score (macro) =   0.887133\n",
      "F1 Score (samples) =   0.948933\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0098  Elapsed: 0:00:00.395468.\n",
      "  Batch    80  of  1,477.  Loss 0.0108  Elapsed: 0:00:00.780256.\n",
      "  Batch   120  of  1,477.  Loss 0.0074  Elapsed: 0:00:01.166116.\n",
      "  Batch   160  of  1,477.  Loss 0.0088  Elapsed: 0:00:01.552725.\n",
      "  Batch   200  of  1,477.  Loss 0.0138  Elapsed: 0:00:01.937389.\n",
      "  Batch   240  of  1,477.  Loss 0.0069  Elapsed: 0:00:02.323851.\n",
      "  Batch   280  of  1,477.  Loss 0.5290  Elapsed: 0:00:02.709474.\n",
      "  Batch   320  of  1,477.  Loss 0.0223  Elapsed: 0:00:03.094474.\n",
      "  Batch   360  of  1,477.  Loss 0.0111  Elapsed: 0:00:03.479423.\n",
      "  Batch   400  of  1,477.  Loss 0.0331  Elapsed: 0:00:03.866062.\n",
      "  Batch   440  of  1,477.  Loss 0.0334  Elapsed: 0:00:04.253199.\n",
      "  Batch   480  of  1,477.  Loss 0.7196  Elapsed: 0:00:04.638387.\n",
      "  Batch   520  of  1,477.  Loss 0.0325  Elapsed: 0:00:05.023582.\n",
      "  Batch   560  of  1,477.  Loss 0.0689  Elapsed: 0:00:05.411195.\n",
      "  Batch   600  of  1,477.  Loss 0.0615  Elapsed: 0:00:05.796757.\n",
      "  Batch   640  of  1,477.  Loss 0.0157  Elapsed: 0:00:06.182961.\n",
      "  Batch   680  of  1,477.  Loss 0.0212  Elapsed: 0:00:06.568614.\n",
      "  Batch   720  of  1,477.  Loss 0.8209  Elapsed: 0:00:06.952743.\n",
      "  Batch   760  of  1,477.  Loss 0.2621  Elapsed: 0:00:07.341045.\n",
      "  Batch   800  of  1,477.  Loss 0.0202  Elapsed: 0:00:07.727856.\n",
      "  Batch   840  of  1,477.  Loss 0.0077  Elapsed: 0:00:08.113565.\n",
      "  Batch   880  of  1,477.  Loss 0.6780  Elapsed: 0:00:08.498295.\n",
      "  Batch   920  of  1,477.  Loss 0.4942  Elapsed: 0:00:08.884297.\n",
      "  Batch   960  of  1,477.  Loss 0.0068  Elapsed: 0:00:09.269765.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0862  Elapsed: 0:00:09.655040.\n",
      "  Batch 1,040  of  1,477.  Loss 0.1685  Elapsed: 0:00:10.041318.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0098  Elapsed: 0:00:10.427412.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0099  Elapsed: 0:00:10.812934.\n",
      "  Batch 1,160  of  1,477.  Loss 0.4706  Elapsed: 0:00:11.197963.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0159  Elapsed: 0:00:11.583715.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0205  Elapsed: 0:00:11.967576.\n",
      "  Batch 1,280  of  1,477.  Loss 0.2365  Elapsed: 0:00:12.349497.\n",
      "  Batch 1,320  of  1,477.  Loss 0.7123  Elapsed: 0:00:12.736102.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0376  Elapsed: 0:00:13.119156.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0191  Elapsed: 0:00:13.505560.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0332  Elapsed: 0:00:13.890741.\n",
      "Avg Validation Loss 0.2979, Completed in 0:00:14.235099 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.625158\n",
      "F1 Score (micro) =   0.645669\n",
      "F1 Score (macro) =   0.438833\n",
      "F1 Score (samples) =   0.609118\n",
      "Running Experiment TALES_3000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    188.  Loss 0.3034  Elapsed: 0:00:26.868480.\n",
      "Avg Training Loss 0.2830, Completed in 0:00:50.219906 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2897  Elapsed: 0:00:03.688137.\n",
      "  Batch    80  of     93.  Loss 0.3732  Elapsed: 0:00:07.288533.\n",
      "Avg Validation Loss 0.2919, Completed in 0:00:08.306869 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.391115\n",
      "F1 Score (micro) =   0.525657\n",
      "F1 Score (macro) =   0.131610\n",
      "F1 Score (samples) =   0.426540\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    188.  Loss 0.2176  Elapsed: 0:00:26.871239.\n",
      "Avg Training Loss 0.2152, Completed in 0:00:50.221908 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1964  Elapsed: 0:00:03.683517.\n",
      "  Batch    80  of     93.  Loss 0.2121  Elapsed: 0:00:07.288985.\n",
      "Avg Validation Loss 0.2545, Completed in 0:00:08.310403 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.520876\n",
      "F1 Score (micro) =   0.598311\n",
      "F1 Score (macro) =   0.271648\n",
      "F1 Score (samples) =   0.503724\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    188.  Loss 0.1243  Elapsed: 0:00:26.864359.\n",
      "Avg Training Loss 0.1460, Completed in 0:00:50.215078 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2808  Elapsed: 0:00:03.683502.\n",
      "  Batch    80  of     93.  Loss 0.2681  Elapsed: 0:00:07.283092.\n",
      "Avg Validation Loss 0.2570, Completed in 0:00:08.302639 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.638581\n",
      "F1 Score (micro) =   0.660902\n",
      "F1 Score (macro) =   0.460949\n",
      "F1 Score (samples) =   0.594900\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    188.  Loss 0.0807  Elapsed: 0:00:26.872703.\n",
      "Avg Training Loss 0.0899, Completed in 0:00:50.211905 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2254  Elapsed: 0:00:03.689484.\n",
      "  Batch    80  of     93.  Loss 0.2600  Elapsed: 0:00:07.286562.\n",
      "Avg Validation Loss 0.2756, Completed in 0:00:08.305021 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.672164\n",
      "F1 Score (micro) =   0.688937\n",
      "F1 Score (macro) =   0.539948\n",
      "F1 Score (samples) =   0.648161\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  3,000.  Loss 0.0078  Elapsed: 0:00:00.393954.\n",
      "  Batch    80  of  3,000.  Loss 0.1739  Elapsed: 0:00:00.780580.\n",
      "  Batch   120  of  3,000.  Loss 0.0066  Elapsed: 0:00:01.166870.\n",
      "  Batch   160  of  3,000.  Loss 0.0119  Elapsed: 0:00:01.555565.\n",
      "  Batch   200  of  3,000.  Loss 0.0279  Elapsed: 0:00:01.942273.\n",
      "  Batch   240  of  3,000.  Loss 0.0088  Elapsed: 0:00:02.328830.\n",
      "  Batch   280  of  3,000.  Loss 0.0057  Elapsed: 0:00:02.714930.\n",
      "  Batch   320  of  3,000.  Loss 0.2423  Elapsed: 0:00:03.102234.\n",
      "  Batch   360  of  3,000.  Loss 0.0723  Elapsed: 0:00:03.488039.\n",
      "  Batch   400  of  3,000.  Loss 0.0559  Elapsed: 0:00:03.874322.\n",
      "  Batch   440  of  3,000.  Loss 0.0055  Elapsed: 0:00:04.262853.\n",
      "  Batch   480  of  3,000.  Loss 0.0233  Elapsed: 0:00:04.647190.\n",
      "  Batch   520  of  3,000.  Loss 0.0582  Elapsed: 0:00:05.032253.\n",
      "  Batch   560  of  3,000.  Loss 0.0058  Elapsed: 0:00:05.420487.\n",
      "  Batch   600  of  3,000.  Loss 0.0057  Elapsed: 0:00:05.807647.\n",
      "  Batch   640  of  3,000.  Loss 0.0054  Elapsed: 0:00:06.191531.\n",
      "  Batch   680  of  3,000.  Loss 0.1182  Elapsed: 0:00:06.577508.\n",
      "  Batch   720  of  3,000.  Loss 0.0876  Elapsed: 0:00:06.965733.\n",
      "  Batch   760  of  3,000.  Loss 0.0210  Elapsed: 0:00:07.350959.\n",
      "  Batch   800  of  3,000.  Loss 0.0335  Elapsed: 0:00:07.736830.\n",
      "  Batch   840  of  3,000.  Loss 0.0440  Elapsed: 0:00:08.122066.\n",
      "  Batch   880  of  3,000.  Loss 0.2126  Elapsed: 0:00:08.507588.\n",
      "  Batch   920  of  3,000.  Loss 0.0101  Elapsed: 0:00:08.893113.\n",
      "  Batch   960  of  3,000.  Loss 0.0060  Elapsed: 0:00:09.276385.\n",
      "  Batch 1,000  of  3,000.  Loss 0.0055  Elapsed: 0:00:09.661511.\n",
      "  Batch 1,040  of  3,000.  Loss 0.2849  Elapsed: 0:00:10.046515.\n",
      "  Batch 1,080  of  3,000.  Loss 0.0681  Elapsed: 0:00:10.429805.\n",
      "  Batch 1,120  of  3,000.  Loss 0.0099  Elapsed: 0:00:10.817532.\n",
      "  Batch 1,160  of  3,000.  Loss 0.0198  Elapsed: 0:00:11.203086.\n",
      "  Batch 1,200  of  3,000.  Loss 0.0657  Elapsed: 0:00:11.588913.\n",
      "  Batch 1,240  of  3,000.  Loss 0.0579  Elapsed: 0:00:11.976212.\n",
      "  Batch 1,280  of  3,000.  Loss 0.0112  Elapsed: 0:00:12.361456.\n",
      "  Batch 1,320  of  3,000.  Loss 0.0132  Elapsed: 0:00:12.748095.\n",
      "  Batch 1,360  of  3,000.  Loss 0.0071  Elapsed: 0:00:13.133613.\n",
      "  Batch 1,400  of  3,000.  Loss 0.0055  Elapsed: 0:00:13.520401.\n",
      "  Batch 1,440  of  3,000.  Loss 0.0108  Elapsed: 0:00:13.908912.\n",
      "  Batch 1,480  of  3,000.  Loss 0.1002  Elapsed: 0:00:14.296061.\n",
      "  Batch 1,520  of  3,000.  Loss 0.0259  Elapsed: 0:00:14.682374.\n",
      "  Batch 1,560  of  3,000.  Loss 0.0056  Elapsed: 0:00:15.069078.\n",
      "  Batch 1,600  of  3,000.  Loss 0.0056  Elapsed: 0:00:15.456366.\n",
      "  Batch 1,640  of  3,000.  Loss 0.0065  Elapsed: 0:00:15.842752.\n",
      "  Batch 1,680  of  3,000.  Loss 0.0616  Elapsed: 0:00:16.227523.\n",
      "  Batch 1,720  of  3,000.  Loss 0.0466  Elapsed: 0:00:16.613052.\n",
      "  Batch 1,760  of  3,000.  Loss 0.0058  Elapsed: 0:00:16.998384.\n",
      "  Batch 1,800  of  3,000.  Loss 0.0056  Elapsed: 0:00:17.384380.\n",
      "  Batch 1,840  of  3,000.  Loss 0.0253  Elapsed: 0:00:17.770546.\n",
      "  Batch 1,880  of  3,000.  Loss 0.0062  Elapsed: 0:00:18.156749.\n",
      "  Batch 1,920  of  3,000.  Loss 0.0181  Elapsed: 0:00:18.542584.\n",
      "  Batch 1,960  of  3,000.  Loss 0.0059  Elapsed: 0:00:18.928741.\n",
      "  Batch 2,000  of  3,000.  Loss 0.0076  Elapsed: 0:00:19.316288.\n",
      "  Batch 2,040  of  3,000.  Loss 0.0063  Elapsed: 0:00:19.704370.\n",
      "  Batch 2,080  of  3,000.  Loss 0.0055  Elapsed: 0:00:20.088826.\n",
      "  Batch 2,120  of  3,000.  Loss 0.0055  Elapsed: 0:00:20.473644.\n",
      "  Batch 2,160  of  3,000.  Loss 0.0253  Elapsed: 0:00:20.859635.\n",
      "  Batch 2,200  of  3,000.  Loss 0.0141  Elapsed: 0:00:21.245522.\n",
      "  Batch 2,240  of  3,000.  Loss 0.0259  Elapsed: 0:00:21.633246.\n",
      "  Batch 2,280  of  3,000.  Loss 0.0057  Elapsed: 0:00:22.019916.\n",
      "  Batch 2,320  of  3,000.  Loss 0.0076  Elapsed: 0:00:22.406236.\n",
      "  Batch 2,360  of  3,000.  Loss 0.0061  Elapsed: 0:00:22.791111.\n",
      "  Batch 2,400  of  3,000.  Loss 0.0057  Elapsed: 0:00:23.179080.\n",
      "  Batch 2,440  of  3,000.  Loss 0.0064  Elapsed: 0:00:23.567618.\n",
      "  Batch 2,480  of  3,000.  Loss 0.0528  Elapsed: 0:00:23.954525.\n",
      "  Batch 2,520  of  3,000.  Loss 0.0203  Elapsed: 0:00:24.342064.\n",
      "  Batch 2,560  of  3,000.  Loss 0.0885  Elapsed: 0:00:24.728317.\n",
      "  Batch 2,600  of  3,000.  Loss 0.0725  Elapsed: 0:00:25.112751.\n",
      "  Batch 2,640  of  3,000.  Loss 0.0612  Elapsed: 0:00:25.498475.\n",
      "  Batch 2,680  of  3,000.  Loss 0.0075  Elapsed: 0:00:25.883941.\n",
      "  Batch 2,720  of  3,000.  Loss 0.0663  Elapsed: 0:00:26.269227.\n",
      "  Batch 2,760  of  3,000.  Loss 0.0076  Elapsed: 0:00:26.654962.\n",
      "  Batch 2,800  of  3,000.  Loss 0.2445  Elapsed: 0:00:27.042091.\n",
      "  Batch 2,840  of  3,000.  Loss 0.0105  Elapsed: 0:00:27.426499.\n",
      "  Batch 2,880  of  3,000.  Loss 0.0212  Elapsed: 0:00:27.814145.\n",
      "  Batch 2,920  of  3,000.  Loss 0.0060  Elapsed: 0:00:28.201269.\n",
      "  Batch 2,960  of  3,000.  Loss 0.0089  Elapsed: 0:00:28.584910.\n",
      "Avg Validation Loss 0.0440, Completed in 0:00:28.960441 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.964702\n",
      "F1 Score (micro) =   0.965598\n",
      "F1 Score (macro) =   0.912200\n",
      "F1 Score (samples) =   0.957667\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0223  Elapsed: 0:00:00.406355.\n",
      "  Batch    80  of  1,477.  Loss 0.0076  Elapsed: 0:00:00.790739.\n",
      "  Batch   120  of  1,477.  Loss 0.4099  Elapsed: 0:00:01.176548.\n",
      "  Batch   160  of  1,477.  Loss 0.1375  Elapsed: 0:00:01.562802.\n",
      "  Batch   200  of  1,477.  Loss 0.2950  Elapsed: 0:00:01.949526.\n",
      "  Batch   240  of  1,477.  Loss 0.0084  Elapsed: 0:00:02.338054.\n",
      "  Batch   280  of  1,477.  Loss 0.0159  Elapsed: 0:00:02.725188.\n",
      "  Batch   320  of  1,477.  Loss 0.0200  Elapsed: 0:00:03.113033.\n",
      "  Batch   360  of  1,477.  Loss 1.0395  Elapsed: 0:00:03.498921.\n",
      "  Batch   400  of  1,477.  Loss 0.1910  Elapsed: 0:00:03.886012.\n",
      "  Batch   440  of  1,477.  Loss 0.8313  Elapsed: 0:00:04.271457.\n",
      "  Batch   480  of  1,477.  Loss 0.0062  Elapsed: 0:00:04.658695.\n",
      "  Batch   520  of  1,477.  Loss 0.0055  Elapsed: 0:00:05.045951.\n",
      "  Batch   560  of  1,477.  Loss 1.3726  Elapsed: 0:00:05.432378.\n",
      "  Batch   600  of  1,477.  Loss 0.0702  Elapsed: 0:00:05.819171.\n",
      "  Batch   640  of  1,477.  Loss 0.0172  Elapsed: 0:00:06.208125.\n",
      "  Batch   680  of  1,477.  Loss 0.5926  Elapsed: 0:00:06.597123.\n",
      "  Batch   720  of  1,477.  Loss 0.0056  Elapsed: 0:00:06.981860.\n",
      "  Batch   760  of  1,477.  Loss 0.0099  Elapsed: 0:00:07.368328.\n",
      "  Batch   800  of  1,477.  Loss 1.0683  Elapsed: 0:00:07.753025.\n",
      "  Batch   840  of  1,477.  Loss 0.0104  Elapsed: 0:00:08.138740.\n",
      "  Batch   880  of  1,477.  Loss 0.0060  Elapsed: 0:00:08.525116.\n",
      "  Batch   920  of  1,477.  Loss 1.0170  Elapsed: 0:00:08.911480.\n",
      "  Batch   960  of  1,477.  Loss 1.4606  Elapsed: 0:00:09.297673.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0077  Elapsed: 0:00:09.684506.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0070  Elapsed: 0:00:10.070373.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0387  Elapsed: 0:00:10.456355.\n",
      "  Batch 1,120  of  1,477.  Loss 0.2461  Elapsed: 0:00:10.842739.\n",
      "  Batch 1,160  of  1,477.  Loss 1.4317  Elapsed: 0:00:11.228944.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0778  Elapsed: 0:00:11.616321.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0061  Elapsed: 0:00:12.001780.\n",
      "  Batch 1,280  of  1,477.  Loss 0.2308  Elapsed: 0:00:12.386691.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0137  Elapsed: 0:00:12.771962.\n",
      "  Batch 1,360  of  1,477.  Loss 0.4073  Elapsed: 0:00:13.159570.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0600  Elapsed: 0:00:13.545748.\n",
      "  Batch 1,440  of  1,477.  Loss 0.6303  Elapsed: 0:00:13.931786.\n",
      "Avg Validation Loss 0.2620, Completed in 0:00:14.280503 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.682266\n",
      "F1 Score (micro) =   0.701691\n",
      "F1 Score (macro) =   0.508764\n",
      "F1 Score (samples) =   0.659671\n",
      "Running Experiment TALES_3500\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    219.  Loss 0.1988  Elapsed: 0:00:26.855523.\n",
      "Epoch  0  Batch   200  of    219.  Loss 0.2330  Elapsed: 0:00:53.631108.\n",
      "Avg Training Loss 0.2725, Completed in 0:00:58.565485 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2213  Elapsed: 0:00:03.686163.\n",
      "  Batch    80  of     93.  Loss 0.3186  Elapsed: 0:00:07.287106.\n",
      "Avg Validation Loss 0.2901, Completed in 0:00:08.309760 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.483283\n",
      "F1 Score (micro) =   0.589612\n",
      "F1 Score (macro) =   0.218602\n",
      "F1 Score (samples) =   0.543895\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    219.  Loss 0.1688  Elapsed: 0:00:26.882867.\n",
      "Epoch  1  Batch   200  of    219.  Loss 0.2054  Elapsed: 0:00:53.659263.\n",
      "Avg Training Loss 0.2007, Completed in 0:00:58.585710 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2547  Elapsed: 0:00:03.689863.\n",
      "  Batch    80  of     93.  Loss 0.2508  Elapsed: 0:00:07.285675.\n",
      "Avg Validation Loss 0.2423, Completed in 0:00:08.302798 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.592023\n",
      "F1 Score (micro) =   0.627778\n",
      "F1 Score (macro) =   0.387647\n",
      "F1 Score (samples) =   0.535094\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    219.  Loss 0.1644  Elapsed: 0:00:26.886589.\n",
      "Epoch  2  Batch   200  of    219.  Loss 0.1650  Elapsed: 0:00:53.670683.\n",
      "Avg Training Loss 0.1347, Completed in 0:00:58.598344 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1412  Elapsed: 0:00:03.685818.\n",
      "  Batch    80  of     93.  Loss 0.1649  Elapsed: 0:00:07.284027.\n",
      "Avg Validation Loss 0.2356, Completed in 0:00:08.303111 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.659255\n",
      "F1 Score (micro) =   0.690226\n",
      "F1 Score (macro) =   0.468023\n",
      "F1 Score (samples) =   0.621530\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    219.  Loss 0.0228  Elapsed: 0:00:26.881428.\n",
      "Epoch  3  Batch   200  of    219.  Loss 0.0672  Elapsed: 0:00:53.656668.\n",
      "Avg Training Loss 0.0789, Completed in 0:00:58.589609 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1076  Elapsed: 0:00:03.688150.\n",
      "  Batch    80  of     93.  Loss 0.2815  Elapsed: 0:00:07.290257.\n",
      "Avg Validation Loss 0.2706, Completed in 0:00:08.309678 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.674323\n",
      "F1 Score (micro) =   0.690221\n",
      "F1 Score (macro) =   0.529291\n",
      "F1 Score (samples) =   0.654705\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  3,500.  Loss 0.0159  Elapsed: 0:00:00.408364.\n",
      "  Batch    80  of  3,500.  Loss 0.0050  Elapsed: 0:00:00.796816.\n",
      "  Batch   120  of  3,500.  Loss 0.0543  Elapsed: 0:00:01.182631.\n",
      "  Batch   160  of  3,500.  Loss 0.0077  Elapsed: 0:00:01.573949.\n",
      "  Batch   200  of  3,500.  Loss 0.0051  Elapsed: 0:00:01.961068.\n",
      "  Batch   240  of  3,500.  Loss 0.0404  Elapsed: 0:00:02.350945.\n",
      "  Batch   280  of  3,500.  Loss 0.6617  Elapsed: 0:00:02.740573.\n",
      "  Batch   320  of  3,500.  Loss 0.1129  Elapsed: 0:00:03.129831.\n",
      "  Batch   360  of  3,500.  Loss 0.0050  Elapsed: 0:00:03.520077.\n",
      "  Batch   400  of  3,500.  Loss 0.1217  Elapsed: 0:00:03.908385.\n",
      "  Batch   440  of  3,500.  Loss 0.1330  Elapsed: 0:00:04.295391.\n",
      "  Batch   480  of  3,500.  Loss 0.0048  Elapsed: 0:00:04.683700.\n",
      "  Batch   520  of  3,500.  Loss 0.0050  Elapsed: 0:00:05.073048.\n",
      "  Batch   560  of  3,500.  Loss 0.0841  Elapsed: 0:00:05.462085.\n",
      "  Batch   600  of  3,500.  Loss 0.1204  Elapsed: 0:00:05.852463.\n",
      "  Batch   640  of  3,500.  Loss 0.0170  Elapsed: 0:00:06.241196.\n",
      "  Batch   680  of  3,500.  Loss 0.0301  Elapsed: 0:00:06.631046.\n",
      "  Batch   720  of  3,500.  Loss 0.0273  Elapsed: 0:00:07.021704.\n",
      "  Batch   760  of  3,500.  Loss 0.0049  Elapsed: 0:00:07.412359.\n",
      "  Batch   800  of  3,500.  Loss 0.0048  Elapsed: 0:00:07.799787.\n",
      "  Batch   840  of  3,500.  Loss 0.0064  Elapsed: 0:00:08.189133.\n",
      "  Batch   880  of  3,500.  Loss 0.0364  Elapsed: 0:00:08.576701.\n",
      "  Batch   920  of  3,500.  Loss 0.0049  Elapsed: 0:00:08.964397.\n",
      "  Batch   960  of  3,500.  Loss 0.0452  Elapsed: 0:00:09.353099.\n",
      "  Batch 1,000  of  3,500.  Loss 0.0963  Elapsed: 0:00:09.742519.\n",
      "  Batch 1,040  of  3,500.  Loss 0.0050  Elapsed: 0:00:10.129078.\n",
      "  Batch 1,080  of  3,500.  Loss 0.0055  Elapsed: 0:00:10.516205.\n",
      "  Batch 1,120  of  3,500.  Loss 0.0052  Elapsed: 0:00:10.905413.\n",
      "  Batch 1,160  of  3,500.  Loss 0.0202  Elapsed: 0:00:11.293482.\n",
      "  Batch 1,200  of  3,500.  Loss 0.0050  Elapsed: 0:00:11.682346.\n",
      "  Batch 1,240  of  3,500.  Loss 0.0388  Elapsed: 0:00:12.072767.\n",
      "  Batch 1,280  of  3,500.  Loss 0.0048  Elapsed: 0:00:12.461929.\n",
      "  Batch 1,320  of  3,500.  Loss 0.2590  Elapsed: 0:00:12.851185.\n",
      "  Batch 1,360  of  3,500.  Loss 0.0204  Elapsed: 0:00:13.241276.\n",
      "  Batch 1,400  of  3,500.  Loss 0.0487  Elapsed: 0:00:13.630299.\n",
      "  Batch 1,440  of  3,500.  Loss 0.0100  Elapsed: 0:00:14.016913.\n",
      "  Batch 1,480  of  3,500.  Loss 0.0049  Elapsed: 0:00:14.403474.\n",
      "  Batch 1,520  of  3,500.  Loss 0.0047  Elapsed: 0:00:14.789148.\n",
      "  Batch 1,560  of  3,500.  Loss 0.0047  Elapsed: 0:00:15.175468.\n",
      "  Batch 1,600  of  3,500.  Loss 0.0389  Elapsed: 0:00:15.565725.\n",
      "  Batch 1,640  of  3,500.  Loss 0.0088  Elapsed: 0:00:15.954807.\n",
      "  Batch 1,680  of  3,500.  Loss 0.0049  Elapsed: 0:00:16.343287.\n",
      "  Batch 1,720  of  3,500.  Loss 0.0053  Elapsed: 0:00:16.731065.\n",
      "  Batch 1,760  of  3,500.  Loss 0.0051  Elapsed: 0:00:17.119425.\n",
      "  Batch 1,800  of  3,500.  Loss 0.0048  Elapsed: 0:00:17.507993.\n",
      "  Batch 1,840  of  3,500.  Loss 0.4503  Elapsed: 0:00:17.899622.\n",
      "  Batch 1,880  of  3,500.  Loss 0.0082  Elapsed: 0:00:18.288212.\n",
      "  Batch 1,920  of  3,500.  Loss 0.0117  Elapsed: 0:00:18.677547.\n",
      "  Batch 1,960  of  3,500.  Loss 0.0102  Elapsed: 0:00:19.067001.\n",
      "  Batch 2,000  of  3,500.  Loss 0.0571  Elapsed: 0:00:19.455306.\n",
      "  Batch 2,040  of  3,500.  Loss 0.4939  Elapsed: 0:00:19.845516.\n",
      "  Batch 2,080  of  3,500.  Loss 0.0499  Elapsed: 0:00:20.236125.\n",
      "  Batch 2,120  of  3,500.  Loss 0.0062  Elapsed: 0:00:20.623007.\n",
      "  Batch 2,160  of  3,500.  Loss 0.0357  Elapsed: 0:00:21.013008.\n",
      "  Batch 2,200  of  3,500.  Loss 0.0055  Elapsed: 0:00:21.399939.\n",
      "  Batch 2,240  of  3,500.  Loss 0.0139  Elapsed: 0:00:21.789143.\n",
      "  Batch 2,280  of  3,500.  Loss 0.0187  Elapsed: 0:00:22.179661.\n",
      "  Batch 2,320  of  3,500.  Loss 0.0156  Elapsed: 0:00:22.568240.\n",
      "  Batch 2,360  of  3,500.  Loss 0.0061  Elapsed: 0:00:22.957992.\n",
      "  Batch 2,400  of  3,500.  Loss 0.0049  Elapsed: 0:00:23.344392.\n",
      "  Batch 2,440  of  3,500.  Loss 0.0212  Elapsed: 0:00:23.731858.\n",
      "  Batch 2,480  of  3,500.  Loss 0.0110  Elapsed: 0:00:24.121181.\n",
      "  Batch 2,520  of  3,500.  Loss 0.0081  Elapsed: 0:00:24.511709.\n",
      "  Batch 2,560  of  3,500.  Loss 0.0055  Elapsed: 0:00:24.899695.\n",
      "  Batch 2,600  of  3,500.  Loss 1.2818  Elapsed: 0:00:25.286928.\n",
      "  Batch 2,640  of  3,500.  Loss 0.0086  Elapsed: 0:00:25.674767.\n",
      "  Batch 2,680  of  3,500.  Loss 0.0554  Elapsed: 0:00:26.062502.\n",
      "  Batch 2,720  of  3,500.  Loss 0.0729  Elapsed: 0:00:26.451008.\n",
      "  Batch 2,760  of  3,500.  Loss 0.0050  Elapsed: 0:00:26.839202.\n",
      "  Batch 2,800  of  3,500.  Loss 0.0896  Elapsed: 0:00:27.228767.\n",
      "  Batch 2,840  of  3,500.  Loss 0.0056  Elapsed: 0:00:27.616979.\n",
      "  Batch 2,880  of  3,500.  Loss 0.0350  Elapsed: 0:00:28.006950.\n",
      "  Batch 2,920  of  3,500.  Loss 0.0055  Elapsed: 0:00:28.394752.\n",
      "  Batch 2,960  of  3,500.  Loss 0.0052  Elapsed: 0:00:28.784986.\n",
      "  Batch 3,000  of  3,500.  Loss 0.0179  Elapsed: 0:00:29.176221.\n",
      "  Batch 3,040  of  3,500.  Loss 0.0053  Elapsed: 0:00:29.564476.\n",
      "  Batch 3,080  of  3,500.  Loss 0.0604  Elapsed: 0:00:29.954687.\n",
      "  Batch 3,120  of  3,500.  Loss 0.0050  Elapsed: 0:00:30.344276.\n",
      "  Batch 3,160  of  3,500.  Loss 0.0047  Elapsed: 0:00:30.730532.\n",
      "  Batch 3,200  of  3,500.  Loss 0.0170  Elapsed: 0:00:31.118745.\n",
      "  Batch 3,240  of  3,500.  Loss 0.7098  Elapsed: 0:00:31.505906.\n",
      "  Batch 3,280  of  3,500.  Loss 0.0106  Elapsed: 0:00:31.895638.\n",
      "  Batch 3,320  of  3,500.  Loss 0.0052  Elapsed: 0:00:32.287780.\n",
      "  Batch 3,360  of  3,500.  Loss 0.0053  Elapsed: 0:00:32.674155.\n",
      "  Batch 3,400  of  3,500.  Loss 0.0157  Elapsed: 0:00:33.063462.\n",
      "  Batch 3,440  of  3,500.  Loss 0.0158  Elapsed: 0:00:33.453376.\n",
      "  Batch 3,480  of  3,500.  Loss 0.0048  Elapsed: 0:00:33.842036.\n",
      "Avg Validation Loss 0.0420, Completed in 0:00:34.026738 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.967888\n",
      "F1 Score (micro) =   0.968485\n",
      "F1 Score (macro) =   0.926789\n",
      "F1 Score (samples) =   0.960952\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0565  Elapsed: 0:00:00.405815.\n",
      "  Batch    80  of  1,477.  Loss 0.0374  Elapsed: 0:00:00.793322.\n",
      "  Batch   120  of  1,477.  Loss 0.0056  Elapsed: 0:00:01.181423.\n",
      "  Batch   160  of  1,477.  Loss 0.0399  Elapsed: 0:00:01.569315.\n",
      "  Batch   200  of  1,477.  Loss 0.0113  Elapsed: 0:00:01.960685.\n",
      "  Batch   240  of  1,477.  Loss 0.0330  Elapsed: 0:00:02.347382.\n",
      "  Batch   280  of  1,477.  Loss 0.0056  Elapsed: 0:00:02.735979.\n",
      "  Batch   320  of  1,477.  Loss 0.0886  Elapsed: 0:00:03.122998.\n",
      "  Batch   360  of  1,477.  Loss 0.8455  Elapsed: 0:00:03.510769.\n",
      "  Batch   400  of  1,477.  Loss 0.0488  Elapsed: 0:00:03.898012.\n",
      "  Batch   440  of  1,477.  Loss 0.0050  Elapsed: 0:00:04.286226.\n",
      "  Batch   480  of  1,477.  Loss 0.0097  Elapsed: 0:00:04.671829.\n",
      "  Batch   520  of  1,477.  Loss 0.0073  Elapsed: 0:00:05.060000.\n",
      "  Batch   560  of  1,477.  Loss 0.0496  Elapsed: 0:00:05.447439.\n",
      "  Batch   600  of  1,477.  Loss 0.4649  Elapsed: 0:00:05.839888.\n",
      "  Batch   640  of  1,477.  Loss 0.0405  Elapsed: 0:00:06.228870.\n",
      "  Batch   680  of  1,477.  Loss 0.0059  Elapsed: 0:00:06.616192.\n",
      "  Batch   720  of  1,477.  Loss 0.0422  Elapsed: 0:00:07.006672.\n",
      "  Batch   760  of  1,477.  Loss 0.0160  Elapsed: 0:00:07.393949.\n",
      "  Batch   800  of  1,477.  Loss 0.0496  Elapsed: 0:00:07.781056.\n",
      "  Batch   840  of  1,477.  Loss 0.0080  Elapsed: 0:00:08.168675.\n",
      "  Batch   880  of  1,477.  Loss 0.9606  Elapsed: 0:00:08.556893.\n",
      "  Batch   920  of  1,477.  Loss 0.0056  Elapsed: 0:00:08.947135.\n",
      "  Batch   960  of  1,477.  Loss 0.6699  Elapsed: 0:00:09.334342.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0677  Elapsed: 0:00:09.724613.\n",
      "  Batch 1,040  of  1,477.  Loss 0.6849  Elapsed: 0:00:10.112758.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0057  Elapsed: 0:00:10.502209.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0059  Elapsed: 0:00:10.890614.\n",
      "  Batch 1,160  of  1,477.  Loss 1.2733  Elapsed: 0:00:11.278841.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0422  Elapsed: 0:00:11.666692.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0710  Elapsed: 0:00:12.053640.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0160  Elapsed: 0:00:12.440470.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0049  Elapsed: 0:00:12.826992.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0106  Elapsed: 0:00:13.212708.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0340  Elapsed: 0:00:13.600717.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0368  Elapsed: 0:00:13.987197.\n",
      "Avg Validation Loss 0.2552, Completed in 0:00:14.335711 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.695855\n",
      "F1 Score (micro) =   0.705924\n",
      "F1 Score (macro) =   0.535931\n",
      "F1 Score (samples) =   0.669375\n",
      "Running Experiment TALES_4000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    250.  Loss 0.1757  Elapsed: 0:00:26.881669.\n",
      "Epoch  0  Batch   200  of    250.  Loss 0.2344  Elapsed: 0:00:53.645746.\n",
      "Avg Training Loss 0.2765, Completed in 0:01:06.939105 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2519  Elapsed: 0:00:03.694558.\n",
      "  Batch    80  of     93.  Loss 0.2491  Elapsed: 0:00:07.293690.\n",
      "Avg Validation Loss 0.2912, Completed in 0:00:08.317247 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.404026\n",
      "F1 Score (micro) =   0.539326\n",
      "F1 Score (macro) =   0.133602\n",
      "F1 Score (samples) =   0.454976\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    250.  Loss 0.1215  Elapsed: 0:00:26.885135.\n",
      "Epoch  1  Batch   200  of    250.  Loss 0.2488  Elapsed: 0:00:53.653930.\n",
      "Avg Training Loss 0.2174, Completed in 0:01:06.944651 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2330  Elapsed: 0:00:03.699712.\n",
      "  Batch    80  of     93.  Loss 0.1290  Elapsed: 0:00:07.302354.\n",
      "Avg Validation Loss 0.2645, Completed in 0:00:08.319323 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.522368\n",
      "F1 Score (micro) =   0.610209\n",
      "F1 Score (macro) =   0.263676\n",
      "F1 Score (samples) =   0.534191\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    250.  Loss 0.1827  Elapsed: 0:00:26.867803.\n",
      "Epoch  2  Batch   200  of    250.  Loss 0.2564  Elapsed: 0:00:53.647848.\n",
      "Avg Training Loss 0.1530, Completed in 0:01:06.963428 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1919  Elapsed: 0:00:03.691305.\n",
      "  Batch    80  of     93.  Loss 0.2321  Elapsed: 0:00:07.295569.\n",
      "Avg Validation Loss 0.2499, Completed in 0:00:08.314445 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.611501\n",
      "F1 Score (micro) =   0.633387\n",
      "F1 Score (macro) =   0.429545\n",
      "F1 Score (samples) =   0.536899\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    250.  Loss 0.0605  Elapsed: 0:00:26.909045.\n",
      "Epoch  3  Batch   200  of    250.  Loss 0.0632  Elapsed: 0:00:53.685836.\n",
      "Avg Training Loss 0.0908, Completed in 0:01:06.981048 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2892  Elapsed: 0:00:03.688272.\n",
      "  Batch    80  of     93.  Loss 0.4351  Elapsed: 0:00:07.287878.\n",
      "Avg Validation Loss 0.2726, Completed in 0:00:08.311166 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.656592\n",
      "F1 Score (micro) =   0.665201\n",
      "F1 Score (macro) =   0.501877\n",
      "F1 Score (samples) =   0.614308\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  4,000.  Loss 0.0684  Elapsed: 0:00:00.400491.\n",
      "  Batch    80  of  4,000.  Loss 0.0059  Elapsed: 0:00:00.790147.\n",
      "  Batch   120  of  4,000.  Loss 0.0190  Elapsed: 0:00:01.178521.\n",
      "  Batch   160  of  4,000.  Loss 0.0117  Elapsed: 0:00:01.566729.\n",
      "  Batch   200  of  4,000.  Loss 0.0058  Elapsed: 0:00:01.956512.\n",
      "  Batch   240  of  4,000.  Loss 0.1556  Elapsed: 0:00:02.345884.\n",
      "  Batch   280  of  4,000.  Loss 0.0299  Elapsed: 0:00:02.735926.\n",
      "  Batch   320  of  4,000.  Loss 0.0047  Elapsed: 0:00:03.124790.\n",
      "  Batch   360  of  4,000.  Loss 0.0414  Elapsed: 0:00:03.515927.\n",
      "  Batch   400  of  4,000.  Loss 0.0506  Elapsed: 0:00:03.904338.\n",
      "  Batch   440  of  4,000.  Loss 0.1425  Elapsed: 0:00:04.292121.\n",
      "  Batch   480  of  4,000.  Loss 0.2830  Elapsed: 0:00:04.682385.\n",
      "  Batch   520  of  4,000.  Loss 0.0194  Elapsed: 0:00:05.071593.\n",
      "  Batch   560  of  4,000.  Loss 0.0219  Elapsed: 0:00:05.461349.\n",
      "  Batch   600  of  4,000.  Loss 0.0660  Elapsed: 0:00:05.851777.\n",
      "  Batch   640  of  4,000.  Loss 0.0074  Elapsed: 0:00:06.240273.\n",
      "  Batch   680  of  4,000.  Loss 0.0126  Elapsed: 0:00:06.630490.\n",
      "  Batch   720  of  4,000.  Loss 0.0060  Elapsed: 0:00:07.020114.\n",
      "  Batch   760  of  4,000.  Loss 0.2331  Elapsed: 0:00:07.409160.\n",
      "  Batch   800  of  4,000.  Loss 0.0192  Elapsed: 0:00:07.796330.\n",
      "  Batch   840  of  4,000.  Loss 0.1087  Elapsed: 0:00:08.186654.\n",
      "  Batch   880  of  4,000.  Loss 0.0070  Elapsed: 0:00:08.576683.\n",
      "  Batch   920  of  4,000.  Loss 0.1781  Elapsed: 0:00:08.968266.\n",
      "  Batch   960  of  4,000.  Loss 0.0096  Elapsed: 0:00:09.356848.\n",
      "  Batch 1,000  of  4,000.  Loss 0.1410  Elapsed: 0:00:09.745629.\n",
      "  Batch 1,040  of  4,000.  Loss 0.0049  Elapsed: 0:00:10.133668.\n",
      "  Batch 1,080  of  4,000.  Loss 0.0049  Elapsed: 0:00:10.521444.\n",
      "  Batch 1,120  of  4,000.  Loss 0.0188  Elapsed: 0:00:10.910800.\n",
      "  Batch 1,160  of  4,000.  Loss 0.0048  Elapsed: 0:00:11.300669.\n",
      "  Batch 1,200  of  4,000.  Loss 0.0049  Elapsed: 0:00:11.691426.\n",
      "  Batch 1,240  of  4,000.  Loss 0.0680  Elapsed: 0:00:12.080908.\n",
      "  Batch 1,280  of  4,000.  Loss 0.1336  Elapsed: 0:00:12.469049.\n",
      "  Batch 1,320  of  4,000.  Loss 0.0048  Elapsed: 0:00:12.857485.\n",
      "  Batch 1,360  of  4,000.  Loss 0.0052  Elapsed: 0:00:13.245856.\n",
      "  Batch 1,400  of  4,000.  Loss 0.0355  Elapsed: 0:00:13.637181.\n",
      "  Batch 1,440  of  4,000.  Loss 0.0047  Elapsed: 0:00:14.027740.\n",
      "  Batch 1,480  of  4,000.  Loss 0.0090  Elapsed: 0:00:14.415230.\n",
      "  Batch 1,520  of  4,000.  Loss 0.0736  Elapsed: 0:00:14.803260.\n",
      "  Batch 1,560  of  4,000.  Loss 0.0291  Elapsed: 0:00:15.191582.\n",
      "  Batch 1,600  of  4,000.  Loss 0.0151  Elapsed: 0:00:15.581284.\n",
      "  Batch 1,640  of  4,000.  Loss 0.0051  Elapsed: 0:00:15.968660.\n",
      "  Batch 1,680  of  4,000.  Loss 0.0048  Elapsed: 0:00:16.358048.\n",
      "  Batch 1,720  of  4,000.  Loss 0.0175  Elapsed: 0:00:16.745821.\n",
      "  Batch 1,760  of  4,000.  Loss 0.0055  Elapsed: 0:00:17.134811.\n",
      "  Batch 1,800  of  4,000.  Loss 0.3264  Elapsed: 0:00:17.522305.\n",
      "  Batch 1,840  of  4,000.  Loss 0.2189  Elapsed: 0:00:17.911437.\n",
      "  Batch 1,880  of  4,000.  Loss 0.0153  Elapsed: 0:00:18.302212.\n",
      "  Batch 1,920  of  4,000.  Loss 0.0505  Elapsed: 0:00:18.689469.\n",
      "  Batch 1,960  of  4,000.  Loss 0.0173  Elapsed: 0:00:19.079472.\n",
      "  Batch 2,000  of  4,000.  Loss 0.0056  Elapsed: 0:00:19.470560.\n",
      "  Batch 2,040  of  4,000.  Loss 0.0955  Elapsed: 0:00:19.859731.\n",
      "  Batch 2,080  of  4,000.  Loss 0.0055  Elapsed: 0:00:20.250679.\n",
      "  Batch 2,120  of  4,000.  Loss 0.0169  Elapsed: 0:00:20.643224.\n",
      "  Batch 2,160  of  4,000.  Loss 0.0455  Elapsed: 0:00:21.032856.\n",
      "  Batch 2,200  of  4,000.  Loss 0.1202  Elapsed: 0:00:21.422943.\n",
      "  Batch 2,240  of  4,000.  Loss 0.0343  Elapsed: 0:00:21.811601.\n",
      "  Batch 2,280  of  4,000.  Loss 0.1788  Elapsed: 0:00:22.199897.\n",
      "  Batch 2,320  of  4,000.  Loss 0.0048  Elapsed: 0:00:22.590403.\n",
      "  Batch 2,360  of  4,000.  Loss 0.0061  Elapsed: 0:00:22.979536.\n",
      "  Batch 2,400  of  4,000.  Loss 0.0051  Elapsed: 0:00:23.367943.\n",
      "  Batch 2,440  of  4,000.  Loss 0.0053  Elapsed: 0:00:23.757225.\n",
      "  Batch 2,480  of  4,000.  Loss 0.0067  Elapsed: 0:00:24.149011.\n",
      "  Batch 2,520  of  4,000.  Loss 0.0066  Elapsed: 0:00:24.537440.\n",
      "  Batch 2,560  of  4,000.  Loss 0.0051  Elapsed: 0:00:24.925551.\n",
      "  Batch 2,600  of  4,000.  Loss 0.1042  Elapsed: 0:00:25.313057.\n",
      "  Batch 2,640  of  4,000.  Loss 0.0123  Elapsed: 0:00:25.702062.\n",
      "  Batch 2,680  of  4,000.  Loss 0.0056  Elapsed: 0:00:26.092428.\n",
      "  Batch 2,720  of  4,000.  Loss 0.0073  Elapsed: 0:00:26.483232.\n",
      "  Batch 2,760  of  4,000.  Loss 0.0194  Elapsed: 0:00:26.871682.\n",
      "  Batch 2,800  of  4,000.  Loss 0.1058  Elapsed: 0:00:27.262625.\n",
      "  Batch 2,840  of  4,000.  Loss 0.0072  Elapsed: 0:00:27.650208.\n",
      "  Batch 2,880  of  4,000.  Loss 0.0514  Elapsed: 0:00:28.038642.\n",
      "  Batch 2,920  of  4,000.  Loss 0.0295  Elapsed: 0:00:28.428386.\n",
      "  Batch 2,960  of  4,000.  Loss 0.0447  Elapsed: 0:00:28.817754.\n",
      "  Batch 3,000  of  4,000.  Loss 0.0071  Elapsed: 0:00:29.209696.\n",
      "  Batch 3,040  of  4,000.  Loss 0.0076  Elapsed: 0:00:29.598625.\n",
      "  Batch 3,080  of  4,000.  Loss 0.0068  Elapsed: 0:00:29.990065.\n",
      "  Batch 3,120  of  4,000.  Loss 0.0208  Elapsed: 0:00:30.379808.\n",
      "  Batch 3,160  of  4,000.  Loss 0.0114  Elapsed: 0:00:30.768173.\n",
      "  Batch 3,200  of  4,000.  Loss 0.0192  Elapsed: 0:00:31.155241.\n",
      "  Batch 3,240  of  4,000.  Loss 0.0395  Elapsed: 0:00:31.544243.\n",
      "  Batch 3,280  of  4,000.  Loss 0.0116  Elapsed: 0:00:31.932700.\n",
      "  Batch 3,320  of  4,000.  Loss 0.0212  Elapsed: 0:00:32.324002.\n",
      "  Batch 3,360  of  4,000.  Loss 0.0163  Elapsed: 0:00:32.714403.\n",
      "  Batch 3,400  of  4,000.  Loss 0.0062  Elapsed: 0:00:33.105983.\n",
      "  Batch 3,440  of  4,000.  Loss 0.0051  Elapsed: 0:00:33.492888.\n",
      "  Batch 3,480  of  4,000.  Loss 0.0081  Elapsed: 0:00:33.882436.\n",
      "  Batch 3,520  of  4,000.  Loss 0.0051  Elapsed: 0:00:34.273007.\n",
      "  Batch 3,560  of  4,000.  Loss 0.0049  Elapsed: 0:00:34.662105.\n",
      "  Batch 3,600  of  4,000.  Loss 0.0213  Elapsed: 0:00:35.052200.\n",
      "  Batch 3,640  of  4,000.  Loss 0.0052  Elapsed: 0:00:35.440120.\n",
      "  Batch 3,680  of  4,000.  Loss 0.0203  Elapsed: 0:00:35.829617.\n",
      "  Batch 3,720  of  4,000.  Loss 0.0145  Elapsed: 0:00:36.219148.\n",
      "  Batch 3,760  of  4,000.  Loss 0.0050  Elapsed: 0:00:36.608283.\n",
      "  Batch 3,800  of  4,000.  Loss 0.0050  Elapsed: 0:00:36.997410.\n",
      "  Batch 3,840  of  4,000.  Loss 0.2189  Elapsed: 0:00:37.386695.\n",
      "  Batch 3,880  of  4,000.  Loss 0.1244  Elapsed: 0:00:37.775702.\n",
      "  Batch 3,920  of  4,000.  Loss 0.1882  Elapsed: 0:00:38.163976.\n",
      "  Batch 3,960  of  4,000.  Loss 0.0047  Elapsed: 0:00:38.553593.\n",
      "Avg Validation Loss 0.0462, Completed in 0:00:38.934782 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.954519\n",
      "F1 Score (micro) =   0.956976\n",
      "F1 Score (macro) =   0.876182\n",
      "F1 Score (samples) =   0.939583\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0094  Elapsed: 0:00:00.396296.\n",
      "  Batch    80  of  1,477.  Loss 0.0088  Elapsed: 0:00:00.782522.\n",
      "  Batch   120  of  1,477.  Loss 0.3377  Elapsed: 0:00:01.169046.\n",
      "  Batch   160  of  1,477.  Loss 0.0105  Elapsed: 0:00:01.554806.\n",
      "  Batch   200  of  1,477.  Loss 0.4711  Elapsed: 0:00:01.941836.\n",
      "  Batch   240  of  1,477.  Loss 0.0960  Elapsed: 0:00:02.330276.\n",
      "  Batch   280  of  1,477.  Loss 0.4030  Elapsed: 0:00:02.717383.\n",
      "  Batch   320  of  1,477.  Loss 0.0054  Elapsed: 0:00:03.106779.\n",
      "  Batch   360  of  1,477.  Loss 0.0688  Elapsed: 0:00:03.495477.\n",
      "  Batch   400  of  1,477.  Loss 0.0133  Elapsed: 0:00:03.883811.\n",
      "  Batch   440  of  1,477.  Loss 0.0247  Elapsed: 0:00:04.271060.\n",
      "  Batch   480  of  1,477.  Loss 0.0603  Elapsed: 0:00:04.658157.\n",
      "  Batch   520  of  1,477.  Loss 0.0060  Elapsed: 0:00:05.044963.\n",
      "  Batch   560  of  1,477.  Loss 0.8966  Elapsed: 0:00:05.432900.\n",
      "  Batch   600  of  1,477.  Loss 0.1468  Elapsed: 0:00:05.820444.\n",
      "  Batch   640  of  1,477.  Loss 0.0687  Elapsed: 0:00:06.204295.\n",
      "  Batch   680  of  1,477.  Loss 0.6880  Elapsed: 0:00:06.590077.\n",
      "  Batch   720  of  1,477.  Loss 0.0063  Elapsed: 0:00:06.974470.\n",
      "  Batch   760  of  1,477.  Loss 0.6164  Elapsed: 0:00:07.358622.\n",
      "  Batch   800  of  1,477.  Loss 0.0280  Elapsed: 0:00:07.743474.\n",
      "  Batch   840  of  1,477.  Loss 0.0071  Elapsed: 0:00:08.131070.\n",
      "  Batch   880  of  1,477.  Loss 0.4872  Elapsed: 0:00:08.516993.\n",
      "  Batch   920  of  1,477.  Loss 0.6536  Elapsed: 0:00:08.901572.\n",
      "  Batch   960  of  1,477.  Loss 0.2069  Elapsed: 0:00:09.291151.\n",
      "  Batch 1,000  of  1,477.  Loss 0.2482  Elapsed: 0:00:09.677394.\n",
      "  Batch 1,040  of  1,477.  Loss 1.3106  Elapsed: 0:00:10.065105.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0179  Elapsed: 0:00:10.451604.\n",
      "  Batch 1,120  of  1,477.  Loss 0.1754  Elapsed: 0:00:10.838534.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0093  Elapsed: 0:00:11.226104.\n",
      "  Batch 1,200  of  1,477.  Loss 1.4195  Elapsed: 0:00:11.610307.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0062  Elapsed: 0:00:11.995928.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0207  Elapsed: 0:00:12.383904.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0054  Elapsed: 0:00:12.771013.\n",
      "  Batch 1,360  of  1,477.  Loss 1.3675  Elapsed: 0:00:13.159025.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0068  Elapsed: 0:00:13.544745.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0116  Elapsed: 0:00:13.932811.\n",
      "Avg Validation Loss 0.2613, Completed in 0:00:14.280529 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.669128\n",
      "F1 Score (micro) =   0.673007\n",
      "F1 Score (macro) =   0.498247\n",
      "F1 Score (samples) =   0.622433\n",
      "Running Experiment TALES_4500\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    282.  Loss 0.2149  Elapsed: 0:00:26.856898.\n",
      "Epoch  0  Batch   200  of    282.  Loss 0.3343  Elapsed: 0:00:53.628064.\n",
      "Avg Training Loss 0.2719, Completed in 0:01:15.299025 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2979  Elapsed: 0:00:03.685337.\n",
      "  Batch    80  of     93.  Loss 0.2235  Elapsed: 0:00:07.288310.\n",
      "Avg Validation Loss 0.2991, Completed in 0:00:08.308223 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.454521\n",
      "F1 Score (micro) =   0.574337\n",
      "F1 Score (macro) =   0.190160\n",
      "F1 Score (samples) =   0.534868\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    282.  Loss 0.1749  Elapsed: 0:00:26.849632.\n",
      "Epoch  1  Batch   200  of    282.  Loss 0.2124  Elapsed: 0:00:53.617360.\n",
      "Avg Training Loss 0.2077, Completed in 0:01:15.319184 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3561  Elapsed: 0:00:03.692958.\n",
      "  Batch    80  of     93.  Loss 0.2704  Elapsed: 0:00:07.290026.\n",
      "Avg Validation Loss 0.2471, Completed in 0:00:08.302268 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.595436\n",
      "F1 Score (micro) =   0.648249\n",
      "F1 Score (macro) =   0.389320\n",
      "F1 Score (samples) =   0.563755\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    282.  Loss 0.2489  Elapsed: 0:00:26.869700.\n",
      "Epoch  2  Batch   200  of    282.  Loss 0.1065  Elapsed: 0:00:53.665689.\n",
      "Avg Training Loss 0.1448, Completed in 0:01:15.353029 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2367  Elapsed: 0:00:03.687959.\n",
      "  Batch    80  of     93.  Loss 0.2213  Elapsed: 0:00:07.288300.\n",
      "Avg Validation Loss 0.2265, Completed in 0:00:08.302750 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.665212\n",
      "F1 Score (micro) =   0.685520\n",
      "F1 Score (macro) =   0.507524\n",
      "F1 Score (samples) =   0.615437\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    282.  Loss 0.0500  Elapsed: 0:00:26.860311.\n",
      "Epoch  3  Batch   200  of    282.  Loss 0.0581  Elapsed: 0:00:53.625971.\n",
      "Avg Training Loss 0.0862, Completed in 0:01:15.305396 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1431  Elapsed: 0:00:03.694035.\n",
      "  Batch    80  of     93.  Loss 0.2054  Elapsed: 0:00:07.288232.\n",
      "Avg Validation Loss 0.2598, Completed in 0:00:08.304151 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.703793\n",
      "F1 Score (micro) =   0.724004\n",
      "F1 Score (macro) =   0.571444\n",
      "F1 Score (samples) =   0.693974\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  4,500.  Loss 0.0048  Elapsed: 0:00:00.396378.\n",
      "  Batch    80  of  4,500.  Loss 0.0377  Elapsed: 0:00:00.782032.\n",
      "  Batch   120  of  4,500.  Loss 0.0052  Elapsed: 0:00:01.166379.\n",
      "  Batch   160  of  4,500.  Loss 0.0183  Elapsed: 0:00:01.550686.\n",
      "  Batch   200  of  4,500.  Loss 0.0080  Elapsed: 0:00:01.935170.\n",
      "  Batch   240  of  4,500.  Loss 0.0294  Elapsed: 0:00:02.322108.\n",
      "  Batch   280  of  4,500.  Loss 0.0049  Elapsed: 0:00:02.707288.\n",
      "  Batch   320  of  4,500.  Loss 0.0052  Elapsed: 0:00:03.094661.\n",
      "  Batch   360  of  4,500.  Loss 0.0054  Elapsed: 0:00:03.480102.\n",
      "  Batch   400  of  4,500.  Loss 0.0350  Elapsed: 0:00:03.864583.\n",
      "  Batch   440  of  4,500.  Loss 0.0166  Elapsed: 0:00:04.250512.\n",
      "  Batch   480  of  4,500.  Loss 0.0047  Elapsed: 0:00:04.633872.\n",
      "  Batch   520  of  4,500.  Loss 0.0121  Elapsed: 0:00:05.019241.\n",
      "  Batch   560  of  4,500.  Loss 0.0233  Elapsed: 0:00:05.404296.\n",
      "  Batch   600  of  4,500.  Loss 0.0059  Elapsed: 0:00:05.791026.\n",
      "  Batch   640  of  4,500.  Loss 0.0169  Elapsed: 0:00:06.173592.\n",
      "  Batch   680  of  4,500.  Loss 0.0048  Elapsed: 0:00:06.559190.\n",
      "  Batch   720  of  4,500.  Loss 0.0050  Elapsed: 0:00:06.941996.\n",
      "  Batch   760  of  4,500.  Loss 0.0197  Elapsed: 0:00:07.324889.\n",
      "  Batch   800  of  4,500.  Loss 0.0056  Elapsed: 0:00:07.709247.\n",
      "  Batch   840  of  4,500.  Loss 0.0053  Elapsed: 0:00:08.092967.\n",
      "  Batch   880  of  4,500.  Loss 0.1904  Elapsed: 0:00:08.473946.\n",
      "  Batch   920  of  4,500.  Loss 0.0638  Elapsed: 0:00:08.857490.\n",
      "  Batch   960  of  4,500.  Loss 0.0042  Elapsed: 0:00:09.241317.\n",
      "  Batch 1,000  of  4,500.  Loss 0.0044  Elapsed: 0:00:09.624988.\n",
      "  Batch 1,040  of  4,500.  Loss 0.0045  Elapsed: 0:00:10.007993.\n",
      "  Batch 1,080  of  4,500.  Loss 0.0165  Elapsed: 0:00:10.390670.\n",
      "  Batch 1,120  of  4,500.  Loss 0.0324  Elapsed: 0:00:10.775260.\n",
      "  Batch 1,160  of  4,500.  Loss 0.0048  Elapsed: 0:00:11.159760.\n",
      "  Batch 1,200  of  4,500.  Loss 0.0449  Elapsed: 0:00:11.544767.\n",
      "  Batch 1,240  of  4,500.  Loss 0.0050  Elapsed: 0:00:11.928097.\n",
      "  Batch 1,280  of  4,500.  Loss 0.1215  Elapsed: 0:00:12.310824.\n",
      "  Batch 1,320  of  4,500.  Loss 0.0594  Elapsed: 0:00:12.694624.\n",
      "  Batch 1,360  of  4,500.  Loss 0.5206  Elapsed: 0:00:13.078155.\n",
      "  Batch 1,400  of  4,500.  Loss 0.2591  Elapsed: 0:00:13.461451.\n",
      "  Batch 1,440  of  4,500.  Loss 0.0070  Elapsed: 0:00:13.846394.\n",
      "  Batch 1,480  of  4,500.  Loss 0.0404  Elapsed: 0:00:14.230086.\n",
      "  Batch 1,520  of  4,500.  Loss 0.0062  Elapsed: 0:00:14.613418.\n",
      "  Batch 1,560  of  4,500.  Loss 0.0048  Elapsed: 0:00:14.997850.\n",
      "  Batch 1,600  of  4,500.  Loss 0.0044  Elapsed: 0:00:15.382917.\n",
      "  Batch 1,640  of  4,500.  Loss 0.0043  Elapsed: 0:00:15.766609.\n",
      "  Batch 1,680  of  4,500.  Loss 0.0079  Elapsed: 0:00:16.151301.\n",
      "  Batch 1,720  of  4,500.  Loss 0.0057  Elapsed: 0:00:16.534731.\n",
      "  Batch 1,760  of  4,500.  Loss 0.0147  Elapsed: 0:00:16.917553.\n",
      "  Batch 1,800  of  4,500.  Loss 0.0053  Elapsed: 0:00:17.300905.\n",
      "  Batch 1,840  of  4,500.  Loss 0.0042  Elapsed: 0:00:17.686047.\n",
      "  Batch 1,880  of  4,500.  Loss 0.0048  Elapsed: 0:00:18.069701.\n",
      "  Batch 1,920  of  4,500.  Loss 0.0097  Elapsed: 0:00:18.455509.\n",
      "  Batch 1,960  of  4,500.  Loss 0.1035  Elapsed: 0:00:18.839394.\n",
      "  Batch 2,000  of  4,500.  Loss 0.0103  Elapsed: 0:00:19.225741.\n",
      "  Batch 2,040  of  4,500.  Loss 0.0044  Elapsed: 0:00:19.608945.\n",
      "  Batch 2,080  of  4,500.  Loss 0.0046  Elapsed: 0:00:19.991524.\n",
      "  Batch 2,120  of  4,500.  Loss 0.0413  Elapsed: 0:00:20.377140.\n",
      "  Batch 2,160  of  4,500.  Loss 0.0062  Elapsed: 0:00:20.760490.\n",
      "  Batch 2,200  of  4,500.  Loss 0.0470  Elapsed: 0:00:21.143540.\n",
      "  Batch 2,240  of  4,500.  Loss 0.0041  Elapsed: 0:00:21.528013.\n",
      "  Batch 2,280  of  4,500.  Loss 0.0059  Elapsed: 0:00:21.911300.\n",
      "  Batch 2,320  of  4,500.  Loss 0.0407  Elapsed: 0:00:22.295720.\n",
      "  Batch 2,360  of  4,500.  Loss 0.0044  Elapsed: 0:00:22.681103.\n",
      "  Batch 2,400  of  4,500.  Loss 0.0156  Elapsed: 0:00:23.065611.\n",
      "  Batch 2,440  of  4,500.  Loss 0.0050  Elapsed: 0:00:23.449748.\n",
      "  Batch 2,480  of  4,500.  Loss 0.0045  Elapsed: 0:00:23.832514.\n",
      "  Batch 2,520  of  4,500.  Loss 0.1401  Elapsed: 0:00:24.215003.\n",
      "  Batch 2,560  of  4,500.  Loss 0.0042  Elapsed: 0:00:24.598453.\n",
      "  Batch 2,600  of  4,500.  Loss 0.0043  Elapsed: 0:00:24.981966.\n",
      "  Batch 2,640  of  4,500.  Loss 0.0063  Elapsed: 0:00:25.365447.\n",
      "  Batch 2,680  of  4,500.  Loss 0.0042  Elapsed: 0:00:25.748662.\n",
      "  Batch 2,720  of  4,500.  Loss 0.0049  Elapsed: 0:00:26.131460.\n",
      "  Batch 2,760  of  4,500.  Loss 0.0081  Elapsed: 0:00:26.513587.\n",
      "  Batch 2,800  of  4,500.  Loss 0.0167  Elapsed: 0:00:26.898749.\n",
      "  Batch 2,840  of  4,500.  Loss 0.0180  Elapsed: 0:00:27.283532.\n",
      "  Batch 2,880  of  4,500.  Loss 0.0655  Elapsed: 0:00:27.668809.\n",
      "  Batch 2,920  of  4,500.  Loss 0.0150  Elapsed: 0:00:28.051366.\n",
      "  Batch 2,960  of  4,500.  Loss 0.0063  Elapsed: 0:00:28.437981.\n",
      "  Batch 3,000  of  4,500.  Loss 0.2403  Elapsed: 0:00:28.825373.\n",
      "  Batch 3,040  of  4,500.  Loss 0.0054  Elapsed: 0:00:29.209061.\n",
      "  Batch 3,080  of  4,500.  Loss 0.0042  Elapsed: 0:00:29.592648.\n",
      "  Batch 3,120  of  4,500.  Loss 0.0050  Elapsed: 0:00:29.976506.\n",
      "  Batch 3,160  of  4,500.  Loss 0.0177  Elapsed: 0:00:30.360445.\n",
      "  Batch 3,200  of  4,500.  Loss 0.0635  Elapsed: 0:00:30.747222.\n",
      "  Batch 3,240  of  4,500.  Loss 0.0042  Elapsed: 0:00:31.134971.\n",
      "  Batch 3,280  of  4,500.  Loss 0.0051  Elapsed: 0:00:31.518663.\n",
      "  Batch 3,320  of  4,500.  Loss 0.0048  Elapsed: 0:00:31.901641.\n",
      "  Batch 3,360  of  4,500.  Loss 0.0042  Elapsed: 0:00:32.284384.\n",
      "  Batch 3,400  of  4,500.  Loss 0.0058  Elapsed: 0:00:32.669203.\n",
      "  Batch 3,440  of  4,500.  Loss 0.0046  Elapsed: 0:00:33.051477.\n",
      "  Batch 3,480  of  4,500.  Loss 0.0047  Elapsed: 0:00:33.432373.\n",
      "  Batch 3,520  of  4,500.  Loss 0.0055  Elapsed: 0:00:33.816413.\n",
      "  Batch 3,560  of  4,500.  Loss 0.0403  Elapsed: 0:00:34.200017.\n",
      "  Batch 3,600  of  4,500.  Loss 0.0161  Elapsed: 0:00:34.582266.\n",
      "  Batch 3,640  of  4,500.  Loss 0.0043  Elapsed: 0:00:34.966869.\n",
      "  Batch 3,680  of  4,500.  Loss 0.0045  Elapsed: 0:00:35.351142.\n",
      "  Batch 3,720  of  4,500.  Loss 0.0042  Elapsed: 0:00:35.734607.\n",
      "  Batch 3,760  of  4,500.  Loss 0.0043  Elapsed: 0:00:36.119045.\n",
      "  Batch 3,800  of  4,500.  Loss 0.0440  Elapsed: 0:00:36.503605.\n",
      "  Batch 3,840  of  4,500.  Loss 0.0062  Elapsed: 0:00:36.883825.\n",
      "  Batch 3,880  of  4,500.  Loss 0.0053  Elapsed: 0:00:37.267229.\n",
      "  Batch 3,920  of  4,500.  Loss 0.0050  Elapsed: 0:00:37.654399.\n",
      "  Batch 3,960  of  4,500.  Loss 0.0043  Elapsed: 0:00:38.036949.\n",
      "  Batch 4,000  of  4,500.  Loss 0.0191  Elapsed: 0:00:38.422215.\n",
      "  Batch 4,040  of  4,500.  Loss 0.0425  Elapsed: 0:00:38.805865.\n",
      "  Batch 4,080  of  4,500.  Loss 0.0152  Elapsed: 0:00:39.188429.\n",
      "  Batch 4,120  of  4,500.  Loss 0.0096  Elapsed: 0:00:39.573227.\n",
      "  Batch 4,160  of  4,500.  Loss 0.0048  Elapsed: 0:00:39.960592.\n",
      "  Batch 4,200  of  4,500.  Loss 0.0491  Elapsed: 0:00:40.343398.\n",
      "  Batch 4,240  of  4,500.  Loss 0.0043  Elapsed: 0:00:40.728365.\n",
      "  Batch 4,280  of  4,500.  Loss 0.0053  Elapsed: 0:00:41.114489.\n",
      "  Batch 4,320  of  4,500.  Loss 0.0448  Elapsed: 0:00:41.498517.\n",
      "  Batch 4,360  of  4,500.  Loss 0.0044  Elapsed: 0:00:41.879572.\n",
      "  Batch 4,400  of  4,500.  Loss 0.0045  Elapsed: 0:00:42.262837.\n",
      "  Batch 4,440  of  4,500.  Loss 0.0042  Elapsed: 0:00:42.646754.\n",
      "  Batch 4,480  of  4,500.  Loss 0.0288  Elapsed: 0:00:43.032122.\n",
      "Avg Validation Loss 0.0418, Completed in 0:00:43.215167 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.962787\n",
      "F1 Score (micro) =   0.964539\n",
      "F1 Score (macro) =   0.908095\n",
      "F1 Score (samples) =   0.951704\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0199  Elapsed: 0:00:00.402713.\n",
      "  Batch    80  of  1,477.  Loss 0.0044  Elapsed: 0:00:00.787159.\n",
      "  Batch   120  of  1,477.  Loss 0.0915  Elapsed: 0:00:01.171116.\n",
      "  Batch   160  of  1,477.  Loss 0.0044  Elapsed: 0:00:01.555241.\n",
      "  Batch   200  of  1,477.  Loss 0.0440  Elapsed: 0:00:01.939515.\n",
      "  Batch   240  of  1,477.  Loss 0.0042  Elapsed: 0:00:02.322356.\n",
      "  Batch   280  of  1,477.  Loss 0.4575  Elapsed: 0:00:02.706775.\n",
      "  Batch   320  of  1,477.  Loss 0.0155  Elapsed: 0:00:03.091426.\n",
      "  Batch   360  of  1,477.  Loss 0.0044  Elapsed: 0:00:03.474716.\n",
      "  Batch   400  of  1,477.  Loss 0.0203  Elapsed: 0:00:03.859010.\n",
      "  Batch   440  of  1,477.  Loss 0.0052  Elapsed: 0:00:04.241540.\n",
      "  Batch   480  of  1,477.  Loss 0.0463  Elapsed: 0:00:04.625661.\n",
      "  Batch   520  of  1,477.  Loss 0.0069  Elapsed: 0:00:05.011208.\n",
      "  Batch   560  of  1,477.  Loss 0.0054  Elapsed: 0:00:05.394887.\n",
      "  Batch   600  of  1,477.  Loss 0.0102  Elapsed: 0:00:05.779093.\n",
      "  Batch   640  of  1,477.  Loss 0.0102  Elapsed: 0:00:06.163392.\n",
      "  Batch   680  of  1,477.  Loss 0.0166  Elapsed: 0:00:06.547892.\n",
      "  Batch   720  of  1,477.  Loss 0.0371  Elapsed: 0:00:06.933041.\n",
      "  Batch   760  of  1,477.  Loss 1.4178  Elapsed: 0:00:07.316238.\n",
      "  Batch   800  of  1,477.  Loss 0.0212  Elapsed: 0:00:07.700188.\n",
      "  Batch   840  of  1,477.  Loss 0.0057  Elapsed: 0:00:08.082987.\n",
      "  Batch   880  of  1,477.  Loss 0.0062  Elapsed: 0:00:08.467850.\n",
      "  Batch   920  of  1,477.  Loss 0.0404  Elapsed: 0:00:08.849816.\n",
      "  Batch   960  of  1,477.  Loss 0.2557  Elapsed: 0:00:09.234480.\n",
      "  Batch 1,000  of  1,477.  Loss 0.1754  Elapsed: 0:00:09.619020.\n",
      "  Batch 1,040  of  1,477.  Loss 0.7684  Elapsed: 0:00:10.003516.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0169  Elapsed: 0:00:10.390400.\n",
      "  Batch 1,120  of  1,477.  Loss 0.3066  Elapsed: 0:00:10.774875.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0465  Elapsed: 0:00:11.158483.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0133  Elapsed: 0:00:11.541386.\n",
      "  Batch 1,240  of  1,477.  Loss 0.7473  Elapsed: 0:00:11.925128.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0054  Elapsed: 0:00:12.308909.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0057  Elapsed: 0:00:12.693421.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0063  Elapsed: 0:00:13.077101.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0064  Elapsed: 0:00:13.463814.\n",
      "  Batch 1,440  of  1,477.  Loss 0.1707  Elapsed: 0:00:13.848730.\n",
      "Avg Validation Loss 0.2472, Completed in 0:00:14.195129 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.715245\n",
      "F1 Score (micro) =   0.734231\n",
      "F1 Score (macro) =   0.579318\n",
      "F1 Score (samples) =   0.700293\n",
      "Running Experiment TALES_5000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    313.  Loss 0.2579  Elapsed: 0:00:26.875068.\n",
      "Epoch  0  Batch   200  of    313.  Loss 0.1533  Elapsed: 0:00:53.631469.\n",
      "Epoch  0  Batch   300  of    313.  Loss 0.1984  Elapsed: 0:01:20.387006.\n",
      "Avg Training Loss 0.2621, Completed in 0:01:23.644224 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2576  Elapsed: 0:00:03.690460.\n",
      "  Batch    80  of     93.  Loss 0.3013  Elapsed: 0:00:07.295599.\n",
      "Avg Validation Loss 0.2793, Completed in 0:00:08.311745 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.520741\n",
      "F1 Score (micro) =   0.603403\n",
      "F1 Score (macro) =   0.267685\n",
      "F1 Score (samples) =   0.539607\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    313.  Loss 0.1538  Elapsed: 0:00:26.860992.\n",
      "Epoch  1  Batch   200  of    313.  Loss 0.1323  Elapsed: 0:00:53.624957.\n",
      "Epoch  1  Batch   300  of    313.  Loss 0.2352  Elapsed: 0:01:20.400666.\n",
      "Avg Training Loss 0.2023, Completed in 0:01:23.667434 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3822  Elapsed: 0:00:03.690475.\n",
      "  Batch    80  of     93.  Loss 0.2800  Elapsed: 0:00:07.290281.\n",
      "Avg Validation Loss 0.2374, Completed in 0:00:08.311558 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.618567\n",
      "F1 Score (micro) =   0.658305\n",
      "F1 Score (macro) =   0.405381\n",
      "F1 Score (samples) =   0.585872\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    313.  Loss 0.2525  Elapsed: 0:00:26.875813.\n",
      "Epoch  2  Batch   200  of    313.  Loss 0.2063  Elapsed: 0:00:53.671914.\n",
      "Epoch  2  Batch   300  of    313.  Loss 0.0813  Elapsed: 0:01:20.451711.\n",
      "Avg Training Loss 0.1457, Completed in 0:01:23.716144 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3202  Elapsed: 0:00:03.682959.\n",
      "  Batch    80  of     93.  Loss 0.1526  Elapsed: 0:00:07.292433.\n",
      "Avg Validation Loss 0.2271, Completed in 0:00:08.314369 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.669451\n",
      "F1 Score (micro) =   0.680514\n",
      "F1 Score (macro) =   0.537243\n",
      "F1 Score (samples) =   0.609569\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    313.  Loss 0.0661  Elapsed: 0:00:26.861379.\n",
      "Epoch  3  Batch   200  of    313.  Loss 0.0603  Elapsed: 0:00:53.627424.\n",
      "Epoch  3  Batch   300  of    313.  Loss 0.0621  Elapsed: 0:01:20.407609.\n",
      "Avg Training Loss 0.0882, Completed in 0:01:23.672649 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3331  Elapsed: 0:00:03.683681.\n",
      "  Batch    80  of     93.  Loss 0.0984  Elapsed: 0:00:07.278455.\n",
      "Avg Validation Loss 0.2544, Completed in 0:00:08.300591 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.703656\n",
      "F1 Score (micro) =   0.721003\n",
      "F1 Score (macro) =   0.584831\n",
      "F1 Score (samples) =   0.698037\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  5,000.  Loss 0.0399  Elapsed: 0:00:00.397332.\n",
      "  Batch    80  of  5,000.  Loss 0.1298  Elapsed: 0:00:00.782624.\n",
      "  Batch   120  of  5,000.  Loss 0.0060  Elapsed: 0:00:01.168652.\n",
      "  Batch   160  of  5,000.  Loss 0.0043  Elapsed: 0:00:01.552224.\n",
      "  Batch   200  of  5,000.  Loss 0.0049  Elapsed: 0:00:01.937511.\n",
      "  Batch   240  of  5,000.  Loss 0.0070  Elapsed: 0:00:02.323489.\n",
      "  Batch   280  of  5,000.  Loss 0.0058  Elapsed: 0:00:02.708401.\n",
      "  Batch   320  of  5,000.  Loss 0.0515  Elapsed: 0:00:03.093364.\n",
      "  Batch   360  of  5,000.  Loss 0.0073  Elapsed: 0:00:03.476434.\n",
      "  Batch   400  of  5,000.  Loss 0.0046  Elapsed: 0:00:03.864999.\n",
      "  Batch   440  of  5,000.  Loss 0.0044  Elapsed: 0:00:04.249645.\n",
      "  Batch   480  of  5,000.  Loss 0.0049  Elapsed: 0:00:04.632822.\n",
      "  Batch   520  of  5,000.  Loss 0.0045  Elapsed: 0:00:05.018106.\n",
      "  Batch   560  of  5,000.  Loss 0.0316  Elapsed: 0:00:05.402356.\n",
      "  Batch   600  of  5,000.  Loss 0.0051  Elapsed: 0:00:05.785629.\n",
      "  Batch   640  of  5,000.  Loss 0.0064  Elapsed: 0:00:06.169352.\n",
      "  Batch   680  of  5,000.  Loss 0.0046  Elapsed: 0:00:06.554580.\n",
      "  Batch   720  of  5,000.  Loss 0.0056  Elapsed: 0:00:06.938003.\n",
      "  Batch   760  of  5,000.  Loss 0.0043  Elapsed: 0:00:07.323307.\n",
      "  Batch   800  of  5,000.  Loss 0.0098  Elapsed: 0:00:07.706237.\n",
      "  Batch   840  of  5,000.  Loss 0.0053  Elapsed: 0:00:08.093103.\n",
      "  Batch   880  of  5,000.  Loss 0.0049  Elapsed: 0:00:08.477690.\n",
      "  Batch   920  of  5,000.  Loss 0.0046  Elapsed: 0:00:08.862588.\n",
      "  Batch   960  of  5,000.  Loss 0.0059  Elapsed: 0:00:09.247883.\n",
      "  Batch 1,000  of  5,000.  Loss 0.0046  Elapsed: 0:00:09.633346.\n",
      "  Batch 1,040  of  5,000.  Loss 0.5184  Elapsed: 0:00:10.018342.\n",
      "  Batch 1,080  of  5,000.  Loss 0.0581  Elapsed: 0:00:10.404542.\n",
      "  Batch 1,120  of  5,000.  Loss 0.0117  Elapsed: 0:00:10.789418.\n",
      "  Batch 1,160  of  5,000.  Loss 0.0134  Elapsed: 0:00:11.174527.\n",
      "  Batch 1,200  of  5,000.  Loss 0.0213  Elapsed: 0:00:11.560277.\n",
      "  Batch 1,240  of  5,000.  Loss 0.0114  Elapsed: 0:00:11.944912.\n",
      "  Batch 1,280  of  5,000.  Loss 0.0071  Elapsed: 0:00:12.329260.\n",
      "  Batch 1,320  of  5,000.  Loss 0.0058  Elapsed: 0:00:12.715387.\n",
      "  Batch 1,360  of  5,000.  Loss 0.0050  Elapsed: 0:00:13.101086.\n",
      "  Batch 1,400  of  5,000.  Loss 0.0241  Elapsed: 0:00:13.487910.\n",
      "  Batch 1,440  of  5,000.  Loss 0.0064  Elapsed: 0:00:13.875149.\n",
      "  Batch 1,480  of  5,000.  Loss 0.0040  Elapsed: 0:00:14.261210.\n",
      "  Batch 1,520  of  5,000.  Loss 0.0049  Elapsed: 0:00:14.646336.\n",
      "  Batch 1,560  of  5,000.  Loss 0.0059  Elapsed: 0:00:15.031959.\n",
      "  Batch 1,600  of  5,000.  Loss 0.0145  Elapsed: 0:00:15.417335.\n",
      "  Batch 1,640  of  5,000.  Loss 0.0047  Elapsed: 0:00:15.802853.\n",
      "  Batch 1,680  of  5,000.  Loss 0.0056  Elapsed: 0:00:16.186654.\n",
      "  Batch 1,720  of  5,000.  Loss 0.0067  Elapsed: 0:00:16.571556.\n",
      "  Batch 1,760  of  5,000.  Loss 0.0054  Elapsed: 0:00:16.955573.\n",
      "  Batch 1,800  of  5,000.  Loss 0.0043  Elapsed: 0:00:17.342661.\n",
      "  Batch 1,840  of  5,000.  Loss 0.0057  Elapsed: 0:00:17.727250.\n",
      "  Batch 1,880  of  5,000.  Loss 0.0051  Elapsed: 0:00:18.113236.\n",
      "  Batch 1,920  of  5,000.  Loss 0.0047  Elapsed: 0:00:18.498539.\n",
      "  Batch 1,960  of  5,000.  Loss 0.0067  Elapsed: 0:00:18.885520.\n",
      "  Batch 2,000  of  5,000.  Loss 0.0045  Elapsed: 0:00:19.271053.\n",
      "  Batch 2,040  of  5,000.  Loss 0.0047  Elapsed: 0:00:19.657689.\n",
      "  Batch 2,080  of  5,000.  Loss 0.0715  Elapsed: 0:00:20.041499.\n",
      "  Batch 2,120  of  5,000.  Loss 0.0081  Elapsed: 0:00:20.426416.\n",
      "  Batch 2,160  of  5,000.  Loss 0.0060  Elapsed: 0:00:20.811586.\n",
      "  Batch 2,200  of  5,000.  Loss 0.0341  Elapsed: 0:00:21.198045.\n",
      "  Batch 2,240  of  5,000.  Loss 0.0062  Elapsed: 0:00:21.579996.\n",
      "  Batch 2,280  of  5,000.  Loss 0.0231  Elapsed: 0:00:21.964182.\n",
      "  Batch 2,320  of  5,000.  Loss 0.0629  Elapsed: 0:00:22.348077.\n",
      "  Batch 2,360  of  5,000.  Loss 0.0047  Elapsed: 0:00:22.733941.\n",
      "  Batch 2,400  of  5,000.  Loss 0.5191  Elapsed: 0:00:23.118561.\n",
      "  Batch 2,440  of  5,000.  Loss 0.0058  Elapsed: 0:00:23.502608.\n",
      "  Batch 2,480  of  5,000.  Loss 0.0362  Elapsed: 0:00:23.889351.\n",
      "  Batch 2,520  of  5,000.  Loss 0.0059  Elapsed: 0:00:24.271426.\n",
      "  Batch 2,560  of  5,000.  Loss 0.0052  Elapsed: 0:00:24.656529.\n",
      "  Batch 2,600  of  5,000.  Loss 0.0091  Elapsed: 0:00:25.041695.\n",
      "  Batch 2,640  of  5,000.  Loss 0.0132  Elapsed: 0:00:25.426421.\n",
      "  Batch 2,680  of  5,000.  Loss 0.0422  Elapsed: 0:00:25.810271.\n",
      "  Batch 2,720  of  5,000.  Loss 0.0075  Elapsed: 0:00:26.195568.\n",
      "  Batch 2,760  of  5,000.  Loss 0.0053  Elapsed: 0:00:26.581417.\n",
      "  Batch 2,800  of  5,000.  Loss 0.0046  Elapsed: 0:00:26.966035.\n",
      "  Batch 2,840  of  5,000.  Loss 0.0069  Elapsed: 0:00:27.352118.\n",
      "  Batch 2,880  of  5,000.  Loss 0.0053  Elapsed: 0:00:27.737466.\n",
      "  Batch 2,920  of  5,000.  Loss 0.0055  Elapsed: 0:00:28.122471.\n",
      "  Batch 2,960  of  5,000.  Loss 0.0044  Elapsed: 0:00:28.508049.\n",
      "  Batch 3,000  of  5,000.  Loss 0.0056  Elapsed: 0:00:28.893985.\n",
      "  Batch 3,040  of  5,000.  Loss 0.1519  Elapsed: 0:00:29.280544.\n",
      "  Batch 3,080  of  5,000.  Loss 0.0061  Elapsed: 0:00:29.665487.\n",
      "  Batch 3,120  of  5,000.  Loss 0.0059  Elapsed: 0:00:30.051410.\n",
      "  Batch 3,160  of  5,000.  Loss 0.0047  Elapsed: 0:00:30.437502.\n",
      "  Batch 3,200  of  5,000.  Loss 0.1454  Elapsed: 0:00:30.824556.\n",
      "  Batch 3,240  of  5,000.  Loss 0.0047  Elapsed: 0:00:31.206996.\n",
      "  Batch 3,280  of  5,000.  Loss 0.0097  Elapsed: 0:00:31.593329.\n",
      "  Batch 3,320  of  5,000.  Loss 0.0084  Elapsed: 0:00:31.979012.\n",
      "  Batch 3,360  of  5,000.  Loss 0.0217  Elapsed: 0:00:32.364633.\n",
      "  Batch 3,400  of  5,000.  Loss 0.0158  Elapsed: 0:00:32.751276.\n",
      "  Batch 3,440  of  5,000.  Loss 0.1888  Elapsed: 0:00:33.136840.\n",
      "  Batch 3,480  of  5,000.  Loss 0.0044  Elapsed: 0:00:33.522433.\n",
      "  Batch 3,520  of  5,000.  Loss 0.0057  Elapsed: 0:00:33.907257.\n",
      "  Batch 3,560  of  5,000.  Loss 0.0077  Elapsed: 0:00:34.291800.\n",
      "  Batch 3,600  of  5,000.  Loss 0.0223  Elapsed: 0:00:34.677473.\n",
      "  Batch 3,640  of  5,000.  Loss 0.0111  Elapsed: 0:00:35.062293.\n",
      "  Batch 3,680  of  5,000.  Loss 0.0043  Elapsed: 0:00:35.449276.\n",
      "  Batch 3,720  of  5,000.  Loss 0.0072  Elapsed: 0:00:35.833583.\n",
      "  Batch 3,760  of  5,000.  Loss 0.0050  Elapsed: 0:00:36.220723.\n",
      "  Batch 3,800  of  5,000.  Loss 0.0460  Elapsed: 0:00:36.606530.\n",
      "  Batch 3,840  of  5,000.  Loss 0.0046  Elapsed: 0:00:36.992580.\n",
      "  Batch 3,880  of  5,000.  Loss 0.0074  Elapsed: 0:00:37.377441.\n",
      "  Batch 3,920  of  5,000.  Loss 0.0051  Elapsed: 0:00:37.763998.\n",
      "  Batch 3,960  of  5,000.  Loss 0.0067  Elapsed: 0:00:38.149773.\n",
      "  Batch 4,000  of  5,000.  Loss 0.0045  Elapsed: 0:00:38.534338.\n",
      "  Batch 4,040  of  5,000.  Loss 0.0583  Elapsed: 0:00:38.919001.\n",
      "  Batch 4,080  of  5,000.  Loss 0.1736  Elapsed: 0:00:39.303079.\n",
      "  Batch 4,120  of  5,000.  Loss 0.0756  Elapsed: 0:00:39.688739.\n",
      "  Batch 4,160  of  5,000.  Loss 0.0668  Elapsed: 0:00:40.074247.\n",
      "  Batch 4,200  of  5,000.  Loss 0.0160  Elapsed: 0:00:40.457537.\n",
      "  Batch 4,240  of  5,000.  Loss 0.0069  Elapsed: 0:00:40.841806.\n",
      "  Batch 4,280  of  5,000.  Loss 0.0058  Elapsed: 0:00:41.228153.\n",
      "  Batch 4,320  of  5,000.  Loss 0.0057  Elapsed: 0:00:41.611291.\n",
      "  Batch 4,360  of  5,000.  Loss 0.0102  Elapsed: 0:00:41.997075.\n",
      "  Batch 4,400  of  5,000.  Loss 0.0044  Elapsed: 0:00:42.386000.\n",
      "  Batch 4,440  of  5,000.  Loss 0.0059  Elapsed: 0:00:42.769636.\n",
      "  Batch 4,480  of  5,000.  Loss 0.0048  Elapsed: 0:00:43.155157.\n",
      "  Batch 4,520  of  5,000.  Loss 0.0471  Elapsed: 0:00:43.541794.\n",
      "  Batch 4,560  of  5,000.  Loss 0.0265  Elapsed: 0:00:43.929461.\n",
      "  Batch 4,600  of  5,000.  Loss 0.0505  Elapsed: 0:00:44.315036.\n",
      "  Batch 4,640  of  5,000.  Loss 0.0451  Elapsed: 0:00:44.700202.\n",
      "  Batch 4,680  of  5,000.  Loss 0.0045  Elapsed: 0:00:45.085190.\n",
      "  Batch 4,720  of  5,000.  Loss 0.0045  Elapsed: 0:00:45.471946.\n",
      "  Batch 4,760  of  5,000.  Loss 0.0130  Elapsed: 0:00:45.856895.\n",
      "  Batch 4,800  of  5,000.  Loss 0.3407  Elapsed: 0:00:46.241444.\n",
      "  Batch 4,840  of  5,000.  Loss 0.0592  Elapsed: 0:00:46.626635.\n",
      "  Batch 4,880  of  5,000.  Loss 0.0321  Elapsed: 0:00:47.012462.\n",
      "  Batch 4,920  of  5,000.  Loss 0.0054  Elapsed: 0:00:47.397928.\n",
      "  Batch 4,960  of  5,000.  Loss 0.0063  Elapsed: 0:00:47.784090.\n",
      "Avg Validation Loss 0.0419, Completed in 0:00:48.159308 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.962335\n",
      "F1 Score (micro) =   0.963523\n",
      "F1 Score (macro) =   0.908256\n",
      "F1 Score (samples) =   0.954800\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0048  Elapsed: 0:00:00.398177.\n",
      "  Batch    80  of  1,477.  Loss 0.0049  Elapsed: 0:00:00.783617.\n",
      "  Batch   120  of  1,477.  Loss 0.0250  Elapsed: 0:00:01.170751.\n",
      "  Batch   160  of  1,477.  Loss 0.0043  Elapsed: 0:00:01.558276.\n",
      "  Batch   200  of  1,477.  Loss 0.6332  Elapsed: 0:00:01.945905.\n",
      "  Batch   240  of  1,477.  Loss 1.0700  Elapsed: 0:00:02.332315.\n",
      "  Batch   280  of  1,477.  Loss 0.4532  Elapsed: 0:00:02.721554.\n",
      "  Batch   320  of  1,477.  Loss 0.0523  Elapsed: 0:00:03.108798.\n",
      "  Batch   360  of  1,477.  Loss 0.0172  Elapsed: 0:00:03.495131.\n",
      "  Batch   400  of  1,477.  Loss 0.0052  Elapsed: 0:00:03.880830.\n",
      "  Batch   440  of  1,477.  Loss 0.0080  Elapsed: 0:00:04.267525.\n",
      "  Batch   480  of  1,477.  Loss 0.8749  Elapsed: 0:00:04.657056.\n",
      "  Batch   520  of  1,477.  Loss 0.0046  Elapsed: 0:00:05.041830.\n",
      "  Batch   560  of  1,477.  Loss 0.0047  Elapsed: 0:00:05.428308.\n",
      "  Batch   600  of  1,477.  Loss 0.0044  Elapsed: 0:00:05.816941.\n",
      "  Batch   640  of  1,477.  Loss 1.2326  Elapsed: 0:00:06.204568.\n",
      "  Batch   680  of  1,477.  Loss 1.2445  Elapsed: 0:00:06.589796.\n",
      "  Batch   720  of  1,477.  Loss 0.2365  Elapsed: 0:00:06.975513.\n",
      "  Batch   760  of  1,477.  Loss 0.0060  Elapsed: 0:00:07.362232.\n",
      "  Batch   800  of  1,477.  Loss 0.0049  Elapsed: 0:00:07.746189.\n",
      "  Batch   840  of  1,477.  Loss 0.9317  Elapsed: 0:00:08.131500.\n",
      "  Batch   880  of  1,477.  Loss 0.2407  Elapsed: 0:00:08.518573.\n",
      "  Batch   920  of  1,477.  Loss 0.0152  Elapsed: 0:00:08.905401.\n",
      "  Batch   960  of  1,477.  Loss 0.1057  Elapsed: 0:00:09.291418.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0332  Elapsed: 0:00:09.678078.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0329  Elapsed: 0:00:10.063091.\n",
      "  Batch 1,080  of  1,477.  Loss 1.0094  Elapsed: 0:00:10.448385.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0068  Elapsed: 0:00:10.832438.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0217  Elapsed: 0:00:11.218352.\n",
      "  Batch 1,200  of  1,477.  Loss 0.1649  Elapsed: 0:00:11.603285.\n",
      "  Batch 1,240  of  1,477.  Loss 0.8271  Elapsed: 0:00:11.990760.\n",
      "  Batch 1,280  of  1,477.  Loss 0.7606  Elapsed: 0:00:12.378031.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0148  Elapsed: 0:00:12.765648.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0265  Elapsed: 0:00:13.151101.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0042  Elapsed: 0:00:13.536826.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0057  Elapsed: 0:00:13.920038.\n",
      "Avg Validation Loss 0.2309, Completed in 0:00:14.267711 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.734922\n",
      "F1 Score (micro) =   0.751131\n",
      "F1 Score (macro) =   0.592452\n",
      "F1 Score (samples) =   0.728278\n",
      "Running Experiment TALES_6000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    375.  Loss 0.2490  Elapsed: 0:00:26.861931.\n",
      "Epoch  0  Batch   200  of    375.  Loss 0.2688  Elapsed: 0:00:53.627942.\n",
      "Epoch  0  Batch   300  of    375.  Loss 0.1491  Elapsed: 0:01:20.391167.\n",
      "Avg Training Loss 0.2655, Completed in 0:01:40.374070 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1997  Elapsed: 0:00:03.691845.\n",
      "  Batch    80  of     93.  Loss 0.2431  Elapsed: 0:00:07.292943.\n",
      "Avg Validation Loss 0.2672, Completed in 0:00:08.308905 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.477270\n",
      "F1 Score (micro) =   0.579929\n",
      "F1 Score (macro) =   0.211234\n",
      "F1 Score (samples) =   0.494922\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    375.  Loss 0.2231  Elapsed: 0:00:26.854649.\n",
      "Epoch  1  Batch   200  of    375.  Loss 0.1661  Elapsed: 0:00:53.616568.\n",
      "Epoch  1  Batch   300  of    375.  Loss 0.2583  Elapsed: 0:01:20.397671.\n",
      "Avg Training Loss 0.2043, Completed in 0:01:40.395249 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1875  Elapsed: 0:00:03.686364.\n",
      "  Batch    80  of     93.  Loss 0.2993  Elapsed: 0:00:07.284251.\n",
      "Avg Validation Loss 0.2297, Completed in 0:00:08.305105 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.617996\n",
      "F1 Score (micro) =   0.665077\n",
      "F1 Score (macro) =   0.398247\n",
      "F1 Score (samples) =   0.566464\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    375.  Loss 0.0963  Elapsed: 0:00:26.860413.\n",
      "Epoch  2  Batch   200  of    375.  Loss 0.1546  Elapsed: 0:00:53.634167.\n",
      "Epoch  2  Batch   300  of    375.  Loss 0.2270  Elapsed: 0:01:20.403949.\n",
      "Avg Training Loss 0.1438, Completed in 0:01:40.376876 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1906  Elapsed: 0:00:03.689511.\n",
      "  Batch    80  of     93.  Loss 0.0616  Elapsed: 0:00:07.279794.\n",
      "Avg Validation Loss 0.2134, Completed in 0:00:08.300221 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.694918\n",
      "F1 Score (micro) =   0.728448\n",
      "F1 Score (macro) =   0.498807\n",
      "F1 Score (samples) =   0.685624\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    375.  Loss 0.0974  Elapsed: 0:00:26.861796.\n",
      "Epoch  3  Batch   200  of    375.  Loss 0.0516  Elapsed: 0:00:53.620518.\n",
      "Epoch  3  Batch   300  of    375.  Loss 0.0458  Elapsed: 0:01:20.379035.\n",
      "Avg Training Loss 0.0883, Completed in 0:01:40.371679 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3066  Elapsed: 0:00:03.682948.\n",
      "  Batch    80  of     93.  Loss 0.1732  Elapsed: 0:00:07.282381.\n",
      "Avg Validation Loss 0.2246, Completed in 0:00:08.302389 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.736291\n",
      "F1 Score (micro) =   0.737255\n",
      "F1 Score (macro) =   0.648353\n",
      "F1 Score (samples) =   0.698939\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  6,000.  Loss 0.1121  Elapsed: 0:00:00.395730.\n",
      "  Batch    80  of  6,000.  Loss 0.0104  Elapsed: 0:00:00.780502.\n",
      "  Batch   120  of  6,000.  Loss 0.0048  Elapsed: 0:00:01.165737.\n",
      "  Batch   160  of  6,000.  Loss 0.0045  Elapsed: 0:00:01.551179.\n",
      "  Batch   200  of  6,000.  Loss 0.0056  Elapsed: 0:00:01.938155.\n",
      "  Batch   240  of  6,000.  Loss 0.0259  Elapsed: 0:00:02.319911.\n",
      "  Batch   280  of  6,000.  Loss 0.0110  Elapsed: 0:00:02.705063.\n",
      "  Batch   320  of  6,000.  Loss 0.1452  Elapsed: 0:00:03.092650.\n",
      "  Batch   360  of  6,000.  Loss 0.0422  Elapsed: 0:00:03.479106.\n",
      "  Batch   400  of  6,000.  Loss 0.6411  Elapsed: 0:00:03.864573.\n",
      "  Batch   440  of  6,000.  Loss 0.0039  Elapsed: 0:00:04.249630.\n",
      "  Batch   480  of  6,000.  Loss 0.0038  Elapsed: 0:00:04.635696.\n",
      "  Batch   520  of  6,000.  Loss 0.0051  Elapsed: 0:00:05.018743.\n",
      "  Batch   560  of  6,000.  Loss 0.0041  Elapsed: 0:00:05.402644.\n",
      "  Batch   600  of  6,000.  Loss 0.0055  Elapsed: 0:00:05.787565.\n",
      "  Batch   640  of  6,000.  Loss 0.0063  Elapsed: 0:00:06.172542.\n",
      "  Batch   680  of  6,000.  Loss 0.0232  Elapsed: 0:00:06.556927.\n",
      "  Batch   720  of  6,000.  Loss 0.0085  Elapsed: 0:00:06.941848.\n",
      "  Batch   760  of  6,000.  Loss 0.0225  Elapsed: 0:00:07.327891.\n",
      "  Batch   800  of  6,000.  Loss 0.0036  Elapsed: 0:00:07.714257.\n",
      "  Batch   840  of  6,000.  Loss 0.0040  Elapsed: 0:00:08.099747.\n",
      "  Batch   880  of  6,000.  Loss 0.0349  Elapsed: 0:00:08.483968.\n",
      "  Batch   920  of  6,000.  Loss 0.0289  Elapsed: 0:00:08.868698.\n",
      "  Batch   960  of  6,000.  Loss 0.0071  Elapsed: 0:00:09.254449.\n",
      "  Batch 1,000  of  6,000.  Loss 0.0035  Elapsed: 0:00:09.639333.\n",
      "  Batch 1,040  of  6,000.  Loss 0.0109  Elapsed: 0:00:10.025579.\n",
      "  Batch 1,080  of  6,000.  Loss 0.0503  Elapsed: 0:00:10.411289.\n",
      "  Batch 1,120  of  6,000.  Loss 0.0059  Elapsed: 0:00:10.797597.\n",
      "  Batch 1,160  of  6,000.  Loss 0.0212  Elapsed: 0:00:11.182589.\n",
      "  Batch 1,200  of  6,000.  Loss 0.0251  Elapsed: 0:00:11.566543.\n",
      "  Batch 1,240  of  6,000.  Loss 0.0648  Elapsed: 0:00:11.953815.\n",
      "  Batch 1,280  of  6,000.  Loss 0.3784  Elapsed: 0:00:12.340330.\n",
      "  Batch 1,320  of  6,000.  Loss 0.0993  Elapsed: 0:00:12.725154.\n",
      "  Batch 1,360  of  6,000.  Loss 0.0175  Elapsed: 0:00:13.112438.\n",
      "  Batch 1,400  of  6,000.  Loss 0.0045  Elapsed: 0:00:13.496289.\n",
      "  Batch 1,440  of  6,000.  Loss 0.0423  Elapsed: 0:00:13.882780.\n",
      "  Batch 1,480  of  6,000.  Loss 0.0050  Elapsed: 0:00:14.265681.\n",
      "  Batch 1,520  of  6,000.  Loss 0.0035  Elapsed: 0:00:14.650610.\n",
      "  Batch 1,560  of  6,000.  Loss 0.0161  Elapsed: 0:00:15.034202.\n",
      "  Batch 1,600  of  6,000.  Loss 0.0044  Elapsed: 0:00:15.420548.\n",
      "  Batch 1,640  of  6,000.  Loss 0.0112  Elapsed: 0:00:15.805105.\n",
      "  Batch 1,680  of  6,000.  Loss 0.0320  Elapsed: 0:00:16.188451.\n",
      "  Batch 1,720  of  6,000.  Loss 0.0079  Elapsed: 0:00:16.571654.\n",
      "  Batch 1,760  of  6,000.  Loss 0.0200  Elapsed: 0:00:16.957556.\n",
      "  Batch 1,800  of  6,000.  Loss 0.1012  Elapsed: 0:00:17.344879.\n",
      "  Batch 1,840  of  6,000.  Loss 0.0033  Elapsed: 0:00:17.731854.\n",
      "  Batch 1,880  of  6,000.  Loss 0.0321  Elapsed: 0:00:18.116817.\n",
      "  Batch 1,920  of  6,000.  Loss 0.0053  Elapsed: 0:00:18.502180.\n",
      "  Batch 1,960  of  6,000.  Loss 0.0035  Elapsed: 0:00:18.887117.\n",
      "  Batch 2,000  of  6,000.  Loss 0.0075  Elapsed: 0:00:19.273181.\n",
      "  Batch 2,040  of  6,000.  Loss 0.1540  Elapsed: 0:00:19.656906.\n",
      "  Batch 2,080  of  6,000.  Loss 0.0035  Elapsed: 0:00:20.042265.\n",
      "  Batch 2,120  of  6,000.  Loss 0.0037  Elapsed: 0:00:20.427320.\n",
      "  Batch 2,160  of  6,000.  Loss 0.0043  Elapsed: 0:00:20.814570.\n",
      "  Batch 2,200  of  6,000.  Loss 0.0570  Elapsed: 0:00:21.198631.\n",
      "  Batch 2,240  of  6,000.  Loss 0.0043  Elapsed: 0:00:21.580965.\n",
      "  Batch 2,280  of  6,000.  Loss 0.0888  Elapsed: 0:00:21.965050.\n",
      "  Batch 2,320  of  6,000.  Loss 0.0034  Elapsed: 0:00:22.352198.\n",
      "  Batch 2,360  of  6,000.  Loss 0.0251  Elapsed: 0:00:22.736920.\n",
      "  Batch 2,400  of  6,000.  Loss 0.0130  Elapsed: 0:00:23.121499.\n",
      "  Batch 2,440  of  6,000.  Loss 0.0141  Elapsed: 0:00:23.507575.\n",
      "  Batch 2,480  of  6,000.  Loss 0.0387  Elapsed: 0:00:23.892973.\n",
      "  Batch 2,520  of  6,000.  Loss 0.0042  Elapsed: 0:00:24.279581.\n",
      "  Batch 2,560  of  6,000.  Loss 0.0154  Elapsed: 0:00:24.663138.\n",
      "  Batch 2,600  of  6,000.  Loss 0.0033  Elapsed: 0:00:25.048094.\n",
      "  Batch 2,640  of  6,000.  Loss 0.0347  Elapsed: 0:00:25.437793.\n",
      "  Batch 2,680  of  6,000.  Loss 0.0038  Elapsed: 0:00:25.825405.\n",
      "  Batch 2,720  of  6,000.  Loss 0.0038  Elapsed: 0:00:26.209079.\n",
      "  Batch 2,760  of  6,000.  Loss 0.0037  Elapsed: 0:00:26.593345.\n",
      "  Batch 2,800  of  6,000.  Loss 0.0062  Elapsed: 0:00:26.977957.\n",
      "  Batch 2,840  of  6,000.  Loss 0.0069  Elapsed: 0:00:27.362096.\n",
      "  Batch 2,880  of  6,000.  Loss 0.0224  Elapsed: 0:00:27.746149.\n",
      "  Batch 2,920  of  6,000.  Loss 0.0075  Elapsed: 0:00:28.130907.\n",
      "  Batch 2,960  of  6,000.  Loss 0.0045  Elapsed: 0:00:28.517461.\n",
      "  Batch 3,000  of  6,000.  Loss 0.1693  Elapsed: 0:00:28.903218.\n",
      "  Batch 3,040  of  6,000.  Loss 0.1351  Elapsed: 0:00:29.289398.\n",
      "  Batch 3,080  of  6,000.  Loss 0.1238  Elapsed: 0:00:29.674431.\n",
      "  Batch 3,120  of  6,000.  Loss 0.0615  Elapsed: 0:00:30.058584.\n",
      "  Batch 3,160  of  6,000.  Loss 0.0160  Elapsed: 0:00:30.444227.\n",
      "  Batch 3,200  of  6,000.  Loss 0.0040  Elapsed: 0:00:30.831190.\n",
      "  Batch 3,240  of  6,000.  Loss 0.0051  Elapsed: 0:00:31.216374.\n",
      "  Batch 3,280  of  6,000.  Loss 0.0369  Elapsed: 0:00:31.601044.\n",
      "  Batch 3,320  of  6,000.  Loss 0.0120  Elapsed: 0:00:31.986300.\n",
      "  Batch 3,360  of  6,000.  Loss 0.0528  Elapsed: 0:00:32.373517.\n",
      "  Batch 3,400  of  6,000.  Loss 0.5340  Elapsed: 0:00:32.758379.\n",
      "  Batch 3,440  of  6,000.  Loss 0.0287  Elapsed: 0:00:33.146531.\n",
      "  Batch 3,480  of  6,000.  Loss 0.0034  Elapsed: 0:00:33.531046.\n",
      "  Batch 3,520  of  6,000.  Loss 0.0056  Elapsed: 0:00:33.918452.\n",
      "  Batch 3,560  of  6,000.  Loss 0.1367  Elapsed: 0:00:34.305389.\n",
      "  Batch 3,600  of  6,000.  Loss 0.0040  Elapsed: 0:00:34.691195.\n",
      "  Batch 3,640  of  6,000.  Loss 0.0672  Elapsed: 0:00:35.077017.\n",
      "  Batch 3,680  of  6,000.  Loss 0.0298  Elapsed: 0:00:35.462412.\n",
      "  Batch 3,720  of  6,000.  Loss 0.0036  Elapsed: 0:00:35.848351.\n",
      "  Batch 3,760  of  6,000.  Loss 0.0342  Elapsed: 0:00:36.234986.\n",
      "  Batch 3,800  of  6,000.  Loss 0.0080  Elapsed: 0:00:36.619858.\n",
      "  Batch 3,840  of  6,000.  Loss 0.8298  Elapsed: 0:00:37.005300.\n",
      "  Batch 3,880  of  6,000.  Loss 0.0144  Elapsed: 0:00:37.391293.\n",
      "  Batch 3,920  of  6,000.  Loss 0.0181  Elapsed: 0:00:37.778029.\n",
      "  Batch 3,960  of  6,000.  Loss 0.0036  Elapsed: 0:00:38.164314.\n",
      "  Batch 4,000  of  6,000.  Loss 0.0068  Elapsed: 0:00:38.549734.\n",
      "  Batch 4,040  of  6,000.  Loss 0.0048  Elapsed: 0:00:38.936246.\n",
      "  Batch 4,080  of  6,000.  Loss 0.0047  Elapsed: 0:00:39.321073.\n",
      "  Batch 4,120  of  6,000.  Loss 0.0044  Elapsed: 0:00:39.706099.\n",
      "  Batch 4,160  of  6,000.  Loss 0.0160  Elapsed: 0:00:40.092777.\n",
      "  Batch 4,200  of  6,000.  Loss 0.0051  Elapsed: 0:00:40.476412.\n",
      "  Batch 4,240  of  6,000.  Loss 0.0133  Elapsed: 0:00:40.861828.\n",
      "  Batch 4,280  of  6,000.  Loss 0.0049  Elapsed: 0:00:41.246540.\n",
      "  Batch 4,320  of  6,000.  Loss 0.0297  Elapsed: 0:00:41.633536.\n",
      "  Batch 4,360  of  6,000.  Loss 0.0338  Elapsed: 0:00:42.017235.\n",
      "  Batch 4,400  of  6,000.  Loss 0.0046  Elapsed: 0:00:42.402653.\n",
      "  Batch 4,440  of  6,000.  Loss 0.0436  Elapsed: 0:00:42.789988.\n",
      "  Batch 4,480  of  6,000.  Loss 0.0179  Elapsed: 0:00:43.177204.\n",
      "  Batch 4,520  of  6,000.  Loss 0.0075  Elapsed: 0:00:43.561627.\n",
      "  Batch 4,560  of  6,000.  Loss 0.2045  Elapsed: 0:00:43.946601.\n",
      "  Batch 4,600  of  6,000.  Loss 0.0143  Elapsed: 0:00:44.332909.\n",
      "  Batch 4,640  of  6,000.  Loss 0.0061  Elapsed: 0:00:44.718302.\n",
      "  Batch 4,680  of  6,000.  Loss 0.0033  Elapsed: 0:00:45.102555.\n",
      "  Batch 4,720  of  6,000.  Loss 0.0370  Elapsed: 0:00:45.487640.\n",
      "  Batch 4,760  of  6,000.  Loss 0.0148  Elapsed: 0:00:45.873278.\n",
      "  Batch 4,800  of  6,000.  Loss 0.0188  Elapsed: 0:00:46.259661.\n",
      "  Batch 4,840  of  6,000.  Loss 0.0046  Elapsed: 0:00:46.645047.\n",
      "  Batch 4,880  of  6,000.  Loss 0.0040  Elapsed: 0:00:47.030583.\n",
      "  Batch 4,920  of  6,000.  Loss 0.0043  Elapsed: 0:00:47.415620.\n",
      "  Batch 4,960  of  6,000.  Loss 0.0054  Elapsed: 0:00:47.801250.\n",
      "  Batch 5,000  of  6,000.  Loss 0.7204  Elapsed: 0:00:48.186675.\n",
      "  Batch 5,040  of  6,000.  Loss 0.1792  Elapsed: 0:00:48.571980.\n",
      "  Batch 5,080  of  6,000.  Loss 0.0174  Elapsed: 0:00:48.959564.\n",
      "  Batch 5,120  of  6,000.  Loss 0.0058  Elapsed: 0:00:49.346121.\n",
      "  Batch 5,160  of  6,000.  Loss 0.0134  Elapsed: 0:00:49.729852.\n",
      "  Batch 5,200  of  6,000.  Loss 0.0540  Elapsed: 0:00:50.115004.\n",
      "  Batch 5,240  of  6,000.  Loss 0.0046  Elapsed: 0:00:50.497956.\n",
      "  Batch 5,280  of  6,000.  Loss 0.0157  Elapsed: 0:00:50.882978.\n",
      "  Batch 5,320  of  6,000.  Loss 0.0212  Elapsed: 0:00:51.267546.\n",
      "  Batch 5,360  of  6,000.  Loss 0.0115  Elapsed: 0:00:51.654583.\n",
      "  Batch 5,400  of  6,000.  Loss 0.0468  Elapsed: 0:00:52.041732.\n",
      "  Batch 5,440  of  6,000.  Loss 0.0076  Elapsed: 0:00:52.428098.\n",
      "  Batch 5,480  of  6,000.  Loss 0.0648  Elapsed: 0:00:52.813809.\n",
      "  Batch 5,520  of  6,000.  Loss 0.0205  Elapsed: 0:00:53.199184.\n",
      "  Batch 5,560  of  6,000.  Loss 0.0149  Elapsed: 0:00:53.582024.\n",
      "  Batch 5,600  of  6,000.  Loss 0.0256  Elapsed: 0:00:53.967630.\n",
      "  Batch 5,640  of  6,000.  Loss 0.0050  Elapsed: 0:00:54.352742.\n",
      "  Batch 5,680  of  6,000.  Loss 0.0420  Elapsed: 0:00:54.742630.\n",
      "  Batch 5,720  of  6,000.  Loss 0.0382  Elapsed: 0:00:55.127073.\n",
      "  Batch 5,760  of  6,000.  Loss 0.0737  Elapsed: 0:00:55.511060.\n",
      "  Batch 5,800  of  6,000.  Loss 0.0175  Elapsed: 0:00:55.898156.\n",
      "  Batch 5,840  of  6,000.  Loss 0.0302  Elapsed: 0:00:56.285014.\n",
      "  Batch 5,880  of  6,000.  Loss 0.0555  Elapsed: 0:00:56.671157.\n",
      "  Batch 5,920  of  6,000.  Loss 0.1108  Elapsed: 0:00:57.057109.\n",
      "  Batch 5,960  of  6,000.  Loss 0.0229  Elapsed: 0:00:57.442950.\n",
      "Avg Validation Loss 0.0485, Completed in 0:00:57.819378 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.956266\n",
      "F1 Score (micro) =   0.956199\n",
      "F1 Score (macro) =   0.922339\n",
      "F1 Score (samples) =   0.944944\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0081  Elapsed: 0:00:00.403363.\n",
      "  Batch    80  of  1,477.  Loss 0.5600  Elapsed: 0:00:00.788116.\n",
      "  Batch   120  of  1,477.  Loss 0.0449  Elapsed: 0:00:01.174306.\n",
      "  Batch   160  of  1,477.  Loss 0.0056  Elapsed: 0:00:01.557778.\n",
      "  Batch   200  of  1,477.  Loss 0.1231  Elapsed: 0:00:01.942726.\n",
      "  Batch   240  of  1,477.  Loss 1.2078  Elapsed: 0:00:02.329098.\n",
      "  Batch   280  of  1,477.  Loss 0.0916  Elapsed: 0:00:02.713891.\n",
      "  Batch   320  of  1,477.  Loss 1.2866  Elapsed: 0:00:03.098652.\n",
      "  Batch   360  of  1,477.  Loss 0.0331  Elapsed: 0:00:03.484416.\n",
      "  Batch   400  of  1,477.  Loss 0.0300  Elapsed: 0:00:03.871590.\n",
      "  Batch   440  of  1,477.  Loss 0.0658  Elapsed: 0:00:04.257211.\n",
      "  Batch   480  of  1,477.  Loss 0.0063  Elapsed: 0:00:04.643746.\n",
      "  Batch   520  of  1,477.  Loss 0.0379  Elapsed: 0:00:05.030373.\n",
      "  Batch   560  of  1,477.  Loss 0.0077  Elapsed: 0:00:05.416617.\n",
      "  Batch   600  of  1,477.  Loss 0.0052  Elapsed: 0:00:05.803217.\n",
      "  Batch   640  of  1,477.  Loss 0.0059  Elapsed: 0:00:06.189424.\n",
      "  Batch   680  of  1,477.  Loss 0.0139  Elapsed: 0:00:06.577768.\n",
      "  Batch   720  of  1,477.  Loss 0.0054  Elapsed: 0:00:06.962636.\n",
      "  Batch   760  of  1,477.  Loss 0.0034  Elapsed: 0:00:07.349126.\n",
      "  Batch   800  of  1,477.  Loss 0.0114  Elapsed: 0:00:07.735259.\n",
      "  Batch   840  of  1,477.  Loss 0.0328  Elapsed: 0:00:08.120481.\n",
      "  Batch   880  of  1,477.  Loss 0.0913  Elapsed: 0:00:08.505057.\n",
      "  Batch   920  of  1,477.  Loss 0.0210  Elapsed: 0:00:08.891198.\n",
      "  Batch   960  of  1,477.  Loss 0.0036  Elapsed: 0:00:09.274932.\n",
      "  Batch 1,000  of  1,477.  Loss 0.7540  Elapsed: 0:00:09.659952.\n",
      "  Batch 1,040  of  1,477.  Loss 0.1904  Elapsed: 0:00:10.046519.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0032  Elapsed: 0:00:10.432600.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0068  Elapsed: 0:00:10.815913.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0524  Elapsed: 0:00:11.201474.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0532  Elapsed: 0:00:11.585158.\n",
      "  Batch 1,240  of  1,477.  Loss 0.1952  Elapsed: 0:00:11.973785.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0776  Elapsed: 0:00:12.357810.\n",
      "  Batch 1,320  of  1,477.  Loss 0.4202  Elapsed: 0:00:12.743832.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0377  Elapsed: 0:00:13.131270.\n",
      "  Batch 1,400  of  1,477.  Loss 0.5174  Elapsed: 0:00:13.517603.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0065  Elapsed: 0:00:13.903375.\n",
      "Avg Validation Loss 0.2182, Completed in 0:00:14.250870 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.742159\n",
      "F1 Score (micro) =   0.741774\n",
      "F1 Score (macro) =   0.640216\n",
      "F1 Score (samples) =   0.700970\n",
      "Running Experiment TALES_7000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    438.  Loss 0.2959  Elapsed: 0:00:26.865139.\n",
      "Epoch  0  Batch   200  of    438.  Loss 0.4005  Elapsed: 0:00:53.621309.\n",
      "Epoch  0  Batch   300  of    438.  Loss 0.3264  Elapsed: 0:01:20.373608.\n",
      "Epoch  0  Batch   400  of    438.  Loss 0.1379  Elapsed: 0:01:47.135413.\n",
      "Avg Training Loss 0.2646, Completed in 0:01:57.091709 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2365  Elapsed: 0:00:03.689719.\n",
      "  Batch    80  of     93.  Loss 0.2325  Elapsed: 0:00:07.286392.\n",
      "Avg Validation Loss 0.2619, Completed in 0:00:08.303212 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.530416\n",
      "F1 Score (micro) =   0.612571\n",
      "F1 Score (macro) =   0.284598\n",
      "F1 Score (samples) =   0.543218\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    438.  Loss 0.1662  Elapsed: 0:00:26.871250.\n",
      "Epoch  1  Batch   200  of    438.  Loss 0.1883  Elapsed: 0:00:53.648985.\n",
      "Epoch  1  Batch   300  of    438.  Loss 0.2504  Elapsed: 0:01:20.399653.\n",
      "Epoch  1  Batch   400  of    438.  Loss 0.2491  Elapsed: 0:01:47.163460.\n",
      "Avg Training Loss 0.2014, Completed in 0:01:57.119514 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1721  Elapsed: 0:00:03.688714.\n",
      "  Batch    80  of     93.  Loss 0.2010  Elapsed: 0:00:07.282006.\n",
      "Avg Validation Loss 0.2235, Completed in 0:00:08.305157 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.627740\n",
      "F1 Score (micro) =   0.686702\n",
      "F1 Score (macro) =   0.398393\n",
      "F1 Score (samples) =   0.618596\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    438.  Loss 0.1027  Elapsed: 0:00:26.849323.\n",
      "Epoch  2  Batch   200  of    438.  Loss 0.0598  Elapsed: 0:00:53.614459.\n",
      "Epoch  2  Batch   300  of    438.  Loss 0.1926  Elapsed: 0:01:20.386131.\n",
      "Epoch  2  Batch   400  of    438.  Loss 0.0993  Elapsed: 0:01:47.150480.\n",
      "Avg Training Loss 0.1419, Completed in 0:01:57.099738 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3238  Elapsed: 0:00:03.690061.\n",
      "  Batch    80  of     93.  Loss 0.2092  Elapsed: 0:00:07.285660.\n",
      "Avg Validation Loss 0.1924, Completed in 0:00:08.304293 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.747355\n",
      "F1 Score (micro) =   0.758870\n",
      "F1 Score (macro) =   0.617633\n",
      "F1 Score (samples) =   0.708418\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    438.  Loss 0.0885  Elapsed: 0:00:26.865488.\n",
      "Epoch  3  Batch   200  of    438.  Loss 0.0461  Elapsed: 0:00:53.626358.\n",
      "Epoch  3  Batch   300  of    438.  Loss 0.0317  Elapsed: 0:01:20.405352.\n",
      "Epoch  3  Batch   400  of    438.  Loss 0.2198  Elapsed: 0:01:47.178501.\n",
      "Avg Training Loss 0.0835, Completed in 0:01:57.132232 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1656  Elapsed: 0:00:03.682590.\n",
      "  Batch    80  of     93.  Loss 0.1420  Elapsed: 0:00:07.283108.\n",
      "Avg Validation Loss 0.2064, Completed in 0:00:08.302913 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.769144\n",
      "F1 Score (micro) =   0.776265\n",
      "F1 Score (macro) =   0.652001\n",
      "F1 Score (samples) =   0.751975\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  7,000.  Loss 0.0049  Elapsed: 0:00:00.394203.\n",
      "  Batch    80  of  7,000.  Loss 0.5957  Elapsed: 0:00:00.779822.\n",
      "  Batch   120  of  7,000.  Loss 0.0039  Elapsed: 0:00:01.166142.\n",
      "  Batch   160  of  7,000.  Loss 0.4910  Elapsed: 0:00:01.552672.\n",
      "  Batch   200  of  7,000.  Loss 0.0034  Elapsed: 0:00:01.935503.\n",
      "  Batch   240  of  7,000.  Loss 0.0036  Elapsed: 0:00:02.319899.\n",
      "  Batch   280  of  7,000.  Loss 0.0045  Elapsed: 0:00:02.702761.\n",
      "  Batch   320  of  7,000.  Loss 0.0070  Elapsed: 0:00:03.087703.\n",
      "  Batch   360  of  7,000.  Loss 0.0243  Elapsed: 0:00:03.474341.\n",
      "  Batch   400  of  7,000.  Loss 0.0036  Elapsed: 0:00:03.859123.\n",
      "  Batch   440  of  7,000.  Loss 0.0034  Elapsed: 0:00:04.244919.\n",
      "  Batch   480  of  7,000.  Loss 0.0072  Elapsed: 0:00:04.630317.\n",
      "  Batch   520  of  7,000.  Loss 0.0036  Elapsed: 0:00:05.016210.\n",
      "  Batch   560  of  7,000.  Loss 0.7151  Elapsed: 0:00:05.400232.\n",
      "  Batch   600  of  7,000.  Loss 0.0034  Elapsed: 0:00:05.785846.\n",
      "  Batch   640  of  7,000.  Loss 0.0056  Elapsed: 0:00:06.170857.\n",
      "  Batch   680  of  7,000.  Loss 0.0300  Elapsed: 0:00:06.556518.\n",
      "  Batch   720  of  7,000.  Loss 0.0033  Elapsed: 0:00:06.940874.\n",
      "  Batch   760  of  7,000.  Loss 0.4844  Elapsed: 0:00:07.325594.\n",
      "  Batch   800  of  7,000.  Loss 0.0033  Elapsed: 0:00:07.709066.\n",
      "  Batch   840  of  7,000.  Loss 0.0282  Elapsed: 0:00:08.094916.\n",
      "  Batch   880  of  7,000.  Loss 0.0379  Elapsed: 0:00:08.480909.\n",
      "  Batch   920  of  7,000.  Loss 0.5089  Elapsed: 0:00:08.867788.\n",
      "  Batch   960  of  7,000.  Loss 0.0332  Elapsed: 0:00:09.252263.\n",
      "  Batch 1,000  of  7,000.  Loss 0.0616  Elapsed: 0:00:09.635189.\n",
      "  Batch 1,040  of  7,000.  Loss 0.0039  Elapsed: 0:00:10.020251.\n",
      "  Batch 1,080  of  7,000.  Loss 0.0236  Elapsed: 0:00:10.405822.\n",
      "  Batch 1,120  of  7,000.  Loss 0.0346  Elapsed: 0:00:10.792962.\n",
      "  Batch 1,160  of  7,000.  Loss 0.0041  Elapsed: 0:00:11.177976.\n",
      "  Batch 1,200  of  7,000.  Loss 0.0038  Elapsed: 0:00:11.562935.\n",
      "  Batch 1,240  of  7,000.  Loss 0.0052  Elapsed: 0:00:11.948317.\n",
      "  Batch 1,280  of  7,000.  Loss 0.0253  Elapsed: 0:00:12.331947.\n",
      "  Batch 1,320  of  7,000.  Loss 0.0521  Elapsed: 0:00:12.717559.\n",
      "  Batch 1,360  of  7,000.  Loss 0.0190  Elapsed: 0:00:13.104198.\n",
      "  Batch 1,400  of  7,000.  Loss 0.0035  Elapsed: 0:00:13.489191.\n",
      "  Batch 1,440  of  7,000.  Loss 0.0427  Elapsed: 0:00:13.872520.\n",
      "  Batch 1,480  of  7,000.  Loss 0.0070  Elapsed: 0:00:14.258149.\n",
      "  Batch 1,520  of  7,000.  Loss 0.0034  Elapsed: 0:00:14.643103.\n",
      "  Batch 1,560  of  7,000.  Loss 0.0035  Elapsed: 0:00:15.028453.\n",
      "  Batch 1,600  of  7,000.  Loss 0.0224  Elapsed: 0:00:15.413456.\n",
      "  Batch 1,640  of  7,000.  Loss 0.0054  Elapsed: 0:00:15.798324.\n",
      "  Batch 1,680  of  7,000.  Loss 0.0073  Elapsed: 0:00:16.182912.\n",
      "  Batch 1,720  of  7,000.  Loss 0.1995  Elapsed: 0:00:16.567764.\n",
      "  Batch 1,760  of  7,000.  Loss 0.0075  Elapsed: 0:00:16.954587.\n",
      "  Batch 1,800  of  7,000.  Loss 0.0167  Elapsed: 0:00:17.339686.\n",
      "  Batch 1,840  of  7,000.  Loss 0.0050  Elapsed: 0:00:17.725684.\n",
      "  Batch 1,880  of  7,000.  Loss 0.0232  Elapsed: 0:00:18.111740.\n",
      "  Batch 1,920  of  7,000.  Loss 0.0951  Elapsed: 0:00:18.496758.\n",
      "  Batch 1,960  of  7,000.  Loss 0.0138  Elapsed: 0:00:18.881999.\n",
      "  Batch 2,000  of  7,000.  Loss 0.0537  Elapsed: 0:00:19.267427.\n",
      "  Batch 2,040  of  7,000.  Loss 0.0372  Elapsed: 0:00:19.653260.\n",
      "  Batch 2,080  of  7,000.  Loss 0.0040  Elapsed: 0:00:20.037792.\n",
      "  Batch 2,120  of  7,000.  Loss 0.0033  Elapsed: 0:00:20.423126.\n",
      "  Batch 2,160  of  7,000.  Loss 0.0057  Elapsed: 0:00:20.809180.\n",
      "  Batch 2,200  of  7,000.  Loss 0.0032  Elapsed: 0:00:21.194332.\n",
      "  Batch 2,240  of  7,000.  Loss 0.0038  Elapsed: 0:00:21.580465.\n",
      "  Batch 2,280  of  7,000.  Loss 0.0252  Elapsed: 0:00:21.965328.\n",
      "  Batch 2,320  of  7,000.  Loss 0.0850  Elapsed: 0:00:22.350671.\n",
      "  Batch 2,360  of  7,000.  Loss 0.0057  Elapsed: 0:00:22.737968.\n",
      "  Batch 2,400  of  7,000.  Loss 0.0054  Elapsed: 0:00:23.121428.\n",
      "  Batch 2,440  of  7,000.  Loss 0.0035  Elapsed: 0:00:23.506370.\n",
      "  Batch 2,480  of  7,000.  Loss 0.0035  Elapsed: 0:00:23.891957.\n",
      "  Batch 2,520  of  7,000.  Loss 0.0055  Elapsed: 0:00:24.275912.\n",
      "  Batch 2,560  of  7,000.  Loss 0.0044  Elapsed: 0:00:24.659582.\n",
      "  Batch 2,600  of  7,000.  Loss 0.0041  Elapsed: 0:00:25.045258.\n",
      "  Batch 2,640  of  7,000.  Loss 0.0379  Elapsed: 0:00:25.429891.\n",
      "  Batch 2,680  of  7,000.  Loss 0.0051  Elapsed: 0:00:25.814399.\n",
      "  Batch 2,720  of  7,000.  Loss 0.0037  Elapsed: 0:00:26.198838.\n",
      "  Batch 2,760  of  7,000.  Loss 0.0303  Elapsed: 0:00:26.582734.\n",
      "  Batch 2,800  of  7,000.  Loss 0.0037  Elapsed: 0:00:26.966794.\n",
      "  Batch 2,840  of  7,000.  Loss 0.0037  Elapsed: 0:00:27.350009.\n",
      "  Batch 2,880  of  7,000.  Loss 0.1217  Elapsed: 0:00:27.734394.\n",
      "  Batch 2,920  of  7,000.  Loss 0.0194  Elapsed: 0:00:28.120986.\n",
      "  Batch 2,960  of  7,000.  Loss 0.0477  Elapsed: 0:00:28.506269.\n",
      "  Batch 3,000  of  7,000.  Loss 0.0055  Elapsed: 0:00:28.894509.\n",
      "  Batch 3,040  of  7,000.  Loss 0.0108  Elapsed: 0:00:29.282750.\n",
      "  Batch 3,080  of  7,000.  Loss 0.0071  Elapsed: 0:00:29.668708.\n",
      "  Batch 3,120  of  7,000.  Loss 0.0473  Elapsed: 0:00:30.053163.\n",
      "  Batch 3,160  of  7,000.  Loss 0.0087  Elapsed: 0:00:30.439550.\n",
      "  Batch 3,200  of  7,000.  Loss 0.0051  Elapsed: 0:00:30.823334.\n",
      "  Batch 3,240  of  7,000.  Loss 0.1912  Elapsed: 0:00:31.207160.\n",
      "  Batch 3,280  of  7,000.  Loss 0.0035  Elapsed: 0:00:31.591044.\n",
      "  Batch 3,320  of  7,000.  Loss 0.0349  Elapsed: 0:00:31.974413.\n",
      "  Batch 3,360  of  7,000.  Loss 0.0040  Elapsed: 0:00:32.360246.\n",
      "  Batch 3,400  of  7,000.  Loss 0.0036  Elapsed: 0:00:32.743968.\n",
      "  Batch 3,440  of  7,000.  Loss 0.0035  Elapsed: 0:00:33.131331.\n",
      "  Batch 3,480  of  7,000.  Loss 0.0049  Elapsed: 0:00:33.516512.\n",
      "  Batch 3,520  of  7,000.  Loss 0.0040  Elapsed: 0:00:33.901604.\n",
      "  Batch 3,560  of  7,000.  Loss 0.0345  Elapsed: 0:00:34.286767.\n",
      "  Batch 3,600  of  7,000.  Loss 0.0144  Elapsed: 0:00:34.671779.\n",
      "  Batch 3,640  of  7,000.  Loss 0.0039  Elapsed: 0:00:35.059368.\n",
      "  Batch 3,680  of  7,000.  Loss 0.0048  Elapsed: 0:00:35.445294.\n",
      "  Batch 3,720  of  7,000.  Loss 0.2307  Elapsed: 0:00:35.829060.\n",
      "  Batch 3,760  of  7,000.  Loss 0.0348  Elapsed: 0:00:36.214499.\n",
      "  Batch 3,800  of  7,000.  Loss 0.0149  Elapsed: 0:00:36.600921.\n",
      "  Batch 3,840  of  7,000.  Loss 0.0040  Elapsed: 0:00:36.986006.\n",
      "  Batch 3,880  of  7,000.  Loss 0.0038  Elapsed: 0:00:37.369874.\n",
      "  Batch 3,920  of  7,000.  Loss 0.0037  Elapsed: 0:00:37.754039.\n",
      "  Batch 3,960  of  7,000.  Loss 0.0045  Elapsed: 0:00:38.141007.\n",
      "  Batch 4,000  of  7,000.  Loss 0.0036  Elapsed: 0:00:38.525625.\n",
      "  Batch 4,040  of  7,000.  Loss 0.6693  Elapsed: 0:00:38.909544.\n",
      "  Batch 4,080  of  7,000.  Loss 0.0307  Elapsed: 0:00:39.293899.\n",
      "  Batch 4,120  of  7,000.  Loss 0.0265  Elapsed: 0:00:39.679581.\n",
      "  Batch 4,160  of  7,000.  Loss 0.0920  Elapsed: 0:00:40.064263.\n",
      "  Batch 4,200  of  7,000.  Loss 0.0033  Elapsed: 0:00:40.450160.\n",
      "  Batch 4,240  of  7,000.  Loss 0.0731  Elapsed: 0:00:40.832789.\n",
      "  Batch 4,280  of  7,000.  Loss 0.0144  Elapsed: 0:00:41.216230.\n",
      "  Batch 4,320  of  7,000.  Loss 0.0035  Elapsed: 0:00:41.601341.\n",
      "  Batch 4,360  of  7,000.  Loss 0.0153  Elapsed: 0:00:41.985517.\n",
      "  Batch 4,400  of  7,000.  Loss 0.0366  Elapsed: 0:00:42.369326.\n",
      "  Batch 4,440  of  7,000.  Loss 0.0173  Elapsed: 0:00:42.754177.\n",
      "  Batch 4,480  of  7,000.  Loss 0.0037  Elapsed: 0:00:43.141600.\n",
      "  Batch 4,520  of  7,000.  Loss 0.0388  Elapsed: 0:00:43.526149.\n",
      "  Batch 4,560  of  7,000.  Loss 0.0036  Elapsed: 0:00:43.912236.\n",
      "  Batch 4,600  of  7,000.  Loss 0.0068  Elapsed: 0:00:44.297111.\n",
      "  Batch 4,640  of  7,000.  Loss 0.4606  Elapsed: 0:00:44.679742.\n",
      "  Batch 4,680  of  7,000.  Loss 0.0315  Elapsed: 0:00:45.064860.\n",
      "  Batch 4,720  of  7,000.  Loss 0.0054  Elapsed: 0:00:45.448560.\n",
      "  Batch 4,760  of  7,000.  Loss 0.0107  Elapsed: 0:00:45.834175.\n",
      "  Batch 4,800  of  7,000.  Loss 0.0052  Elapsed: 0:00:46.221120.\n",
      "  Batch 4,840  of  7,000.  Loss 0.0330  Elapsed: 0:00:46.606475.\n",
      "  Batch 4,880  of  7,000.  Loss 0.1367  Elapsed: 0:00:46.990447.\n",
      "  Batch 4,920  of  7,000.  Loss 0.0058  Elapsed: 0:00:47.375966.\n",
      "  Batch 4,960  of  7,000.  Loss 0.0033  Elapsed: 0:00:47.759862.\n",
      "  Batch 5,000  of  7,000.  Loss 0.3441  Elapsed: 0:00:48.143561.\n",
      "  Batch 5,040  of  7,000.  Loss 0.0200  Elapsed: 0:00:48.528149.\n",
      "  Batch 5,080  of  7,000.  Loss 0.0035  Elapsed: 0:00:48.911600.\n",
      "  Batch 5,120  of  7,000.  Loss 0.0037  Elapsed: 0:00:49.294529.\n",
      "  Batch 5,160  of  7,000.  Loss 0.0035  Elapsed: 0:00:49.677854.\n",
      "  Batch 5,200  of  7,000.  Loss 0.0088  Elapsed: 0:00:50.061419.\n",
      "  Batch 5,240  of  7,000.  Loss 0.0209  Elapsed: 0:00:50.446412.\n",
      "  Batch 5,280  of  7,000.  Loss 0.0159  Elapsed: 0:00:50.830377.\n",
      "  Batch 5,320  of  7,000.  Loss 0.0048  Elapsed: 0:00:51.216194.\n",
      "  Batch 5,360  of  7,000.  Loss 0.1188  Elapsed: 0:00:51.600920.\n",
      "  Batch 5,400  of  7,000.  Loss 0.0161  Elapsed: 0:00:51.991367.\n",
      "  Batch 5,440  of  7,000.  Loss 0.0322  Elapsed: 0:00:52.375859.\n",
      "  Batch 5,480  of  7,000.  Loss 0.0222  Elapsed: 0:00:52.762932.\n",
      "  Batch 5,520  of  7,000.  Loss 0.0238  Elapsed: 0:00:53.146744.\n",
      "  Batch 5,560  of  7,000.  Loss 0.0033  Elapsed: 0:00:53.532333.\n",
      "  Batch 5,600  of  7,000.  Loss 0.0052  Elapsed: 0:00:53.917998.\n",
      "  Batch 5,640  of  7,000.  Loss 0.0033  Elapsed: 0:00:54.304540.\n",
      "  Batch 5,680  of  7,000.  Loss 0.0040  Elapsed: 0:00:54.689393.\n",
      "  Batch 5,720  of  7,000.  Loss 0.0106  Elapsed: 0:00:55.074352.\n",
      "  Batch 5,760  of  7,000.  Loss 0.0309  Elapsed: 0:00:55.459308.\n",
      "  Batch 5,800  of  7,000.  Loss 0.0081  Elapsed: 0:00:55.843575.\n",
      "  Batch 5,840  of  7,000.  Loss 0.0085  Elapsed: 0:00:56.228273.\n",
      "  Batch 5,880  of  7,000.  Loss 0.2595  Elapsed: 0:00:56.612718.\n",
      "  Batch 5,920  of  7,000.  Loss 0.0042  Elapsed: 0:00:56.998465.\n",
      "  Batch 5,960  of  7,000.  Loss 0.0143  Elapsed: 0:00:57.382032.\n",
      "  Batch 6,000  of  7,000.  Loss 0.0034  Elapsed: 0:00:57.768287.\n",
      "  Batch 6,040  of  7,000.  Loss 0.0043  Elapsed: 0:00:58.155208.\n",
      "  Batch 6,080  of  7,000.  Loss 0.0334  Elapsed: 0:00:58.542259.\n",
      "  Batch 6,120  of  7,000.  Loss 0.0229  Elapsed: 0:00:58.928625.\n",
      "  Batch 6,160  of  7,000.  Loss 0.0040  Elapsed: 0:00:59.313037.\n",
      "  Batch 6,200  of  7,000.  Loss 0.0037  Elapsed: 0:00:59.698224.\n",
      "  Batch 6,240  of  7,000.  Loss 0.0143  Elapsed: 0:01:00.081661.\n",
      "  Batch 6,280  of  7,000.  Loss 0.0048  Elapsed: 0:01:00.467670.\n",
      "  Batch 6,320  of  7,000.  Loss 0.0243  Elapsed: 0:01:00.852278.\n",
      "  Batch 6,360  of  7,000.  Loss 0.0133  Elapsed: 0:01:01.236891.\n",
      "  Batch 6,400  of  7,000.  Loss 0.0284  Elapsed: 0:01:01.622139.\n",
      "  Batch 6,440  of  7,000.  Loss 0.0142  Elapsed: 0:01:02.005401.\n",
      "  Batch 6,480  of  7,000.  Loss 0.2196  Elapsed: 0:01:02.388411.\n",
      "  Batch 6,520  of  7,000.  Loss 0.0054  Elapsed: 0:01:02.774793.\n",
      "  Batch 6,560  of  7,000.  Loss 0.0135  Elapsed: 0:01:03.157336.\n",
      "  Batch 6,600  of  7,000.  Loss 0.0042  Elapsed: 0:01:03.539197.\n",
      "  Batch 6,640  of  7,000.  Loss 0.0037  Elapsed: 0:01:03.925496.\n",
      "  Batch 6,680  of  7,000.  Loss 0.0033  Elapsed: 0:01:04.311031.\n",
      "  Batch 6,720  of  7,000.  Loss 0.0184  Elapsed: 0:01:04.698217.\n",
      "  Batch 6,760  of  7,000.  Loss 0.0471  Elapsed: 0:01:05.083628.\n",
      "  Batch 6,800  of  7,000.  Loss 0.0037  Elapsed: 0:01:05.470154.\n",
      "  Batch 6,840  of  7,000.  Loss 0.0044  Elapsed: 0:01:05.856458.\n",
      "  Batch 6,880  of  7,000.  Loss 0.0134  Elapsed: 0:01:06.242090.\n",
      "  Batch 6,920  of  7,000.  Loss 0.0034  Elapsed: 0:01:06.629242.\n",
      "  Batch 6,960  of  7,000.  Loss 0.0269  Elapsed: 0:01:07.015940.\n",
      "Avg Validation Loss 0.0396, Completed in 0:01:07.393191 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.960803\n",
      "F1 Score (micro) =   0.962135\n",
      "F1 Score (macro) =   0.901562\n",
      "F1 Score (samples) =   0.952381\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0235  Elapsed: 0:00:00.396609.\n",
      "  Batch    80  of  1,477.  Loss 0.3297  Elapsed: 0:00:00.783623.\n",
      "  Batch   120  of  1,477.  Loss 0.0086  Elapsed: 0:00:01.166913.\n",
      "  Batch   160  of  1,477.  Loss 0.0411  Elapsed: 0:00:01.549790.\n",
      "  Batch   200  of  1,477.  Loss 0.0138  Elapsed: 0:00:01.932766.\n",
      "  Batch   240  of  1,477.  Loss 0.0246  Elapsed: 0:00:02.318238.\n",
      "  Batch   280  of  1,477.  Loss 0.6570  Elapsed: 0:00:02.704501.\n",
      "  Batch   320  of  1,477.  Loss 0.2993  Elapsed: 0:00:03.088835.\n",
      "  Batch   360  of  1,477.  Loss 0.1302  Elapsed: 0:00:03.473363.\n",
      "  Batch   400  of  1,477.  Loss 0.3635  Elapsed: 0:00:03.856074.\n",
      "  Batch   440  of  1,477.  Loss 0.8940  Elapsed: 0:00:04.240432.\n",
      "  Batch   480  of  1,477.  Loss 0.0081  Elapsed: 0:00:04.625944.\n",
      "  Batch   520  of  1,477.  Loss 0.0090  Elapsed: 0:00:05.012582.\n",
      "  Batch   560  of  1,477.  Loss 0.0350  Elapsed: 0:00:05.398439.\n",
      "  Batch   600  of  1,477.  Loss 0.5558  Elapsed: 0:00:05.783584.\n",
      "  Batch   640  of  1,477.  Loss 0.0038  Elapsed: 0:00:06.169576.\n",
      "  Batch   680  of  1,477.  Loss 0.0034  Elapsed: 0:00:06.554663.\n",
      "  Batch   720  of  1,477.  Loss 0.0042  Elapsed: 0:00:06.940279.\n",
      "  Batch   760  of  1,477.  Loss 0.5490  Elapsed: 0:00:07.324252.\n",
      "  Batch   800  of  1,477.  Loss 0.0052  Elapsed: 0:00:07.708344.\n",
      "  Batch   840  of  1,477.  Loss 0.0245  Elapsed: 0:00:08.093339.\n",
      "  Batch   880  of  1,477.  Loss 0.0401  Elapsed: 0:00:08.479111.\n",
      "  Batch   920  of  1,477.  Loss 0.0888  Elapsed: 0:00:08.863190.\n",
      "  Batch   960  of  1,477.  Loss 0.0151  Elapsed: 0:00:09.246617.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0124  Elapsed: 0:00:09.631007.\n",
      "  Batch 1,040  of  1,477.  Loss 0.9471  Elapsed: 0:00:10.015964.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0137  Elapsed: 0:00:10.401196.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0098  Elapsed: 0:00:10.785061.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0043  Elapsed: 0:00:11.171804.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0046  Elapsed: 0:00:11.556801.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0302  Elapsed: 0:00:11.941707.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0055  Elapsed: 0:00:12.327266.\n",
      "  Batch 1,320  of  1,477.  Loss 1.3388  Elapsed: 0:00:12.711775.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0038  Elapsed: 0:00:13.096043.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0336  Elapsed: 0:00:13.480670.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0380  Elapsed: 0:00:13.865304.\n",
      "Avg Validation Loss 0.2022, Completed in 0:00:14.210901 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.771563\n",
      "F1 Score (micro) =   0.779094\n",
      "F1 Score (macro) =   0.623107\n",
      "F1 Score (samples) =   0.756263\n",
      "Running Experiment TALES_8000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    500.  Loss 0.2746  Elapsed: 0:00:26.869776.\n",
      "Epoch  0  Batch   200  of    500.  Loss 0.2662  Elapsed: 0:00:53.627961.\n",
      "Epoch  0  Batch   300  of    500.  Loss 0.2429  Elapsed: 0:01:20.418095.\n",
      "Epoch  0  Batch   400  of    500.  Loss 0.3231  Elapsed: 0:01:47.177938.\n",
      "Avg Training Loss 0.2513, Completed in 0:02:13.843470 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3032  Elapsed: 0:00:03.699199.\n",
      "  Batch    80  of     93.  Loss 0.2915  Elapsed: 0:00:07.297567.\n",
      "Avg Validation Loss 0.2469, Completed in 0:00:08.316114 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.574328\n",
      "F1 Score (micro) =   0.640123\n",
      "F1 Score (macro) =   0.343526\n",
      "F1 Score (samples) =   0.566012\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    500.  Loss 0.2267  Elapsed: 0:00:26.876904.\n",
      "Epoch  1  Batch   200  of    500.  Loss 0.1374  Elapsed: 0:00:53.639371.\n",
      "Epoch  1  Batch   300  of    500.  Loss 0.1549  Elapsed: 0:01:20.405917.\n",
      "Epoch  1  Batch   400  of    500.  Loss 0.0921  Elapsed: 0:01:47.168342.\n",
      "Avg Training Loss 0.1891, Completed in 0:02:13.828362 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1843  Elapsed: 0:00:03.691297.\n",
      "  Batch    80  of     93.  Loss 0.1835  Elapsed: 0:00:07.279208.\n",
      "Avg Validation Loss 0.2095, Completed in 0:00:08.300647 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.673014\n",
      "F1 Score (micro) =   0.691794\n",
      "F1 Score (macro) =   0.486806\n",
      "F1 Score (samples) =   0.596479\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    500.  Loss 0.1106  Elapsed: 0:00:26.852810.\n",
      "Epoch  2  Batch   200  of    500.  Loss 0.0607  Elapsed: 0:00:53.624684.\n",
      "Epoch  2  Batch   300  of    500.  Loss 0.1548  Elapsed: 0:01:20.401425.\n",
      "Epoch  2  Batch   400  of    500.  Loss 0.0496  Elapsed: 0:01:47.167060.\n",
      "Avg Training Loss 0.1281, Completed in 0:02:13.815657 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3877  Elapsed: 0:00:03.684560.\n",
      "  Batch    80  of     93.  Loss 0.2410  Elapsed: 0:00:07.290786.\n",
      "Avg Validation Loss 0.1889, Completed in 0:00:08.311506 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.749895\n",
      "F1 Score (micro) =   0.764539\n",
      "F1 Score (macro) =   0.613951\n",
      "F1 Score (samples) =   0.728955\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    500.  Loss 0.0348  Elapsed: 0:00:26.876544.\n",
      "Epoch  3  Batch   200  of    500.  Loss 0.0233  Elapsed: 0:00:53.639364.\n",
      "Epoch  3  Batch   300  of    500.  Loss 0.0914  Elapsed: 0:01:20.416675.\n",
      "Epoch  3  Batch   400  of    500.  Loss 0.0767  Elapsed: 0:01:47.224139.\n",
      "Avg Training Loss 0.0760, Completed in 0:02:13.915557 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.0342  Elapsed: 0:00:03.684211.\n",
      "  Batch    80  of     93.  Loss 0.0296  Elapsed: 0:00:07.284916.\n",
      "Avg Validation Loss 0.1964, Completed in 0:00:08.304394 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.781542\n",
      "F1 Score (micro) =   0.786313\n",
      "F1 Score (macro) =   0.691849\n",
      "F1 Score (samples) =   0.760325\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  8,000.  Loss 0.0156  Elapsed: 0:00:00.399804.\n",
      "  Batch    80  of  8,000.  Loss 0.0037  Elapsed: 0:00:00.787455.\n",
      "  Batch   120  of  8,000.  Loss 0.0070  Elapsed: 0:00:01.176309.\n",
      "  Batch   160  of  8,000.  Loss 0.0663  Elapsed: 0:00:01.562204.\n",
      "  Batch   200  of  8,000.  Loss 0.0070  Elapsed: 0:00:01.948382.\n",
      "  Batch   240  of  8,000.  Loss 0.0108  Elapsed: 0:00:02.334460.\n",
      "  Batch   280  of  8,000.  Loss 0.0029  Elapsed: 0:00:02.721855.\n",
      "  Batch   320  of  8,000.  Loss 0.2503  Elapsed: 0:00:03.110809.\n",
      "  Batch   360  of  8,000.  Loss 0.0044  Elapsed: 0:00:03.497288.\n",
      "  Batch   400  of  8,000.  Loss 0.0040  Elapsed: 0:00:03.885668.\n",
      "  Batch   440  of  8,000.  Loss 0.0496  Elapsed: 0:00:04.274441.\n",
      "  Batch   480  of  8,000.  Loss 0.0034  Elapsed: 0:00:04.661612.\n",
      "  Batch   520  of  8,000.  Loss 0.0245  Elapsed: 0:00:05.048989.\n",
      "  Batch   560  of  8,000.  Loss 0.0050  Elapsed: 0:00:05.434625.\n",
      "  Batch   600  of  8,000.  Loss 0.0027  Elapsed: 0:00:05.822406.\n",
      "  Batch   640  of  8,000.  Loss 0.0111  Elapsed: 0:00:06.208169.\n",
      "  Batch   680  of  8,000.  Loss 0.0029  Elapsed: 0:00:06.596644.\n",
      "  Batch   720  of  8,000.  Loss 0.0032  Elapsed: 0:00:06.984049.\n",
      "  Batch   760  of  8,000.  Loss 0.0043  Elapsed: 0:00:07.368924.\n",
      "  Batch   800  of  8,000.  Loss 0.0051  Elapsed: 0:00:07.755086.\n",
      "  Batch   840  of  8,000.  Loss 0.0043  Elapsed: 0:00:08.142104.\n",
      "  Batch   880  of  8,000.  Loss 0.0034  Elapsed: 0:00:08.528437.\n",
      "  Batch   920  of  8,000.  Loss 0.0036  Elapsed: 0:00:08.913765.\n",
      "  Batch   960  of  8,000.  Loss 0.1356  Elapsed: 0:00:09.300178.\n",
      "  Batch 1,000  of  8,000.  Loss 0.0311  Elapsed: 0:00:09.686427.\n",
      "  Batch 1,040  of  8,000.  Loss 0.0570  Elapsed: 0:00:10.073671.\n",
      "  Batch 1,080  of  8,000.  Loss 0.0046  Elapsed: 0:00:10.459758.\n",
      "  Batch 1,120  of  8,000.  Loss 0.0049  Elapsed: 0:00:10.844795.\n",
      "  Batch 1,160  of  8,000.  Loss 0.0029  Elapsed: 0:00:11.233703.\n",
      "  Batch 1,200  of  8,000.  Loss 0.1023  Elapsed: 0:00:11.619745.\n",
      "  Batch 1,240  of  8,000.  Loss 0.0030  Elapsed: 0:00:12.008798.\n",
      "  Batch 1,280  of  8,000.  Loss 0.0030  Elapsed: 0:00:12.397576.\n",
      "  Batch 1,320  of  8,000.  Loss 0.0129  Elapsed: 0:00:12.784081.\n",
      "  Batch 1,360  of  8,000.  Loss 0.0028  Elapsed: 0:00:13.171951.\n",
      "  Batch 1,400  of  8,000.  Loss 0.0028  Elapsed: 0:00:13.562471.\n",
      "  Batch 1,440  of  8,000.  Loss 0.0089  Elapsed: 0:00:13.948892.\n",
      "  Batch 1,480  of  8,000.  Loss 0.0722  Elapsed: 0:00:14.337086.\n",
      "  Batch 1,520  of  8,000.  Loss 0.0109  Elapsed: 0:00:14.723905.\n",
      "  Batch 1,560  of  8,000.  Loss 0.0531  Elapsed: 0:00:15.112426.\n",
      "  Batch 1,600  of  8,000.  Loss 0.0063  Elapsed: 0:00:15.499585.\n",
      "  Batch 1,640  of  8,000.  Loss 0.0046  Elapsed: 0:00:15.889267.\n",
      "  Batch 1,680  of  8,000.  Loss 0.0035  Elapsed: 0:00:16.275750.\n",
      "  Batch 1,720  of  8,000.  Loss 0.0033  Elapsed: 0:00:16.664012.\n",
      "  Batch 1,760  of  8,000.  Loss 0.0236  Elapsed: 0:00:17.050864.\n",
      "  Batch 1,800  of  8,000.  Loss 0.0283  Elapsed: 0:00:17.438170.\n",
      "  Batch 1,840  of  8,000.  Loss 0.0078  Elapsed: 0:00:17.827211.\n",
      "  Batch 1,880  of  8,000.  Loss 0.0028  Elapsed: 0:00:18.214389.\n",
      "  Batch 1,920  of  8,000.  Loss 0.0040  Elapsed: 0:00:18.602197.\n",
      "  Batch 1,960  of  8,000.  Loss 0.0053  Elapsed: 0:00:18.990566.\n",
      "  Batch 2,000  of  8,000.  Loss 0.0044  Elapsed: 0:00:19.377941.\n",
      "  Batch 2,040  of  8,000.  Loss 0.5274  Elapsed: 0:00:19.769241.\n",
      "  Batch 2,080  of  8,000.  Loss 0.0138  Elapsed: 0:00:20.158683.\n",
      "  Batch 2,120  of  8,000.  Loss 0.0032  Elapsed: 0:00:20.548776.\n",
      "  Batch 2,160  of  8,000.  Loss 0.0030  Elapsed: 0:00:20.935799.\n",
      "  Batch 2,200  of  8,000.  Loss 0.0040  Elapsed: 0:00:21.323639.\n",
      "  Batch 2,240  of  8,000.  Loss 0.0035  Elapsed: 0:00:21.709547.\n",
      "  Batch 2,280  of  8,000.  Loss 0.0037  Elapsed: 0:00:22.097518.\n",
      "  Batch 2,320  of  8,000.  Loss 0.0091  Elapsed: 0:00:22.486159.\n",
      "  Batch 2,360  of  8,000.  Loss 0.0032  Elapsed: 0:00:22.873057.\n",
      "  Batch 2,400  of  8,000.  Loss 0.0063  Elapsed: 0:00:23.260299.\n",
      "  Batch 2,440  of  8,000.  Loss 0.0096  Elapsed: 0:00:23.644852.\n",
      "  Batch 2,480  of  8,000.  Loss 0.1874  Elapsed: 0:00:24.030538.\n",
      "  Batch 2,520  of  8,000.  Loss 0.0030  Elapsed: 0:00:24.416330.\n",
      "  Batch 2,560  of  8,000.  Loss 0.0051  Elapsed: 0:00:24.803667.\n",
      "  Batch 2,600  of  8,000.  Loss 0.0031  Elapsed: 0:00:25.189524.\n",
      "  Batch 2,640  of  8,000.  Loss 0.0146  Elapsed: 0:00:25.578049.\n",
      "  Batch 2,680  of  8,000.  Loss 0.0093  Elapsed: 0:00:25.965113.\n",
      "  Batch 2,720  of  8,000.  Loss 0.0090  Elapsed: 0:00:26.352212.\n",
      "  Batch 2,760  of  8,000.  Loss 0.0093  Elapsed: 0:00:26.740582.\n",
      "  Batch 2,800  of  8,000.  Loss 0.0212  Elapsed: 0:00:27.126667.\n",
      "  Batch 2,840  of  8,000.  Loss 0.0032  Elapsed: 0:00:27.514046.\n",
      "  Batch 2,880  of  8,000.  Loss 0.0031  Elapsed: 0:00:27.899800.\n",
      "  Batch 2,920  of  8,000.  Loss 0.0033  Elapsed: 0:00:28.288137.\n",
      "  Batch 2,960  of  8,000.  Loss 0.0049  Elapsed: 0:00:28.673985.\n",
      "  Batch 3,000  of  8,000.  Loss 0.0032  Elapsed: 0:00:29.062149.\n",
      "  Batch 3,040  of  8,000.  Loss 0.0575  Elapsed: 0:00:29.450127.\n",
      "  Batch 3,080  of  8,000.  Loss 0.0030  Elapsed: 0:00:29.839399.\n",
      "  Batch 3,120  of  8,000.  Loss 0.0029  Elapsed: 0:00:30.227204.\n",
      "  Batch 3,160  of  8,000.  Loss 0.0029  Elapsed: 0:00:30.614215.\n",
      "  Batch 3,200  of  8,000.  Loss 0.0029  Elapsed: 0:00:31.001695.\n",
      "  Batch 3,240  of  8,000.  Loss 0.0029  Elapsed: 0:00:31.389759.\n",
      "  Batch 3,280  of  8,000.  Loss 0.0029  Elapsed: 0:00:31.775000.\n",
      "  Batch 3,320  of  8,000.  Loss 0.0035  Elapsed: 0:00:32.163253.\n",
      "  Batch 3,360  of  8,000.  Loss 0.0038  Elapsed: 0:00:32.551188.\n",
      "  Batch 3,400  of  8,000.  Loss 0.0725  Elapsed: 0:00:32.938379.\n",
      "  Batch 3,440  of  8,000.  Loss 0.0035  Elapsed: 0:00:33.324824.\n",
      "  Batch 3,480  of  8,000.  Loss 0.0237  Elapsed: 0:00:33.711511.\n",
      "  Batch 3,520  of  8,000.  Loss 0.0032  Elapsed: 0:00:34.098754.\n",
      "  Batch 3,560  of  8,000.  Loss 0.0037  Elapsed: 0:00:34.484703.\n",
      "  Batch 3,600  of  8,000.  Loss 0.0805  Elapsed: 0:00:34.875312.\n",
      "  Batch 3,640  of  8,000.  Loss 0.0642  Elapsed: 0:00:35.265764.\n",
      "  Batch 3,680  of  8,000.  Loss 0.0320  Elapsed: 0:00:35.654112.\n",
      "  Batch 3,720  of  8,000.  Loss 0.0031  Elapsed: 0:00:36.042298.\n",
      "  Batch 3,760  of  8,000.  Loss 0.0041  Elapsed: 0:00:36.429891.\n",
      "  Batch 3,800  of  8,000.  Loss 0.0296  Elapsed: 0:00:36.816798.\n",
      "  Batch 3,840  of  8,000.  Loss 0.0281  Elapsed: 0:00:37.203825.\n",
      "  Batch 3,880  of  8,000.  Loss 0.0903  Elapsed: 0:00:37.591180.\n",
      "  Batch 3,920  of  8,000.  Loss 0.0033  Elapsed: 0:00:37.979363.\n",
      "  Batch 3,960  of  8,000.  Loss 0.0573  Elapsed: 0:00:38.364313.\n",
      "  Batch 4,000  of  8,000.  Loss 0.0072  Elapsed: 0:00:38.751056.\n",
      "  Batch 4,040  of  8,000.  Loss 0.0218  Elapsed: 0:00:39.139486.\n",
      "  Batch 4,080  of  8,000.  Loss 0.0257  Elapsed: 0:00:39.527342.\n",
      "  Batch 4,120  of  8,000.  Loss 0.0435  Elapsed: 0:00:39.915911.\n",
      "  Batch 4,160  of  8,000.  Loss 0.0167  Elapsed: 0:00:40.303113.\n",
      "  Batch 4,200  of  8,000.  Loss 0.0058  Elapsed: 0:00:40.689165.\n",
      "  Batch 4,240  of  8,000.  Loss 0.0089  Elapsed: 0:00:41.075416.\n",
      "  Batch 4,280  of  8,000.  Loss 0.0073  Elapsed: 0:00:41.461754.\n",
      "  Batch 4,320  of  8,000.  Loss 0.0997  Elapsed: 0:00:41.850706.\n",
      "  Batch 4,360  of  8,000.  Loss 0.1343  Elapsed: 0:00:42.235304.\n",
      "  Batch 4,400  of  8,000.  Loss 0.0228  Elapsed: 0:00:42.620643.\n",
      "  Batch 4,440  of  8,000.  Loss 0.0032  Elapsed: 0:00:43.009077.\n",
      "  Batch 4,480  of  8,000.  Loss 0.0036  Elapsed: 0:00:43.396905.\n",
      "  Batch 4,520  of  8,000.  Loss 0.0029  Elapsed: 0:00:43.783821.\n",
      "  Batch 4,560  of  8,000.  Loss 0.0043  Elapsed: 0:00:44.170792.\n",
      "  Batch 4,600  of  8,000.  Loss 0.0092  Elapsed: 0:00:44.556641.\n",
      "  Batch 4,640  of  8,000.  Loss 0.0030  Elapsed: 0:00:44.945724.\n",
      "  Batch 4,680  of  8,000.  Loss 0.0046  Elapsed: 0:00:45.331374.\n",
      "  Batch 4,720  of  8,000.  Loss 0.0034  Elapsed: 0:00:45.717549.\n",
      "  Batch 4,760  of  8,000.  Loss 0.0791  Elapsed: 0:00:46.103781.\n",
      "  Batch 4,800  of  8,000.  Loss 0.0035  Elapsed: 0:00:46.491193.\n",
      "  Batch 4,840  of  8,000.  Loss 0.0098  Elapsed: 0:00:46.879378.\n",
      "  Batch 4,880  of  8,000.  Loss 0.0393  Elapsed: 0:00:47.266078.\n",
      "  Batch 4,920  of  8,000.  Loss 0.0028  Elapsed: 0:00:47.653700.\n",
      "  Batch 4,960  of  8,000.  Loss 0.0032  Elapsed: 0:00:48.041784.\n",
      "  Batch 5,000  of  8,000.  Loss 0.0254  Elapsed: 0:00:48.429130.\n",
      "  Batch 5,040  of  8,000.  Loss 0.0295  Elapsed: 0:00:48.815031.\n",
      "  Batch 5,080  of  8,000.  Loss 0.0070  Elapsed: 0:00:49.203115.\n",
      "  Batch 5,120  of  8,000.  Loss 0.0579  Elapsed: 0:00:49.588613.\n",
      "  Batch 5,160  of  8,000.  Loss 0.0244  Elapsed: 0:00:49.974545.\n",
      "  Batch 5,200  of  8,000.  Loss 0.0040  Elapsed: 0:00:50.360520.\n",
      "  Batch 5,240  of  8,000.  Loss 0.0629  Elapsed: 0:00:50.746518.\n",
      "  Batch 5,280  of  8,000.  Loss 0.0100  Elapsed: 0:00:51.134080.\n",
      "  Batch 5,320  of  8,000.  Loss 0.0031  Elapsed: 0:00:51.520383.\n",
      "  Batch 5,360  of  8,000.  Loss 0.0043  Elapsed: 0:00:51.906950.\n",
      "  Batch 5,400  of  8,000.  Loss 0.0032  Elapsed: 0:00:52.297870.\n",
      "  Batch 5,440  of  8,000.  Loss 0.0159  Elapsed: 0:00:52.687317.\n",
      "  Batch 5,480  of  8,000.  Loss 0.0305  Elapsed: 0:00:53.073039.\n",
      "  Batch 5,520  of  8,000.  Loss 0.5147  Elapsed: 0:00:53.458451.\n",
      "  Batch 5,560  of  8,000.  Loss 0.0090  Elapsed: 0:00:53.849789.\n",
      "  Batch 5,600  of  8,000.  Loss 0.0031  Elapsed: 0:00:54.237255.\n",
      "  Batch 5,640  of  8,000.  Loss 0.0187  Elapsed: 0:00:54.624509.\n",
      "  Batch 5,680  of  8,000.  Loss 0.0029  Elapsed: 0:00:55.011456.\n",
      "  Batch 5,720  of  8,000.  Loss 0.0258  Elapsed: 0:00:55.398630.\n",
      "  Batch 5,760  of  8,000.  Loss 0.7921  Elapsed: 0:00:55.783719.\n",
      "  Batch 5,800  of  8,000.  Loss 0.0042  Elapsed: 0:00:56.172682.\n",
      "  Batch 5,840  of  8,000.  Loss 0.0291  Elapsed: 0:00:56.560967.\n",
      "  Batch 5,880  of  8,000.  Loss 0.0236  Elapsed: 0:00:56.948637.\n",
      "  Batch 5,920  of  8,000.  Loss 0.0139  Elapsed: 0:00:57.336475.\n",
      "  Batch 5,960  of  8,000.  Loss 0.0041  Elapsed: 0:00:57.722707.\n",
      "  Batch 6,000  of  8,000.  Loss 1.1358  Elapsed: 0:00:58.109972.\n",
      "  Batch 6,040  of  8,000.  Loss 0.0060  Elapsed: 0:00:58.497189.\n",
      "  Batch 6,080  of  8,000.  Loss 0.0029  Elapsed: 0:00:58.885641.\n",
      "  Batch 6,120  of  8,000.  Loss 0.0029  Elapsed: 0:00:59.273269.\n",
      "  Batch 6,160  of  8,000.  Loss 0.0032  Elapsed: 0:00:59.660092.\n",
      "  Batch 6,200  of  8,000.  Loss 0.0222  Elapsed: 0:01:00.050001.\n",
      "  Batch 6,240  of  8,000.  Loss 0.0036  Elapsed: 0:01:00.437757.\n",
      "  Batch 6,280  of  8,000.  Loss 0.0051  Elapsed: 0:01:00.824049.\n",
      "  Batch 6,320  of  8,000.  Loss 0.0029  Elapsed: 0:01:01.210978.\n",
      "  Batch 6,360  of  8,000.  Loss 0.0042  Elapsed: 0:01:01.596648.\n",
      "  Batch 6,400  of  8,000.  Loss 0.0029  Elapsed: 0:01:01.990071.\n",
      "  Batch 6,440  of  8,000.  Loss 0.0028  Elapsed: 0:01:02.381543.\n",
      "  Batch 6,480  of  8,000.  Loss 0.0030  Elapsed: 0:01:02.771700.\n",
      "  Batch 6,520  of  8,000.  Loss 0.0792  Elapsed: 0:01:03.163030.\n",
      "  Batch 6,560  of  8,000.  Loss 0.0223  Elapsed: 0:01:03.554519.\n",
      "  Batch 6,600  of  8,000.  Loss 0.0256  Elapsed: 0:01:03.942162.\n",
      "  Batch 6,640  of  8,000.  Loss 0.0212  Elapsed: 0:01:04.332071.\n",
      "  Batch 6,680  of  8,000.  Loss 0.0029  Elapsed: 0:01:04.722538.\n",
      "  Batch 6,720  of  8,000.  Loss 0.0042  Elapsed: 0:01:05.112061.\n",
      "  Batch 6,760  of  8,000.  Loss 0.5654  Elapsed: 0:01:05.501586.\n",
      "  Batch 6,800  of  8,000.  Loss 0.0047  Elapsed: 0:01:05.891091.\n",
      "  Batch 6,840  of  8,000.  Loss 0.0058  Elapsed: 0:01:06.281715.\n",
      "  Batch 6,880  of  8,000.  Loss 0.0355  Elapsed: 0:01:06.669850.\n",
      "  Batch 6,920  of  8,000.  Loss 0.0158  Elapsed: 0:01:07.059649.\n",
      "  Batch 6,960  of  8,000.  Loss 0.0045  Elapsed: 0:01:07.448361.\n",
      "  Batch 7,000  of  8,000.  Loss 0.0043  Elapsed: 0:01:07.836639.\n",
      "  Batch 7,040  of  8,000.  Loss 0.1364  Elapsed: 0:01:08.226666.\n",
      "  Batch 7,080  of  8,000.  Loss 0.0087  Elapsed: 0:01:08.616130.\n",
      "  Batch 7,120  of  8,000.  Loss 0.0029  Elapsed: 0:01:09.005620.\n",
      "  Batch 7,160  of  8,000.  Loss 0.0038  Elapsed: 0:01:09.396569.\n",
      "  Batch 7,200  of  8,000.  Loss 0.0030  Elapsed: 0:01:09.787600.\n",
      "  Batch 7,240  of  8,000.  Loss 0.0175  Elapsed: 0:01:10.179060.\n",
      "  Batch 7,280  of  8,000.  Loss 0.0175  Elapsed: 0:01:10.568577.\n",
      "  Batch 7,320  of  8,000.  Loss 0.5314  Elapsed: 0:01:10.959236.\n",
      "  Batch 7,360  of  8,000.  Loss 0.0029  Elapsed: 0:01:11.350240.\n",
      "  Batch 7,400  of  8,000.  Loss 0.0044  Elapsed: 0:01:11.740441.\n",
      "  Batch 7,440  of  8,000.  Loss 0.0046  Elapsed: 0:01:12.128644.\n",
      "  Batch 7,480  of  8,000.  Loss 0.0037  Elapsed: 0:01:12.517459.\n",
      "  Batch 7,520  of  8,000.  Loss 0.0043  Elapsed: 0:01:12.907931.\n",
      "  Batch 7,560  of  8,000.  Loss 0.0091  Elapsed: 0:01:13.297907.\n",
      "  Batch 7,600  of  8,000.  Loss 0.0268  Elapsed: 0:01:13.686808.\n",
      "  Batch 7,640  of  8,000.  Loss 0.0035  Elapsed: 0:01:14.075913.\n",
      "  Batch 7,680  of  8,000.  Loss 0.0310  Elapsed: 0:01:14.469426.\n",
      "  Batch 7,720  of  8,000.  Loss 0.0050  Elapsed: 0:01:14.860115.\n",
      "  Batch 7,760  of  8,000.  Loss 0.0057  Elapsed: 0:01:15.249977.\n",
      "  Batch 7,800  of  8,000.  Loss 0.0124  Elapsed: 0:01:15.639452.\n",
      "  Batch 7,840  of  8,000.  Loss 0.0032  Elapsed: 0:01:16.031719.\n",
      "  Batch 7,880  of  8,000.  Loss 0.0080  Elapsed: 0:01:16.422511.\n",
      "  Batch 7,920  of  8,000.  Loss 0.0048  Elapsed: 0:01:16.812151.\n",
      "  Batch 7,960  of  8,000.  Loss 0.0856  Elapsed: 0:01:17.201430.\n",
      "Avg Validation Loss 0.0359, Completed in 0:01:17.580406 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.966129\n",
      "F1 Score (micro) =   0.966463\n",
      "F1 Score (macro) =   0.925036\n",
      "F1 Score (samples) =   0.958875\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 1.0424  Elapsed: 0:00:00.403050.\n",
      "  Batch    80  of  1,477.  Loss 0.0790  Elapsed: 0:00:00.792437.\n",
      "  Batch   120  of  1,477.  Loss 0.0401  Elapsed: 0:00:01.183785.\n",
      "  Batch   160  of  1,477.  Loss 0.0098  Elapsed: 0:00:01.573278.\n",
      "  Batch   200  of  1,477.  Loss 0.0148  Elapsed: 0:00:01.964246.\n",
      "  Batch   240  of  1,477.  Loss 0.0285  Elapsed: 0:00:02.353630.\n",
      "  Batch   280  of  1,477.  Loss 0.0063  Elapsed: 0:00:02.743923.\n",
      "  Batch   320  of  1,477.  Loss 0.0034  Elapsed: 0:00:03.134859.\n",
      "  Batch   360  of  1,477.  Loss 0.0030  Elapsed: 0:00:03.523715.\n",
      "  Batch   400  of  1,477.  Loss 0.1323  Elapsed: 0:00:03.910809.\n",
      "  Batch   440  of  1,477.  Loss 0.0345  Elapsed: 0:00:04.299508.\n",
      "  Batch   480  of  1,477.  Loss 0.6761  Elapsed: 0:00:04.689125.\n",
      "  Batch   520  of  1,477.  Loss 0.0192  Elapsed: 0:00:05.079530.\n",
      "  Batch   560  of  1,477.  Loss 0.0048  Elapsed: 0:00:05.467751.\n",
      "  Batch   600  of  1,477.  Loss 0.5314  Elapsed: 0:00:05.857826.\n",
      "  Batch   640  of  1,477.  Loss 0.0053  Elapsed: 0:00:06.247529.\n",
      "  Batch   680  of  1,477.  Loss 0.7447  Elapsed: 0:00:06.637827.\n",
      "  Batch   720  of  1,477.  Loss 0.0039  Elapsed: 0:00:07.027411.\n",
      "  Batch   760  of  1,477.  Loss 0.0264  Elapsed: 0:00:07.414858.\n",
      "  Batch   800  of  1,477.  Loss 0.0123  Elapsed: 0:00:07.802469.\n",
      "  Batch   840  of  1,477.  Loss 0.0029  Elapsed: 0:00:08.191604.\n",
      "  Batch   880  of  1,477.  Loss 0.6602  Elapsed: 0:00:08.580978.\n",
      "  Batch   920  of  1,477.  Loss 0.2278  Elapsed: 0:00:08.970198.\n",
      "  Batch   960  of  1,477.  Loss 0.4522  Elapsed: 0:00:09.360577.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0030  Elapsed: 0:00:09.752824.\n",
      "  Batch 1,040  of  1,477.  Loss 0.1034  Elapsed: 0:00:10.139857.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0250  Elapsed: 0:00:10.529308.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0034  Elapsed: 0:00:10.919790.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0031  Elapsed: 0:00:11.308285.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0264  Elapsed: 0:00:11.698010.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0071  Elapsed: 0:00:12.088168.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0170  Elapsed: 0:00:12.478982.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0278  Elapsed: 0:00:12.867350.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0089  Elapsed: 0:00:13.258073.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0573  Elapsed: 0:00:13.648164.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0174  Elapsed: 0:00:14.038262.\n",
      "Avg Validation Loss 0.1919, Completed in 0:00:14.390789 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.785774\n",
      "F1 Score (micro) =   0.791667\n",
      "F1 Score (macro) =   0.664656\n",
      "F1 Score (samples) =   0.770029\n",
      "Running Experiment GEMB_0\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.2694  Elapsed: 0:00:00.407961.\n",
      "  Batch    80  of  1,477.  Loss 0.6200  Elapsed: 0:00:00.800576.\n",
      "  Batch   120  of  1,477.  Loss 0.0493  Elapsed: 0:00:01.192127.\n",
      "  Batch   160  of  1,477.  Loss 0.0172  Elapsed: 0:00:01.583355.\n",
      "  Batch   200  of  1,477.  Loss 1.4303  Elapsed: 0:00:01.973910.\n",
      "  Batch   240  of  1,477.  Loss 0.4646  Elapsed: 0:00:02.364777.\n",
      "  Batch   280  of  1,477.  Loss 0.9004  Elapsed: 0:00:02.753143.\n",
      "  Batch   320  of  1,477.  Loss 1.1810  Elapsed: 0:00:03.142295.\n",
      "  Batch   360  of  1,477.  Loss 0.0303  Elapsed: 0:00:03.533307.\n",
      "  Batch   400  of  1,477.  Loss 0.0257  Elapsed: 0:00:03.923849.\n",
      "  Batch   440  of  1,477.  Loss 1.4668  Elapsed: 0:00:04.314902.\n",
      "  Batch   480  of  1,477.  Loss 0.0577  Elapsed: 0:00:04.704847.\n",
      "  Batch   520  of  1,477.  Loss 1.8540  Elapsed: 0:00:05.096481.\n",
      "  Batch   560  of  1,477.  Loss 0.2057  Elapsed: 0:00:05.488222.\n",
      "  Batch   600  of  1,477.  Loss 0.0832  Elapsed: 0:00:05.879755.\n",
      "  Batch   640  of  1,477.  Loss 0.0335  Elapsed: 0:00:06.269907.\n",
      "  Batch   680  of  1,477.  Loss 0.5178  Elapsed: 0:00:06.660405.\n",
      "  Batch   720  of  1,477.  Loss 1.1861  Elapsed: 0:00:07.048176.\n",
      "  Batch   760  of  1,477.  Loss 0.1503  Elapsed: 0:00:07.438297.\n",
      "  Batch   800  of  1,477.  Loss 1.0490  Elapsed: 0:00:07.827661.\n",
      "  Batch   840  of  1,477.  Loss 0.8251  Elapsed: 0:00:08.219570.\n",
      "  Batch   880  of  1,477.  Loss 0.0375  Elapsed: 0:00:08.611999.\n",
      "  Batch   920  of  1,477.  Loss 0.0985  Elapsed: 0:00:09.003649.\n",
      "  Batch   960  of  1,477.  Loss 0.0091  Elapsed: 0:00:09.394345.\n",
      "  Batch 1,000  of  1,477.  Loss 0.3056  Elapsed: 0:00:09.783947.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0115  Elapsed: 0:00:10.175263.\n",
      "  Batch 1,080  of  1,477.  Loss 0.6252  Elapsed: 0:00:10.565776.\n",
      "  Batch 1,120  of  1,477.  Loss 1.3403  Elapsed: 0:00:10.956273.\n",
      "  Batch 1,160  of  1,477.  Loss 1.0122  Elapsed: 0:00:11.346123.\n",
      "  Batch 1,200  of  1,477.  Loss 0.4652  Elapsed: 0:00:11.736868.\n",
      "  Batch 1,240  of  1,477.  Loss 0.3087  Elapsed: 0:00:12.130728.\n",
      "  Batch 1,280  of  1,477.  Loss 0.5582  Elapsed: 0:00:12.521676.\n",
      "  Batch 1,320  of  1,477.  Loss 1.7194  Elapsed: 0:00:12.910813.\n",
      "  Batch 1,360  of  1,477.  Loss 0.1263  Elapsed: 0:00:13.303516.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0062  Elapsed: 0:00:13.694206.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0084  Elapsed: 0:00:14.082687.\n",
      "Avg Validation Loss 0.4324, Completed in 0:00:14.437303 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.486186\n",
      "F1 Score (micro) =   0.535097\n",
      "F1 Score (macro) =   0.242073\n",
      "F1 Score (samples) =   0.522455\n",
      "Running Experiment GEMB_100\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.3161, Completed in 0:00:01.557258 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.4361, Completed in 0:00:00.580896 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.385252\n",
      "F1 Score (micro) =   0.446809\n",
      "F1 Score (macro) =   0.232095\n",
      "F1 Score (samples) =   0.420000\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1465: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training Loss 0.1536, Completed in 0:00:01.558450 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.4212, Completed in 0:00:00.582281 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.350950\n",
      "F1 Score (micro) =   0.433862\n",
      "F1 Score (macro) =   0.201289\n",
      "F1 Score (samples) =   0.410000\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.1113, Completed in 0:00:01.557919 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.4254, Completed in 0:00:00.580449 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.345960\n",
      "F1 Score (micro) =   0.425532\n",
      "F1 Score (macro) =   0.199550\n",
      "F1 Score (samples) =   0.400000\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.0780, Completed in 0:00:01.556777 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.4544, Completed in 0:00:00.585771 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.375139\n",
      "F1 Score (micro) =   0.442105\n",
      "F1 Score (macro) =   0.221402\n",
      "F1 Score (samples) =   0.420000\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of    100.  Loss 0.0539  Elapsed: 0:00:00.406459.\n",
      "  Batch    80  of    100.  Loss 0.0056  Elapsed: 0:00:00.794245.\n",
      "Avg Validation Loss 0.0354, Completed in 0:00:00.978833 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.964387\n",
      "F1 Score (micro) =   0.969072\n",
      "F1 Score (macro) =   0.903113\n",
      "F1 Score (samples) =   0.940000\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0138  Elapsed: 0:00:00.398860.\n",
      "  Batch    80  of  1,477.  Loss 0.2663  Elapsed: 0:00:00.786154.\n",
      "  Batch   120  of  1,477.  Loss 0.0084  Elapsed: 0:00:01.175791.\n",
      "  Batch   160  of  1,477.  Loss 0.0293  Elapsed: 0:00:01.564853.\n",
      "  Batch   200  of  1,477.  Loss 0.9282  Elapsed: 0:00:01.953176.\n",
      "  Batch   240  of  1,477.  Loss 0.0115  Elapsed: 0:00:02.348459.\n",
      "  Batch   280  of  1,477.  Loss 1.1340  Elapsed: 0:00:02.735887.\n",
      "  Batch   320  of  1,477.  Loss 0.0121  Elapsed: 0:00:03.124148.\n",
      "  Batch   360  of  1,477.  Loss 0.0043  Elapsed: 0:00:03.513692.\n",
      "  Batch   400  of  1,477.  Loss 0.8175  Elapsed: 0:00:03.902806.\n",
      "  Batch   440  of  1,477.  Loss 0.0154  Elapsed: 0:00:04.291175.\n",
      "  Batch   480  of  1,477.  Loss 0.1106  Elapsed: 0:00:04.678163.\n",
      "  Batch   520  of  1,477.  Loss 0.0145  Elapsed: 0:00:05.065572.\n",
      "  Batch   560  of  1,477.  Loss 1.3598  Elapsed: 0:00:05.453185.\n",
      "  Batch   600  of  1,477.  Loss 0.6194  Elapsed: 0:00:05.843518.\n",
      "  Batch   640  of  1,477.  Loss 0.2515  Elapsed: 0:00:06.239142.\n",
      "  Batch   680  of  1,477.  Loss 0.8733  Elapsed: 0:00:06.627278.\n",
      "  Batch   720  of  1,477.  Loss 0.0801  Elapsed: 0:00:07.016240.\n",
      "  Batch   760  of  1,477.  Loss 1.0803  Elapsed: 0:00:07.407927.\n",
      "  Batch   800  of  1,477.  Loss 0.0542  Elapsed: 0:00:07.799248.\n",
      "  Batch   840  of  1,477.  Loss 0.0189  Elapsed: 0:00:08.187037.\n",
      "  Batch   880  of  1,477.  Loss 0.0883  Elapsed: 0:00:08.578165.\n",
      "  Batch   920  of  1,477.  Loss 0.2178  Elapsed: 0:00:08.965502.\n",
      "  Batch   960  of  1,477.  Loss 0.0105  Elapsed: 0:00:09.353598.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0719  Elapsed: 0:00:09.741634.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0290  Elapsed: 0:00:10.130734.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0340  Elapsed: 0:00:10.522232.\n",
      "  Batch 1,120  of  1,477.  Loss 0.7752  Elapsed: 0:00:10.912454.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0311  Elapsed: 0:00:11.302176.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0726  Elapsed: 0:00:11.691082.\n",
      "  Batch 1,240  of  1,477.  Loss 0.6603  Elapsed: 0:00:12.081294.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0243  Elapsed: 0:00:12.473749.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0039  Elapsed: 0:00:12.863563.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0053  Elapsed: 0:00:13.252998.\n",
      "  Batch 1,400  of  1,477.  Loss 0.2296  Elapsed: 0:00:13.642898.\n",
      "  Batch 1,440  of  1,477.  Loss 0.2497  Elapsed: 0:00:14.032940.\n",
      "Avg Validation Loss 0.3621, Completed in 0:00:14.379855 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.493618\n",
      "F1 Score (micro) =   0.561441\n",
      "F1 Score (macro) =   0.267039\n",
      "F1 Score (samples) =   0.526743\n",
      "Running Experiment GEMB_200\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.3595, Completed in 0:00:03.095833 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3351, Completed in 0:00:01.170539 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.388669\n",
      "F1 Score (micro) =   0.449568\n",
      "F1 Score (macro) =   0.216841\n",
      "F1 Score (samples) =   0.390000\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.1851, Completed in 0:00:03.096156 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3307, Completed in 0:00:01.166802 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.396504\n",
      "F1 Score (micro) =   0.470930\n",
      "F1 Score (macro) =   0.238551\n",
      "F1 Score (samples) =   0.405000\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.1321, Completed in 0:00:03.099050 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3589, Completed in 0:00:01.165968 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.424163\n",
      "F1 Score (micro) =   0.489247\n",
      "F1 Score (macro) =   0.263864\n",
      "F1 Score (samples) =   0.455000\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.0653, Completed in 0:00:03.096249 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.4046, Completed in 0:00:01.169331 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.432380\n",
      "F1 Score (micro) =   0.486486\n",
      "F1 Score (macro) =   0.275103\n",
      "F1 Score (samples) =   0.450000\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of    200.  Loss 0.0079  Elapsed: 0:00:00.398909.\n",
      "  Batch    80  of    200.  Loss 0.0058  Elapsed: 0:00:00.785973.\n",
      "  Batch   120  of    200.  Loss 0.0045  Elapsed: 0:00:01.172559.\n",
      "  Batch   160  of    200.  Loss 0.0189  Elapsed: 0:00:01.560432.\n",
      "Avg Validation Loss 0.0323, Completed in 0:00:01.936813 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.973957\n",
      "F1 Score (micro) =   0.977330\n",
      "F1 Score (macro) =   0.910291\n",
      "F1 Score (samples) =   0.970000\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.2453  Elapsed: 0:00:00.399267.\n",
      "  Batch    80  of  1,477.  Loss 0.0045  Elapsed: 0:00:00.787047.\n",
      "  Batch   120  of  1,477.  Loss 0.0131  Elapsed: 0:00:01.175258.\n",
      "  Batch   160  of  1,477.  Loss 0.0067  Elapsed: 0:00:01.562705.\n",
      "  Batch   200  of  1,477.  Loss 0.0302  Elapsed: 0:00:01.948886.\n",
      "  Batch   240  of  1,477.  Loss 0.0425  Elapsed: 0:00:02.334996.\n",
      "  Batch   280  of  1,477.  Loss 0.2776  Elapsed: 0:00:02.723650.\n",
      "  Batch   320  of  1,477.  Loss 0.2437  Elapsed: 0:00:03.111747.\n",
      "  Batch   360  of  1,477.  Loss 0.2515  Elapsed: 0:00:03.499239.\n",
      "  Batch   400  of  1,477.  Loss 0.0106  Elapsed: 0:00:03.884929.\n",
      "  Batch   440  of  1,477.  Loss 1.2812  Elapsed: 0:00:04.273054.\n",
      "  Batch   480  of  1,477.  Loss 0.1707  Elapsed: 0:00:04.661384.\n",
      "  Batch   520  of  1,477.  Loss 0.1077  Elapsed: 0:00:05.050189.\n",
      "  Batch   560  of  1,477.  Loss 0.2608  Elapsed: 0:00:05.440100.\n",
      "  Batch   600  of  1,477.  Loss 1.5989  Elapsed: 0:00:05.828412.\n",
      "  Batch   640  of  1,477.  Loss 1.3756  Elapsed: 0:00:06.214273.\n",
      "  Batch   680  of  1,477.  Loss 0.7271  Elapsed: 0:00:06.602333.\n",
      "  Batch   720  of  1,477.  Loss 0.8994  Elapsed: 0:00:06.991007.\n",
      "  Batch   760  of  1,477.  Loss 0.5599  Elapsed: 0:00:07.375823.\n",
      "  Batch   800  of  1,477.  Loss 0.0566  Elapsed: 0:00:07.763797.\n",
      "  Batch   840  of  1,477.  Loss 0.0080  Elapsed: 0:00:08.150898.\n",
      "  Batch   880  of  1,477.  Loss 0.0225  Elapsed: 0:00:08.539522.\n",
      "  Batch   920  of  1,477.  Loss 0.2644  Elapsed: 0:00:08.925074.\n",
      "  Batch   960  of  1,477.  Loss 0.1066  Elapsed: 0:00:09.312102.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0082  Elapsed: 0:00:09.703375.\n",
      "  Batch 1,040  of  1,477.  Loss 1.5088  Elapsed: 0:00:10.088220.\n",
      "  Batch 1,080  of  1,477.  Loss 0.6140  Elapsed: 0:00:10.472656.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0050  Elapsed: 0:00:10.862416.\n",
      "  Batch 1,160  of  1,477.  Loss 0.1179  Elapsed: 0:00:11.247960.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0170  Elapsed: 0:00:11.636700.\n",
      "  Batch 1,240  of  1,477.  Loss 0.6647  Elapsed: 0:00:12.021201.\n",
      "  Batch 1,280  of  1,477.  Loss 0.1430  Elapsed: 0:00:12.406225.\n",
      "  Batch 1,320  of  1,477.  Loss 0.1248  Elapsed: 0:00:12.791658.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0411  Elapsed: 0:00:13.179527.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0062  Elapsed: 0:00:13.566286.\n",
      "  Batch 1,440  of  1,477.  Loss 0.1062  Elapsed: 0:00:13.951907.\n",
      "Avg Validation Loss 0.3537, Completed in 0:00:14.302776 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.533946\n",
      "F1 Score (micro) =   0.577475\n",
      "F1 Score (macro) =   0.310113\n",
      "F1 Score (samples) =   0.542316\n",
      "Running Experiment GEMB_300\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2985, Completed in 0:00:04.637222 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3147, Completed in 0:00:01.749702 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.456043\n",
      "F1 Score (micro) =   0.509960\n",
      "F1 Score (macro) =   0.256876\n",
      "F1 Score (samples) =   0.426667\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.1647, Completed in 0:00:04.637292 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3183, Completed in 0:00:01.748492 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.475783\n",
      "F1 Score (micro) =   0.556962\n",
      "F1 Score (macro) =   0.259292\n",
      "F1 Score (samples) =   0.513333\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.0962, Completed in 0:00:04.637931 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3615, Completed in 0:00:01.748767 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.489019\n",
      "F1 Score (micro) =   0.540925\n",
      "F1 Score (macro) =   0.286293\n",
      "F1 Score (samples) =   0.505556\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.0427, Completed in 0:00:04.639034 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.4322, Completed in 0:00:01.746184 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.507670\n",
      "F1 Score (micro) =   0.557093\n",
      "F1 Score (macro) =   0.309316\n",
      "F1 Score (samples) =   0.532222\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of    300.  Loss 0.0172  Elapsed: 0:00:00.399662.\n",
      "  Batch    80  of    300.  Loss 0.0411  Elapsed: 0:00:00.787697.\n",
      "  Batch   120  of    300.  Loss 0.0034  Elapsed: 0:00:01.177604.\n",
      "  Batch   160  of    300.  Loss 0.0242  Elapsed: 0:00:01.563783.\n",
      "  Batch   200  of    300.  Loss 0.0035  Elapsed: 0:00:01.950891.\n",
      "  Batch   240  of    300.  Loss 0.0043  Elapsed: 0:00:02.339576.\n",
      "  Batch   280  of    300.  Loss 0.0161  Elapsed: 0:00:02.727581.\n",
      "Avg Validation Loss 0.0139, Completed in 0:00:02.913902 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.994742\n",
      "F1 Score (micro) =   0.994992\n",
      "F1 Score (macro) =   0.983152\n",
      "F1 Score (samples) =   0.993333\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 1.2872  Elapsed: 0:00:00.398577.\n",
      "  Batch    80  of  1,477.  Loss 1.2140  Elapsed: 0:00:00.787890.\n",
      "  Batch   120  of  1,477.  Loss 0.0719  Elapsed: 0:00:01.175917.\n",
      "  Batch   160  of  1,477.  Loss 0.8820  Elapsed: 0:00:01.565617.\n",
      "  Batch   200  of  1,477.  Loss 0.7425  Elapsed: 0:00:01.955597.\n",
      "  Batch   240  of  1,477.  Loss 1.4017  Elapsed: 0:00:02.344663.\n",
      "  Batch   280  of  1,477.  Loss 0.0041  Elapsed: 0:00:02.732338.\n",
      "  Batch   320  of  1,477.  Loss 0.1459  Elapsed: 0:00:03.122090.\n",
      "  Batch   360  of  1,477.  Loss 0.0037  Elapsed: 0:00:03.510525.\n",
      "  Batch   400  of  1,477.  Loss 0.0044  Elapsed: 0:00:03.900147.\n",
      "  Batch   440  of  1,477.  Loss 0.0038  Elapsed: 0:00:04.287876.\n",
      "  Batch   480  of  1,477.  Loss 0.0034  Elapsed: 0:00:04.674057.\n",
      "  Batch   520  of  1,477.  Loss 0.0077  Elapsed: 0:00:05.062821.\n",
      "  Batch   560  of  1,477.  Loss 0.0052  Elapsed: 0:00:05.448881.\n",
      "  Batch   600  of  1,477.  Loss 0.0081  Elapsed: 0:00:05.838320.\n",
      "  Batch   640  of  1,477.  Loss 0.0039  Elapsed: 0:00:06.224663.\n",
      "  Batch   680  of  1,477.  Loss 0.0066  Elapsed: 0:00:06.614655.\n",
      "  Batch   720  of  1,477.  Loss 0.0059  Elapsed: 0:00:07.006331.\n",
      "  Batch   760  of  1,477.  Loss 0.0752  Elapsed: 0:00:07.393454.\n",
      "  Batch   800  of  1,477.  Loss 0.1977  Elapsed: 0:00:07.783039.\n",
      "  Batch   840  of  1,477.  Loss 0.4463  Elapsed: 0:00:08.170816.\n",
      "  Batch   880  of  1,477.  Loss 0.7778  Elapsed: 0:00:08.557486.\n",
      "  Batch   920  of  1,477.  Loss 0.9911  Elapsed: 0:00:08.944574.\n",
      "  Batch   960  of  1,477.  Loss 0.3733  Elapsed: 0:00:09.332253.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0242  Elapsed: 0:00:09.718686.\n",
      "  Batch 1,040  of  1,477.  Loss 1.2832  Elapsed: 0:00:10.108536.\n",
      "  Batch 1,080  of  1,477.  Loss 0.3938  Elapsed: 0:00:10.496848.\n",
      "  Batch 1,120  of  1,477.  Loss 1.4771  Elapsed: 0:00:10.884855.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0056  Elapsed: 0:00:11.270645.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0124  Elapsed: 0:00:11.657224.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0044  Elapsed: 0:00:12.045633.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0034  Elapsed: 0:00:12.432633.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0100  Elapsed: 0:00:12.819847.\n",
      "  Batch 1,360  of  1,477.  Loss 0.2187  Elapsed: 0:00:13.205973.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0221  Elapsed: 0:00:13.593105.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0070  Elapsed: 0:00:13.980571.\n",
      "Avg Validation Loss 0.3960, Completed in 0:00:14.329817 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.541461\n",
      "F1 Score (micro) =   0.582037\n",
      "F1 Score (macro) =   0.324403\n",
      "F1 Score (samples) =   0.553600\n",
      "Running Experiment GEMB_400\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2859, Completed in 0:00:06.177046 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.2961, Completed in 0:00:02.320556 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.480606\n",
      "F1 Score (micro) =   0.549858\n",
      "F1 Score (macro) =   0.253003\n",
      "F1 Score (samples) =   0.482500\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.1669, Completed in 0:00:06.176537 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.2991, Completed in 0:00:02.321085 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.519198\n",
      "F1 Score (micro) =   0.588549\n",
      "F1 Score (macro) =   0.301572\n",
      "F1 Score (samples) =   0.551667\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.0959, Completed in 0:00:06.176827 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3520, Completed in 0:00:02.320668 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.531529\n",
      "F1 Score (micro) =   0.592398\n",
      "F1 Score (macro) =   0.315470\n",
      "F1 Score (samples) =   0.564167\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.0474, Completed in 0:00:06.175605 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3804, Completed in 0:00:02.317994 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.534753\n",
      "F1 Score (micro) =   0.563758\n",
      "F1 Score (macro) =   0.337127\n",
      "F1 Score (samples) =   0.521667\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of    400.  Loss 0.0266  Elapsed: 0:00:00.400584.\n",
      "  Batch    80  of    400.  Loss 0.0039  Elapsed: 0:00:00.787822.\n",
      "  Batch   120  of    400.  Loss 0.0040  Elapsed: 0:00:01.176336.\n",
      "  Batch   160  of    400.  Loss 0.0040  Elapsed: 0:00:01.563572.\n",
      "  Batch   200  of    400.  Loss 0.0195  Elapsed: 0:00:01.950086.\n",
      "  Batch   240  of    400.  Loss 0.0038  Elapsed: 0:00:02.338496.\n",
      "  Batch   280  of    400.  Loss 0.0046  Elapsed: 0:00:02.728514.\n",
      "  Batch   320  of    400.  Loss 0.0035  Elapsed: 0:00:03.116572.\n",
      "  Batch   360  of    400.  Loss 0.0048  Elapsed: 0:00:03.502659.\n",
      "Avg Validation Loss 0.0188, Completed in 0:00:03.879518 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.985119\n",
      "F1 Score (micro) =   0.985000\n",
      "F1 Score (macro) =   0.959489\n",
      "F1 Score (samples) =   0.983333\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 1.2599  Elapsed: 0:00:00.399655.\n",
      "  Batch    80  of  1,477.  Loss 0.0082  Elapsed: 0:00:00.788182.\n",
      "  Batch   120  of  1,477.  Loss 0.4511  Elapsed: 0:00:01.174454.\n",
      "  Batch   160  of  1,477.  Loss 0.0077  Elapsed: 0:00:01.563848.\n",
      "  Batch   200  of  1,477.  Loss 0.0899  Elapsed: 0:00:01.949675.\n",
      "  Batch   240  of  1,477.  Loss 0.1642  Elapsed: 0:00:02.337473.\n",
      "  Batch   280  of  1,477.  Loss 0.1348  Elapsed: 0:00:02.724218.\n",
      "  Batch   320  of  1,477.  Loss 0.1461  Elapsed: 0:00:03.115734.\n",
      "  Batch   360  of  1,477.  Loss 1.4208  Elapsed: 0:00:03.502786.\n",
      "  Batch   400  of  1,477.  Loss 0.0042  Elapsed: 0:00:03.891740.\n",
      "  Batch   440  of  1,477.  Loss 0.7234  Elapsed: 0:00:04.280206.\n",
      "  Batch   480  of  1,477.  Loss 1.3870  Elapsed: 0:00:04.669531.\n",
      "  Batch   520  of  1,477.  Loss 1.0091  Elapsed: 0:00:05.058735.\n",
      "  Batch   560  of  1,477.  Loss 0.0040  Elapsed: 0:00:05.446946.\n",
      "  Batch   600  of  1,477.  Loss 0.6598  Elapsed: 0:00:05.836661.\n",
      "  Batch   640  of  1,477.  Loss 0.1672  Elapsed: 0:00:06.222452.\n",
      "  Batch   680  of  1,477.  Loss 1.4480  Elapsed: 0:00:06.609895.\n",
      "  Batch   720  of  1,477.  Loss 0.0062  Elapsed: 0:00:06.998605.\n",
      "  Batch   760  of  1,477.  Loss 0.0307  Elapsed: 0:00:07.385687.\n",
      "  Batch   800  of  1,477.  Loss 0.4888  Elapsed: 0:00:07.772448.\n",
      "  Batch   840  of  1,477.  Loss 0.0124  Elapsed: 0:00:08.159339.\n",
      "  Batch   880  of  1,477.  Loss 0.9654  Elapsed: 0:00:08.548698.\n",
      "  Batch   920  of  1,477.  Loss 0.0050  Elapsed: 0:00:08.933392.\n",
      "  Batch   960  of  1,477.  Loss 0.0591  Elapsed: 0:00:09.321558.\n",
      "  Batch 1,000  of  1,477.  Loss 0.6953  Elapsed: 0:00:09.708898.\n",
      "  Batch 1,040  of  1,477.  Loss 0.8305  Elapsed: 0:00:10.096870.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0208  Elapsed: 0:00:10.485023.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0062  Elapsed: 0:00:10.873018.\n",
      "  Batch 1,160  of  1,477.  Loss 0.5429  Elapsed: 0:00:11.258136.\n",
      "  Batch 1,200  of  1,477.  Loss 0.2452  Elapsed: 0:00:11.644754.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0038  Elapsed: 0:00:12.035244.\n",
      "  Batch 1,280  of  1,477.  Loss 0.5060  Elapsed: 0:00:12.424567.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0115  Elapsed: 0:00:12.813455.\n",
      "  Batch 1,360  of  1,477.  Loss 0.5874  Elapsed: 0:00:13.202274.\n",
      "  Batch 1,400  of  1,477.  Loss 0.3564  Elapsed: 0:00:13.589371.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0025  Elapsed: 0:00:13.980112.\n",
      "Avg Validation Loss 0.3857, Completed in 0:00:14.329708 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.536263\n",
      "F1 Score (micro) =   0.557720\n",
      "F1 Score (macro) =   0.342932\n",
      "F1 Score (samples) =   0.520876\n",
      "Running Experiment GEMB_500\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2925, Completed in 0:00:07.806066 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3014, Completed in 0:00:02.830006 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.458179\n",
      "F1 Score (micro) =   0.562780\n",
      "F1 Score (macro) =   0.227759\n",
      "F1 Score (samples) =   0.502000\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.1866, Completed in 0:00:07.807151 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.2895, Completed in 0:00:02.833441 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.515529\n",
      "F1 Score (micro) =   0.562077\n",
      "F1 Score (macro) =   0.308959\n",
      "F1 Score (samples) =   0.496000\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.1109, Completed in 0:00:07.809881 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3437, Completed in 0:00:02.830069 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.551568\n",
      "F1 Score (micro) =   0.583153\n",
      "F1 Score (macro) =   0.364747\n",
      "F1 Score (samples) =   0.538667\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.0459, Completed in 0:00:07.809514 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.4151, Completed in 0:00:02.828521 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.566399\n",
      "F1 Score (micro) =   0.604990\n",
      "F1 Score (macro) =   0.367684\n",
      "F1 Score (samples) =   0.580667\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of    500.  Loss 0.3691  Elapsed: 0:00:00.397723.\n",
      "  Batch    80  of    500.  Loss 0.0042  Elapsed: 0:00:00.783494.\n",
      "  Batch   120  of    500.  Loss 0.0035  Elapsed: 0:00:01.170518.\n",
      "  Batch   160  of    500.  Loss 0.0823  Elapsed: 0:00:01.555931.\n",
      "  Batch   200  of    500.  Loss 0.0032  Elapsed: 0:00:01.941972.\n",
      "  Batch   240  of    500.  Loss 0.0040  Elapsed: 0:00:02.328455.\n",
      "  Batch   280  of    500.  Loss 0.0033  Elapsed: 0:00:02.712224.\n",
      "  Batch   320  of    500.  Loss 0.0032  Elapsed: 0:00:03.099960.\n",
      "  Batch   360  of    500.  Loss 0.0033  Elapsed: 0:00:03.486208.\n",
      "  Batch   400  of    500.  Loss 0.0036  Elapsed: 0:00:03.873122.\n",
      "  Batch   440  of    500.  Loss 0.0035  Elapsed: 0:00:04.258381.\n",
      "  Batch   480  of    500.  Loss 0.0054  Elapsed: 0:00:04.645176.\n",
      "Avg Validation Loss 0.0156, Completed in 0:00:04.827736 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.993754\n",
      "F1 Score (micro) =   0.993976\n",
      "F1 Score (macro) =   0.975684\n",
      "F1 Score (samples) =   0.990000\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0031  Elapsed: 0:00:00.395733.\n",
      "  Batch    80  of  1,477.  Loss 0.8597  Elapsed: 0:00:00.781506.\n",
      "  Batch   120  of  1,477.  Loss 0.0049  Elapsed: 0:00:01.163813.\n",
      "  Batch   160  of  1,477.  Loss 1.4756  Elapsed: 0:00:01.549559.\n",
      "  Batch   200  of  1,477.  Loss 0.0037  Elapsed: 0:00:01.936068.\n",
      "  Batch   240  of  1,477.  Loss 0.0624  Elapsed: 0:00:02.323870.\n",
      "  Batch   280  of  1,477.  Loss 1.4995  Elapsed: 0:00:02.709460.\n",
      "  Batch   320  of  1,477.  Loss 0.0727  Elapsed: 0:00:03.095737.\n",
      "  Batch   360  of  1,477.  Loss 0.3455  Elapsed: 0:00:03.481885.\n",
      "  Batch   400  of  1,477.  Loss 0.4141  Elapsed: 0:00:03.868467.\n",
      "  Batch   440  of  1,477.  Loss 0.0101  Elapsed: 0:00:04.257056.\n",
      "  Batch   480  of  1,477.  Loss 0.0147  Elapsed: 0:00:04.643152.\n",
      "  Batch   520  of  1,477.  Loss 1.2782  Elapsed: 0:00:05.027936.\n",
      "  Batch   560  of  1,477.  Loss 0.0691  Elapsed: 0:00:05.414402.\n",
      "  Batch   600  of  1,477.  Loss 0.0449  Elapsed: 0:00:05.798688.\n",
      "  Batch   640  of  1,477.  Loss 0.0043  Elapsed: 0:00:06.185190.\n",
      "  Batch   680  of  1,477.  Loss 1.6212  Elapsed: 0:00:06.572639.\n",
      "  Batch   720  of  1,477.  Loss 0.0053  Elapsed: 0:00:06.960995.\n",
      "  Batch   760  of  1,477.  Loss 1.5377  Elapsed: 0:00:07.348584.\n",
      "  Batch   800  of  1,477.  Loss 1.5063  Elapsed: 0:00:07.734456.\n",
      "  Batch   840  of  1,477.  Loss 0.9374  Elapsed: 0:00:08.120626.\n",
      "  Batch   880  of  1,477.  Loss 0.0060  Elapsed: 0:00:08.506937.\n",
      "  Batch   920  of  1,477.  Loss 0.5914  Elapsed: 0:00:08.895309.\n",
      "  Batch   960  of  1,477.  Loss 0.0193  Elapsed: 0:00:09.281185.\n",
      "  Batch 1,000  of  1,477.  Loss 1.0667  Elapsed: 0:00:09.669263.\n",
      "  Batch 1,040  of  1,477.  Loss 1.0861  Elapsed: 0:00:10.054429.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0421  Elapsed: 0:00:10.441794.\n",
      "  Batch 1,120  of  1,477.  Loss 1.4316  Elapsed: 0:00:10.826404.\n",
      "  Batch 1,160  of  1,477.  Loss 1.0378  Elapsed: 0:00:11.211158.\n",
      "  Batch 1,200  of  1,477.  Loss 0.2697  Elapsed: 0:00:11.600998.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0061  Elapsed: 0:00:11.987084.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0734  Elapsed: 0:00:12.372070.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0075  Elapsed: 0:00:12.758044.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0041  Elapsed: 0:00:13.148465.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0833  Elapsed: 0:00:13.535391.\n",
      "  Batch 1,440  of  1,477.  Loss 1.0814  Elapsed: 0:00:13.921632.\n",
      "Avg Validation Loss 0.3929, Completed in 0:00:14.270335 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.558990\n",
      "F1 Score (micro) =   0.600071\n",
      "F1 Score (macro) =   0.358485\n",
      "F1 Score (samples) =   0.570526\n",
      "Running Experiment GEMB_1000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2630, Completed in 0:00:15.601827 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     63.  Loss 0.1928  Elapsed: 0:00:03.729940.\n",
      "Avg Validation Loss 0.2868, Completed in 0:00:05.660218 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.460154\n",
      "F1 Score (micro) =   0.552036\n",
      "F1 Score (macro) =   0.238571\n",
      "F1 Score (samples) =   0.488000\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.1716, Completed in 0:00:15.599847 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     63.  Loss 0.3003  Elapsed: 0:00:03.727388.\n",
      "Avg Validation Loss 0.2964, Completed in 0:00:05.666577 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.546468\n",
      "F1 Score (micro) =   0.593785\n",
      "F1 Score (macro) =   0.355700\n",
      "F1 Score (samples) =   0.534667\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.0873, Completed in 0:00:15.602741 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     63.  Loss 0.5430  Elapsed: 0:00:03.726251.\n",
      "Avg Validation Loss 0.3425, Completed in 0:00:05.662022 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.547625\n",
      "F1 Score (micro) =   0.567657\n",
      "F1 Score (macro) =   0.378508\n",
      "F1 Score (samples) =   0.512333\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.0371, Completed in 0:00:15.601982 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     63.  Loss 0.3874  Elapsed: 0:00:03.728608.\n",
      "Avg Validation Loss 0.4096, Completed in 0:00:05.666577 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.571558\n",
      "F1 Score (micro) =   0.591123\n",
      "F1 Score (macro) =   0.403731\n",
      "F1 Score (samples) =   0.563333\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,000.  Loss 0.0047  Elapsed: 0:00:00.402738.\n",
      "  Batch    80  of  1,000.  Loss 0.0051  Elapsed: 0:00:00.792416.\n",
      "  Batch   120  of  1,000.  Loss 0.0214  Elapsed: 0:00:01.183901.\n",
      "  Batch   160  of  1,000.  Loss 0.0038  Elapsed: 0:00:01.577091.\n",
      "  Batch   200  of  1,000.  Loss 0.0144  Elapsed: 0:00:01.968419.\n",
      "  Batch   240  of  1,000.  Loss 0.0037  Elapsed: 0:00:02.359922.\n",
      "  Batch   280  of  1,000.  Loss 0.0269  Elapsed: 0:00:02.750218.\n",
      "  Batch   320  of  1,000.  Loss 0.2453  Elapsed: 0:00:03.139951.\n",
      "  Batch   360  of  1,000.  Loss 0.0043  Elapsed: 0:00:03.530039.\n",
      "  Batch   400  of  1,000.  Loss 0.0117  Elapsed: 0:00:03.920690.\n",
      "  Batch   440  of  1,000.  Loss 0.0030  Elapsed: 0:00:04.312029.\n",
      "  Batch   480  of  1,000.  Loss 0.0062  Elapsed: 0:00:04.702695.\n",
      "  Batch   520  of  1,000.  Loss 0.0038  Elapsed: 0:00:05.093405.\n",
      "  Batch   560  of  1,000.  Loss 0.0041  Elapsed: 0:00:05.485543.\n",
      "  Batch   600  of  1,000.  Loss 0.0026  Elapsed: 0:00:05.875871.\n",
      "  Batch   640  of  1,000.  Loss 0.0206  Elapsed: 0:00:06.266938.\n",
      "  Batch   680  of  1,000.  Loss 0.0177  Elapsed: 0:00:06.655693.\n",
      "  Batch   720  of  1,000.  Loss 0.0027  Elapsed: 0:00:07.048022.\n",
      "  Batch   760  of  1,000.  Loss 0.0029  Elapsed: 0:00:07.436948.\n",
      "  Batch   800  of  1,000.  Loss 0.0028  Elapsed: 0:00:07.828059.\n",
      "  Batch   840  of  1,000.  Loss 0.0078  Elapsed: 0:00:08.217345.\n",
      "  Batch   880  of  1,000.  Loss 0.0028  Elapsed: 0:00:08.608315.\n",
      "  Batch   920  of  1,000.  Loss 0.0036  Elapsed: 0:00:08.997600.\n",
      "  Batch   960  of  1,000.  Loss 0.0029  Elapsed: 0:00:09.385059.\n",
      "Avg Validation Loss 0.0134, Completed in 0:00:09.764853 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.994994\n",
      "F1 Score (micro) =   0.995000\n",
      "F1 Score (macro) =   0.986130\n",
      "F1 Score (samples) =   0.994333\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0026  Elapsed: 0:00:00.401966.\n",
      "  Batch    80  of  1,477.  Loss 0.0042  Elapsed: 0:00:00.791745.\n",
      "  Batch   120  of  1,477.  Loss 0.0257  Elapsed: 0:00:01.185814.\n",
      "  Batch   160  of  1,477.  Loss 0.0029  Elapsed: 0:00:01.577470.\n",
      "  Batch   200  of  1,477.  Loss 0.1941  Elapsed: 0:00:01.967491.\n",
      "  Batch   240  of  1,477.  Loss 0.1502  Elapsed: 0:00:02.356306.\n",
      "  Batch   280  of  1,477.  Loss 0.0355  Elapsed: 0:00:02.749565.\n",
      "  Batch   320  of  1,477.  Loss 0.2954  Elapsed: 0:00:03.141841.\n",
      "  Batch   360  of  1,477.  Loss 0.0046  Elapsed: 0:00:03.530795.\n",
      "  Batch   400  of  1,477.  Loss 0.0097  Elapsed: 0:00:03.922963.\n",
      "  Batch   440  of  1,477.  Loss 0.2277  Elapsed: 0:00:04.313860.\n",
      "  Batch   480  of  1,477.  Loss 1.0867  Elapsed: 0:00:04.704267.\n",
      "  Batch   520  of  1,477.  Loss 0.0060  Elapsed: 0:00:05.093625.\n",
      "  Batch   560  of  1,477.  Loss 0.0741  Elapsed: 0:00:05.486112.\n",
      "  Batch   600  of  1,477.  Loss 0.4271  Elapsed: 0:00:05.876480.\n",
      "  Batch   640  of  1,477.  Loss 0.0030  Elapsed: 0:00:06.266482.\n",
      "  Batch   680  of  1,477.  Loss 0.0995  Elapsed: 0:00:06.657535.\n",
      "  Batch   720  of  1,477.  Loss 0.1363  Elapsed: 0:00:07.050250.\n",
      "  Batch   760  of  1,477.  Loss 0.4336  Elapsed: 0:00:07.439361.\n",
      "  Batch   800  of  1,477.  Loss 0.5289  Elapsed: 0:00:07.831601.\n",
      "  Batch   840  of  1,477.  Loss 0.0072  Elapsed: 0:00:08.224157.\n",
      "  Batch   880  of  1,477.  Loss 0.7652  Elapsed: 0:00:08.614673.\n",
      "  Batch   920  of  1,477.  Loss 0.1747  Elapsed: 0:00:09.003222.\n",
      "  Batch   960  of  1,477.  Loss 0.0027  Elapsed: 0:00:09.392930.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0180  Elapsed: 0:00:09.783971.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0116  Elapsed: 0:00:10.175996.\n",
      "  Batch 1,080  of  1,477.  Loss 0.4526  Elapsed: 0:00:10.569722.\n",
      "  Batch 1,120  of  1,477.  Loss 0.8750  Elapsed: 0:00:10.962112.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0042  Elapsed: 0:00:11.351536.\n",
      "  Batch 1,200  of  1,477.  Loss 0.1224  Elapsed: 0:00:11.743393.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0026  Elapsed: 0:00:12.132248.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0026  Elapsed: 0:00:12.523843.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0038  Elapsed: 0:00:12.917248.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0032  Elapsed: 0:00:13.308406.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0229  Elapsed: 0:00:13.701292.\n",
      "  Batch 1,440  of  1,477.  Loss 0.3372  Elapsed: 0:00:14.089996.\n",
      "Avg Validation Loss 0.3796, Completed in 0:00:14.441112 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.587138\n",
      "F1 Score (micro) =   0.603399\n",
      "F1 Score (macro) =   0.388429\n",
      "F1 Score (samples) =   0.574137\n",
      "Running Experiment GEMB_1500\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2553, Completed in 0:00:23.392201 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3182  Elapsed: 0:00:03.741680.\n",
      "  Batch    80  of     93.  Loss 0.2269  Elapsed: 0:00:07.346471.\n",
      "Avg Validation Loss 0.2774, Completed in 0:00:08.365575 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.503187\n",
      "F1 Score (micro) =   0.556508\n",
      "F1 Score (macro) =   0.299204\n",
      "F1 Score (samples) =   0.461521\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.1711, Completed in 0:00:23.396844 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2386  Elapsed: 0:00:03.752074.\n",
      "  Batch    80  of     93.  Loss 0.1949  Elapsed: 0:00:07.352566.\n",
      "Avg Validation Loss 0.2803, Completed in 0:00:08.366764 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.570858\n",
      "F1 Score (micro) =   0.605385\n",
      "F1 Score (macro) =   0.382636\n",
      "F1 Score (samples) =   0.532611\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.0878, Completed in 0:00:23.393476 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2647  Elapsed: 0:00:03.748230.\n",
      "  Batch    80  of     93.  Loss 0.3190  Elapsed: 0:00:07.346593.\n",
      "Avg Validation Loss 0.3379, Completed in 0:00:08.367578 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.601447\n",
      "F1 Score (micro) =   0.620116\n",
      "F1 Score (macro) =   0.466670\n",
      "F1 Score (samples) =   0.577973\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.0414, Completed in 0:00:23.394914 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1903  Elapsed: 0:00:03.748194.\n",
      "  Batch    80  of     93.  Loss 0.3070  Elapsed: 0:00:07.494457.\n",
      "Avg Validation Loss 0.4130, Completed in 0:00:08.514424 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.605547\n",
      "F1 Score (micro) =   0.636647\n",
      "F1 Score (macro) =   0.460160\n",
      "F1 Score (samples) =   0.620402\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,500.  Loss 0.0030  Elapsed: 0:00:00.402829.\n",
      "  Batch    80  of  1,500.  Loss 0.0028  Elapsed: 0:00:00.791625.\n",
      "  Batch   120  of  1,500.  Loss 0.0030  Elapsed: 0:00:01.181114.\n",
      "  Batch   160  of  1,500.  Loss 0.0024  Elapsed: 0:00:01.572926.\n",
      "  Batch   200  of  1,500.  Loss 0.2820  Elapsed: 0:00:01.961518.\n",
      "  Batch   240  of  1,500.  Loss 0.0025  Elapsed: 0:00:02.349699.\n",
      "  Batch   280  of  1,500.  Loss 0.2397  Elapsed: 0:00:02.744316.\n",
      "  Batch   320  of  1,500.  Loss 0.0033  Elapsed: 0:00:03.134569.\n",
      "  Batch   360  of  1,500.  Loss 0.0033  Elapsed: 0:00:03.526535.\n",
      "  Batch   400  of  1,500.  Loss 0.0252  Elapsed: 0:00:03.918736.\n",
      "  Batch   440  of  1,500.  Loss 0.0031  Elapsed: 0:00:04.309798.\n",
      "  Batch   480  of  1,500.  Loss 0.0069  Elapsed: 0:00:04.702094.\n",
      "  Batch   520  of  1,500.  Loss 0.0027  Elapsed: 0:00:05.091363.\n",
      "  Batch   560  of  1,500.  Loss 0.0024  Elapsed: 0:00:05.482588.\n",
      "  Batch   600  of  1,500.  Loss 0.0028  Elapsed: 0:00:05.872848.\n",
      "  Batch   640  of  1,500.  Loss 0.0107  Elapsed: 0:00:06.261342.\n",
      "  Batch   680  of  1,500.  Loss 0.0034  Elapsed: 0:00:06.655716.\n",
      "  Batch   720  of  1,500.  Loss 0.0184  Elapsed: 0:00:07.045794.\n",
      "  Batch   760  of  1,500.  Loss 0.0029  Elapsed: 0:00:07.437865.\n",
      "  Batch   800  of  1,500.  Loss 0.0025  Elapsed: 0:00:07.829196.\n",
      "  Batch   840  of  1,500.  Loss 0.0152  Elapsed: 0:00:08.220574.\n",
      "  Batch   880  of  1,500.  Loss 0.0639  Elapsed: 0:00:08.611832.\n",
      "  Batch   920  of  1,500.  Loss 0.0030  Elapsed: 0:00:09.003468.\n",
      "  Batch   960  of  1,500.  Loss 0.0031  Elapsed: 0:00:09.394055.\n",
      "  Batch 1,000  of  1,500.  Loss 0.0061  Elapsed: 0:00:09.786335.\n",
      "  Batch 1,040  of  1,500.  Loss 0.0059  Elapsed: 0:00:10.177851.\n",
      "  Batch 1,080  of  1,500.  Loss 0.0034  Elapsed: 0:00:10.569364.\n",
      "  Batch 1,120  of  1,500.  Loss 0.0026  Elapsed: 0:00:10.960051.\n",
      "  Batch 1,160  of  1,500.  Loss 0.0026  Elapsed: 0:00:11.349162.\n",
      "  Batch 1,200  of  1,500.  Loss 0.0027  Elapsed: 0:00:11.740277.\n",
      "  Batch 1,240  of  1,500.  Loss 0.0358  Elapsed: 0:00:12.130256.\n",
      "  Batch 1,280  of  1,500.  Loss 0.0118  Elapsed: 0:00:12.519928.\n",
      "  Batch 1,320  of  1,500.  Loss 0.0126  Elapsed: 0:00:12.910605.\n",
      "  Batch 1,360  of  1,500.  Loss 0.0031  Elapsed: 0:00:13.302211.\n",
      "  Batch 1,400  of  1,500.  Loss 0.0101  Elapsed: 0:00:13.692531.\n",
      "  Batch 1,440  of  1,500.  Loss 0.0025  Elapsed: 0:00:14.081585.\n",
      "  Batch 1,480  of  1,500.  Loss 0.0032  Elapsed: 0:00:14.471620.\n",
      "Avg Validation Loss 0.0132, Completed in 0:00:14.657684 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.990865\n",
      "F1 Score (micro) =   0.990979\n",
      "F1 Score (macro) =   0.975923\n",
      "F1 Score (samples) =   0.987778\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0054  Elapsed: 0:00:00.401195.\n",
      "  Batch    80  of  1,477.  Loss 0.0024  Elapsed: 0:00:00.794156.\n",
      "  Batch   120  of  1,477.  Loss 0.0103  Elapsed: 0:00:01.187733.\n",
      "  Batch   160  of  1,477.  Loss 0.3165  Elapsed: 0:00:01.578013.\n",
      "  Batch   200  of  1,477.  Loss 1.1485  Elapsed: 0:00:01.968799.\n",
      "  Batch   240  of  1,477.  Loss 1.3999  Elapsed: 0:00:02.359772.\n",
      "  Batch   280  of  1,477.  Loss 0.4119  Elapsed: 0:00:02.751559.\n",
      "  Batch   320  of  1,477.  Loss 0.1928  Elapsed: 0:00:03.143092.\n",
      "  Batch   360  of  1,477.  Loss 0.4478  Elapsed: 0:00:03.534274.\n",
      "  Batch   400  of  1,477.  Loss 0.2958  Elapsed: 0:00:03.925561.\n",
      "  Batch   440  of  1,477.  Loss 0.0777  Elapsed: 0:00:04.317138.\n",
      "  Batch   480  of  1,477.  Loss 0.0034  Elapsed: 0:00:04.710174.\n",
      "  Batch   520  of  1,477.  Loss 0.2397  Elapsed: 0:00:05.101223.\n",
      "  Batch   560  of  1,477.  Loss 0.0028  Elapsed: 0:00:05.495319.\n",
      "  Batch   600  of  1,477.  Loss 0.0137  Elapsed: 0:00:05.886884.\n",
      "  Batch   640  of  1,477.  Loss 0.0026  Elapsed: 0:00:06.279968.\n",
      "  Batch   680  of  1,477.  Loss 0.0035  Elapsed: 0:00:06.671657.\n",
      "  Batch   720  of  1,477.  Loss 0.0024  Elapsed: 0:00:07.063597.\n",
      "  Batch   760  of  1,477.  Loss 0.2024  Elapsed: 0:00:07.453094.\n",
      "  Batch   800  of  1,477.  Loss 0.0329  Elapsed: 0:00:07.846014.\n",
      "  Batch   840  of  1,477.  Loss 0.0035  Elapsed: 0:00:08.236291.\n",
      "  Batch   880  of  1,477.  Loss 0.8318  Elapsed: 0:00:08.627955.\n",
      "  Batch   920  of  1,477.  Loss 0.0146  Elapsed: 0:00:09.015960.\n",
      "  Batch   960  of  1,477.  Loss 1.5757  Elapsed: 0:00:09.406319.\n",
      "  Batch 1,000  of  1,477.  Loss 0.9882  Elapsed: 0:00:09.799314.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0249  Elapsed: 0:00:10.191123.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0032  Elapsed: 0:00:10.585765.\n",
      "  Batch 1,120  of  1,477.  Loss 0.4672  Elapsed: 0:00:10.979749.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0029  Elapsed: 0:00:11.374468.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0167  Elapsed: 0:00:11.769108.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0025  Elapsed: 0:00:12.162028.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0299  Elapsed: 0:00:12.555312.\n",
      "  Batch 1,320  of  1,477.  Loss 1.3219  Elapsed: 0:00:12.946362.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0144  Elapsed: 0:00:13.335964.\n",
      "  Batch 1,400  of  1,477.  Loss 0.4727  Elapsed: 0:00:13.731505.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0179  Elapsed: 0:00:14.123598.\n",
      "Avg Validation Loss 0.3886, Completed in 0:00:14.478089 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.616963\n",
      "F1 Score (micro) =   0.647712\n",
      "F1 Score (macro) =   0.437165\n",
      "F1 Score (samples) =   0.630106\n",
      "Running Experiment GEMB_2000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    125.  Loss 0.2847  Elapsed: 0:00:25.110878.\n",
      "Avg Training Loss 0.2559, Completed in 0:00:31.193062 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2988  Elapsed: 0:00:03.765553.\n",
      "  Batch    80  of     93.  Loss 0.3297  Elapsed: 0:00:07.377591.\n",
      "Avg Validation Loss 0.2772, Completed in 0:00:08.391602 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.495226\n",
      "F1 Score (micro) =   0.575269\n",
      "F1 Score (macro) =   0.278786\n",
      "F1 Score (samples) =   0.507109\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    125.  Loss 0.1679  Elapsed: 0:00:25.111359.\n",
      "Avg Training Loss 0.1761, Completed in 0:00:31.195839 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2775  Elapsed: 0:00:03.771368.\n",
      "  Batch    80  of     93.  Loss 0.1525  Elapsed: 0:00:07.372603.\n",
      "Avg Validation Loss 0.2851, Completed in 0:00:08.387355 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.553428\n",
      "F1 Score (micro) =   0.607024\n",
      "F1 Score (macro) =   0.347067\n",
      "F1 Score (samples) =   0.554954\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    125.  Loss 0.0661  Elapsed: 0:00:25.104633.\n",
      "Avg Training Loss 0.0904, Completed in 0:00:31.187442 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2055  Elapsed: 0:00:03.765100.\n",
      "  Batch    80  of     93.  Loss 0.2758  Elapsed: 0:00:07.369925.\n",
      "Avg Validation Loss 0.3322, Completed in 0:00:08.389793 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.593430\n",
      "F1 Score (micro) =   0.609862\n",
      "F1 Score (macro) =   0.444731\n",
      "F1 Score (samples) =   0.566689\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    125.  Loss 0.0112  Elapsed: 0:00:25.104935.\n",
      "Avg Training Loss 0.0395, Completed in 0:00:31.186351 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.5651  Elapsed: 0:00:03.768229.\n",
      "  Batch    80  of     93.  Loss 0.2769  Elapsed: 0:00:07.367544.\n",
      "Avg Validation Loss 0.3824, Completed in 0:00:08.391588 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.603637\n",
      "F1 Score (micro) =   0.621291\n",
      "F1 Score (macro) =   0.444842\n",
      "F1 Score (samples) =   0.599188\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  2,000.  Loss 0.0024  Elapsed: 0:00:00.402787.\n",
      "  Batch    80  of  2,000.  Loss 0.0035  Elapsed: 0:00:00.794573.\n",
      "  Batch   120  of  2,000.  Loss 0.0027  Elapsed: 0:00:01.185105.\n",
      "  Batch   160  of  2,000.  Loss 0.0022  Elapsed: 0:00:01.577792.\n",
      "  Batch   200  of  2,000.  Loss 0.0026  Elapsed: 0:00:01.969176.\n",
      "  Batch   240  of  2,000.  Loss 0.0216  Elapsed: 0:00:02.359683.\n",
      "  Batch   280  of  2,000.  Loss 0.0042  Elapsed: 0:00:02.752323.\n",
      "  Batch   320  of  2,000.  Loss 0.0022  Elapsed: 0:00:03.146164.\n",
      "  Batch   360  of  2,000.  Loss 0.0033  Elapsed: 0:00:03.536285.\n",
      "  Batch   400  of  2,000.  Loss 0.0033  Elapsed: 0:00:03.926414.\n",
      "  Batch   440  of  2,000.  Loss 0.0111  Elapsed: 0:00:04.317421.\n",
      "  Batch   480  of  2,000.  Loss 0.0041  Elapsed: 0:00:04.709985.\n",
      "  Batch   520  of  2,000.  Loss 0.0039  Elapsed: 0:00:05.099883.\n",
      "  Batch   560  of  2,000.  Loss 0.0024  Elapsed: 0:00:05.492028.\n",
      "  Batch   600  of  2,000.  Loss 0.0024  Elapsed: 0:00:05.883885.\n",
      "  Batch   640  of  2,000.  Loss 0.0022  Elapsed: 0:00:06.274992.\n",
      "  Batch   680  of  2,000.  Loss 0.0023  Elapsed: 0:00:06.662274.\n",
      "  Batch   720  of  2,000.  Loss 0.0025  Elapsed: 0:00:07.051466.\n",
      "  Batch   760  of  2,000.  Loss 0.0022  Elapsed: 0:00:07.442240.\n",
      "  Batch   800  of  2,000.  Loss 0.0037  Elapsed: 0:00:07.834295.\n",
      "  Batch   840  of  2,000.  Loss 0.0148  Elapsed: 0:00:08.222544.\n",
      "  Batch   880  of  2,000.  Loss 0.0022  Elapsed: 0:00:08.613285.\n",
      "  Batch   920  of  2,000.  Loss 0.0027  Elapsed: 0:00:09.003848.\n",
      "  Batch   960  of  2,000.  Loss 0.0022  Elapsed: 0:00:09.394551.\n",
      "  Batch 1,000  of  2,000.  Loss 0.0025  Elapsed: 0:00:09.785606.\n",
      "  Batch 1,040  of  2,000.  Loss 0.0113  Elapsed: 0:00:10.177232.\n",
      "  Batch 1,080  of  2,000.  Loss 0.0023  Elapsed: 0:00:10.569856.\n",
      "  Batch 1,120  of  2,000.  Loss 0.0025  Elapsed: 0:00:10.959031.\n",
      "  Batch 1,160  of  2,000.  Loss 0.0021  Elapsed: 0:00:11.351453.\n",
      "  Batch 1,200  of  2,000.  Loss 0.0031  Elapsed: 0:00:11.743129.\n",
      "  Batch 1,240  of  2,000.  Loss 0.0042  Elapsed: 0:00:12.136329.\n",
      "  Batch 1,280  of  2,000.  Loss 0.0041  Elapsed: 0:00:12.528585.\n",
      "  Batch 1,320  of  2,000.  Loss 0.0110  Elapsed: 0:00:12.916608.\n",
      "  Batch 1,360  of  2,000.  Loss 0.0022  Elapsed: 0:00:13.308361.\n",
      "  Batch 1,400  of  2,000.  Loss 0.0112  Elapsed: 0:00:13.701001.\n",
      "  Batch 1,440  of  2,000.  Loss 0.0051  Elapsed: 0:00:14.092161.\n",
      "  Batch 1,480  of  2,000.  Loss 0.0024  Elapsed: 0:00:14.481646.\n",
      "  Batch 1,520  of  2,000.  Loss 0.0040  Elapsed: 0:00:14.872906.\n",
      "  Batch 1,560  of  2,000.  Loss 0.0036  Elapsed: 0:00:15.262568.\n",
      "  Batch 1,600  of  2,000.  Loss 0.0029  Elapsed: 0:00:15.654423.\n",
      "  Batch 1,640  of  2,000.  Loss 0.0025  Elapsed: 0:00:16.045472.\n",
      "  Batch 1,680  of  2,000.  Loss 0.0255  Elapsed: 0:00:16.437314.\n",
      "  Batch 1,720  of  2,000.  Loss 0.0023  Elapsed: 0:00:16.826271.\n",
      "  Batch 1,760  of  2,000.  Loss 0.0027  Elapsed: 0:00:17.217242.\n",
      "  Batch 1,800  of  2,000.  Loss 0.0022  Elapsed: 0:00:17.610748.\n",
      "  Batch 1,840  of  2,000.  Loss 0.0086  Elapsed: 0:00:18.000044.\n",
      "  Batch 1,880  of  2,000.  Loss 0.0023  Elapsed: 0:00:18.392385.\n",
      "  Batch 1,920  of  2,000.  Loss 0.0033  Elapsed: 0:00:18.782971.\n",
      "  Batch 1,960  of  2,000.  Loss 0.0027  Elapsed: 0:00:19.173443.\n",
      "Avg Validation Loss 0.0116, Completed in 0:00:19.552835 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.994254\n",
      "F1 Score (micro) =   0.994249\n",
      "F1 Score (macro) =   0.986792\n",
      "F1 Score (samples) =   0.993500\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.8729  Elapsed: 0:00:00.397557.\n",
      "  Batch    80  of  1,477.  Loss 0.0237  Elapsed: 0:00:00.783162.\n",
      "  Batch   120  of  1,477.  Loss 1.0430  Elapsed: 0:00:01.167402.\n",
      "  Batch   160  of  1,477.  Loss 0.7781  Elapsed: 0:00:01.552781.\n",
      "  Batch   200  of  1,477.  Loss 0.6181  Elapsed: 0:00:01.941589.\n",
      "  Batch   240  of  1,477.  Loss 0.0136  Elapsed: 0:00:02.327419.\n",
      "  Batch   280  of  1,477.  Loss 0.0413  Elapsed: 0:00:02.714237.\n",
      "  Batch   320  of  1,477.  Loss 0.0065  Elapsed: 0:00:03.102868.\n",
      "  Batch   360  of  1,477.  Loss 0.0512  Elapsed: 0:00:03.490220.\n",
      "  Batch   400  of  1,477.  Loss 1.3416  Elapsed: 0:00:03.877964.\n",
      "  Batch   440  of  1,477.  Loss 0.0021  Elapsed: 0:00:04.264069.\n",
      "  Batch   480  of  1,477.  Loss 0.0055  Elapsed: 0:00:04.651013.\n",
      "  Batch   520  of  1,477.  Loss 1.5271  Elapsed: 0:00:05.038152.\n",
      "  Batch   560  of  1,477.  Loss 0.7676  Elapsed: 0:00:05.423464.\n",
      "  Batch   600  of  1,477.  Loss 0.1773  Elapsed: 0:00:05.810135.\n",
      "  Batch   640  of  1,477.  Loss 0.0034  Elapsed: 0:00:06.196100.\n",
      "  Batch   680  of  1,477.  Loss 0.9107  Elapsed: 0:00:06.582010.\n",
      "  Batch   720  of  1,477.  Loss 0.0662  Elapsed: 0:00:06.971608.\n",
      "  Batch   760  of  1,477.  Loss 0.0024  Elapsed: 0:00:07.360579.\n",
      "  Batch   800  of  1,477.  Loss 0.2018  Elapsed: 0:00:07.746450.\n",
      "  Batch   840  of  1,477.  Loss 0.7381  Elapsed: 0:00:08.133704.\n",
      "  Batch   880  of  1,477.  Loss 1.5675  Elapsed: 0:00:08.519113.\n",
      "  Batch   920  of  1,477.  Loss 0.7010  Elapsed: 0:00:08.906527.\n",
      "  Batch   960  of  1,477.  Loss 0.0053  Elapsed: 0:00:09.291635.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0030  Elapsed: 0:00:09.678250.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0023  Elapsed: 0:00:10.063331.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0023  Elapsed: 0:00:10.450872.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0449  Elapsed: 0:00:10.838926.\n",
      "  Batch 1,160  of  1,477.  Loss 1.0607  Elapsed: 0:00:11.228758.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0127  Elapsed: 0:00:11.615703.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0205  Elapsed: 0:00:12.001639.\n",
      "  Batch 1,280  of  1,477.  Loss 0.7436  Elapsed: 0:00:12.389642.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0349  Elapsed: 0:00:12.775738.\n",
      "  Batch 1,360  of  1,477.  Loss 0.2642  Elapsed: 0:00:13.161249.\n",
      "  Batch 1,400  of  1,477.  Loss 0.6405  Elapsed: 0:00:13.546814.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0037  Elapsed: 0:00:13.932892.\n",
      "Avg Validation Loss 0.3614, Completed in 0:00:14.281215 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.621170\n",
      "F1 Score (micro) =   0.635154\n",
      "F1 Score (macro) =   0.451144\n",
      "F1 Score (samples) =   0.609795\n",
      "Running Experiment GEMB_2500\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    157.  Loss 0.1854  Elapsed: 0:00:25.100760.\n",
      "Avg Training Loss 0.2451, Completed in 0:00:39.066494 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2987  Elapsed: 0:00:03.708560.\n",
      "  Batch    80  of     93.  Loss 0.3673  Elapsed: 0:00:07.297599.\n",
      "Avg Validation Loss 0.2746, Completed in 0:00:08.323088 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.526637\n",
      "F1 Score (micro) =   0.603692\n",
      "F1 Score (macro) =   0.306459\n",
      "F1 Score (samples) =   0.561950\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    157.  Loss 0.0722  Elapsed: 0:00:25.100356.\n",
      "Avg Training Loss 0.1669, Completed in 0:00:39.065124 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1326  Elapsed: 0:00:03.703901.\n",
      "  Batch    80  of     93.  Loss 0.2429  Elapsed: 0:00:07.302020.\n",
      "Avg Validation Loss 0.2578, Completed in 0:00:08.316549 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.616432\n",
      "F1 Score (micro) =   0.643123\n",
      "F1 Score (macro) =   0.464000\n",
      "F1 Score (samples) =   0.584067\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    157.  Loss 0.0658  Elapsed: 0:00:25.100864.\n",
      "Avg Training Loss 0.0869, Completed in 0:00:39.065815 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2449  Elapsed: 0:00:03.705098.\n",
      "  Batch    80  of     93.  Loss 0.5328  Elapsed: 0:00:07.301159.\n",
      "Avg Validation Loss 0.2796, Completed in 0:00:08.317195 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.613894\n",
      "F1 Score (micro) =   0.623048\n",
      "F1 Score (macro) =   0.487820\n",
      "F1 Score (samples) =   0.566012\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    157.  Loss 0.0914  Elapsed: 0:00:25.099771.\n",
      "Avg Training Loss 0.0443, Completed in 0:00:39.066078 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3866  Elapsed: 0:00:03.706630.\n",
      "  Batch    80  of     93.  Loss 0.6167  Elapsed: 0:00:07.304168.\n",
      "Avg Validation Loss 0.3412, Completed in 0:00:08.321321 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.627104\n",
      "F1 Score (micro) =   0.640229\n",
      "F1 Score (macro) =   0.485947\n",
      "F1 Score (samples) =   0.605958\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  2,500.  Loss 0.0026  Elapsed: 0:00:00.397483.\n",
      "  Batch    80  of  2,500.  Loss 0.0145  Elapsed: 0:00:00.783979.\n",
      "  Batch   120  of  2,500.  Loss 0.0261  Elapsed: 0:00:01.170066.\n",
      "  Batch   160  of  2,500.  Loss 0.0059  Elapsed: 0:00:01.556766.\n",
      "  Batch   200  of  2,500.  Loss 0.0023  Elapsed: 0:00:01.941884.\n",
      "  Batch   240  of  2,500.  Loss 0.0133  Elapsed: 0:00:02.328124.\n",
      "  Batch   280  of  2,500.  Loss 0.0280  Elapsed: 0:00:02.714155.\n",
      "  Batch   320  of  2,500.  Loss 0.0093  Elapsed: 0:00:03.100430.\n",
      "  Batch   360  of  2,500.  Loss 0.0128  Elapsed: 0:00:03.485126.\n",
      "  Batch   400  of  2,500.  Loss 0.0086  Elapsed: 0:00:03.871269.\n",
      "  Batch   440  of  2,500.  Loss 0.0022  Elapsed: 0:00:04.256222.\n",
      "  Batch   480  of  2,500.  Loss 0.0020  Elapsed: 0:00:04.642825.\n",
      "  Batch   520  of  2,500.  Loss 0.0026  Elapsed: 0:00:05.029948.\n",
      "  Batch   560  of  2,500.  Loss 0.0079  Elapsed: 0:00:05.414423.\n",
      "  Batch   600  of  2,500.  Loss 0.0052  Elapsed: 0:00:05.800034.\n",
      "  Batch   640  of  2,500.  Loss 0.0024  Elapsed: 0:00:06.186300.\n",
      "  Batch   680  of  2,500.  Loss 0.0035  Elapsed: 0:00:06.572209.\n",
      "  Batch   720  of  2,500.  Loss 0.0251  Elapsed: 0:00:06.958971.\n",
      "  Batch   760  of  2,500.  Loss 0.0021  Elapsed: 0:00:07.349148.\n",
      "  Batch   800  of  2,500.  Loss 0.0126  Elapsed: 0:00:07.734241.\n",
      "  Batch   840  of  2,500.  Loss 0.0023  Elapsed: 0:00:08.119191.\n",
      "  Batch   880  of  2,500.  Loss 0.0034  Elapsed: 0:00:08.506350.\n",
      "  Batch   920  of  2,500.  Loss 0.0020  Elapsed: 0:00:08.892428.\n",
      "  Batch   960  of  2,500.  Loss 0.0033  Elapsed: 0:00:09.276956.\n",
      "  Batch 1,000  of  2,500.  Loss 0.0554  Elapsed: 0:00:09.665344.\n",
      "  Batch 1,040  of  2,500.  Loss 0.0142  Elapsed: 0:00:10.051229.\n",
      "  Batch 1,080  of  2,500.  Loss 0.0266  Elapsed: 0:00:10.435009.\n",
      "  Batch 1,120  of  2,500.  Loss 0.0020  Elapsed: 0:00:10.817901.\n",
      "  Batch 1,160  of  2,500.  Loss 0.0838  Elapsed: 0:00:11.202957.\n",
      "  Batch 1,200  of  2,500.  Loss 0.0226  Elapsed: 0:00:11.587782.\n",
      "  Batch 1,240  of  2,500.  Loss 0.0027  Elapsed: 0:00:11.974301.\n",
      "  Batch 1,280  of  2,500.  Loss 0.0031  Elapsed: 0:00:12.359015.\n",
      "  Batch 1,320  of  2,500.  Loss 0.0028  Elapsed: 0:00:12.744778.\n",
      "  Batch 1,360  of  2,500.  Loss 0.0118  Elapsed: 0:00:13.128788.\n",
      "  Batch 1,400  of  2,500.  Loss 0.0027  Elapsed: 0:00:13.514488.\n",
      "  Batch 1,440  of  2,500.  Loss 0.0027  Elapsed: 0:00:13.897540.\n",
      "  Batch 1,480  of  2,500.  Loss 0.0024  Elapsed: 0:00:14.282167.\n",
      "  Batch 1,520  of  2,500.  Loss 0.0052  Elapsed: 0:00:14.668607.\n",
      "  Batch 1,560  of  2,500.  Loss 0.0160  Elapsed: 0:00:15.053861.\n",
      "  Batch 1,600  of  2,500.  Loss 0.1915  Elapsed: 0:00:15.439087.\n",
      "  Batch 1,640  of  2,500.  Loss 0.0111  Elapsed: 0:00:15.826816.\n",
      "  Batch 1,680  of  2,500.  Loss 0.0110  Elapsed: 0:00:16.213956.\n",
      "  Batch 1,720  of  2,500.  Loss 0.0021  Elapsed: 0:00:16.600501.\n",
      "  Batch 1,760  of  2,500.  Loss 0.0023  Elapsed: 0:00:16.986392.\n",
      "  Batch 1,800  of  2,500.  Loss 0.0026  Elapsed: 0:00:17.371334.\n",
      "  Batch 1,840  of  2,500.  Loss 0.0027  Elapsed: 0:00:17.760797.\n",
      "  Batch 1,880  of  2,500.  Loss 0.0019  Elapsed: 0:00:18.147655.\n",
      "  Batch 1,920  of  2,500.  Loss 0.0022  Elapsed: 0:00:18.533638.\n",
      "  Batch 1,960  of  2,500.  Loss 0.0091  Elapsed: 0:00:18.920117.\n",
      "  Batch 2,000  of  2,500.  Loss 0.0191  Elapsed: 0:00:19.308286.\n",
      "  Batch 2,040  of  2,500.  Loss 0.0027  Elapsed: 0:00:19.695753.\n",
      "  Batch 2,080  of  2,500.  Loss 0.0033  Elapsed: 0:00:20.081590.\n",
      "  Batch 2,120  of  2,500.  Loss 0.0027  Elapsed: 0:00:20.470374.\n",
      "  Batch 2,160  of  2,500.  Loss 0.0042  Elapsed: 0:00:20.860267.\n",
      "  Batch 2,200  of  2,500.  Loss 0.0028  Elapsed: 0:00:21.248010.\n",
      "  Batch 2,240  of  2,500.  Loss 0.0026  Elapsed: 0:00:21.637293.\n",
      "  Batch 2,280  of  2,500.  Loss 0.0023  Elapsed: 0:00:22.026265.\n",
      "  Batch 2,320  of  2,500.  Loss 0.0425  Elapsed: 0:00:22.412988.\n",
      "  Batch 2,360  of  2,500.  Loss 0.0024  Elapsed: 0:00:22.804379.\n",
      "  Batch 2,400  of  2,500.  Loss 0.0112  Elapsed: 0:00:23.190187.\n",
      "  Batch 2,440  of  2,500.  Loss 0.0021  Elapsed: 0:00:23.576484.\n",
      "  Batch 2,480  of  2,500.  Loss 0.0040  Elapsed: 0:00:23.964552.\n",
      "Avg Validation Loss 0.0128, Completed in 0:00:24.148572 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.992358\n",
      "F1 Score (micro) =   0.992388\n",
      "F1 Score (macro) =   0.984520\n",
      "F1 Score (samples) =   0.990400\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.2163  Elapsed: 0:00:00.397849.\n",
      "  Batch    80  of  1,477.  Loss 0.0127  Elapsed: 0:00:00.784739.\n",
      "  Batch   120  of  1,477.  Loss 1.0843  Elapsed: 0:00:01.171686.\n",
      "  Batch   160  of  1,477.  Loss 0.0148  Elapsed: 0:00:01.557886.\n",
      "  Batch   200  of  1,477.  Loss 0.0237  Elapsed: 0:00:01.944326.\n",
      "  Batch   240  of  1,477.  Loss 0.0138  Elapsed: 0:00:02.333002.\n",
      "  Batch   280  of  1,477.  Loss 0.0122  Elapsed: 0:00:02.717961.\n",
      "  Batch   320  of  1,477.  Loss 0.0191  Elapsed: 0:00:03.105360.\n",
      "  Batch   360  of  1,477.  Loss 0.3226  Elapsed: 0:00:03.492125.\n",
      "  Batch   400  of  1,477.  Loss 0.0051  Elapsed: 0:00:03.880995.\n",
      "  Batch   440  of  1,477.  Loss 0.0259  Elapsed: 0:00:04.267082.\n",
      "  Batch   480  of  1,477.  Loss 1.0268  Elapsed: 0:00:04.653286.\n",
      "  Batch   520  of  1,477.  Loss 0.0025  Elapsed: 0:00:05.039529.\n",
      "  Batch   560  of  1,477.  Loss 0.0154  Elapsed: 0:00:05.428030.\n",
      "  Batch   600  of  1,477.  Loss 0.1443  Elapsed: 0:00:05.817010.\n",
      "  Batch   640  of  1,477.  Loss 0.0038  Elapsed: 0:00:06.207150.\n",
      "  Batch   680  of  1,477.  Loss 1.5500  Elapsed: 0:00:06.596678.\n",
      "  Batch   720  of  1,477.  Loss 0.0179  Elapsed: 0:00:06.986336.\n",
      "  Batch   760  of  1,477.  Loss 0.0025  Elapsed: 0:00:07.376851.\n",
      "  Batch   800  of  1,477.  Loss 0.6332  Elapsed: 0:00:07.765500.\n",
      "  Batch   840  of  1,477.  Loss 0.0047  Elapsed: 0:00:08.152924.\n",
      "  Batch   880  of  1,477.  Loss 1.1062  Elapsed: 0:00:08.538595.\n",
      "  Batch   920  of  1,477.  Loss 0.0025  Elapsed: 0:00:08.928321.\n",
      "  Batch   960  of  1,477.  Loss 1.4431  Elapsed: 0:00:09.312718.\n",
      "  Batch 1,000  of  1,477.  Loss 0.3243  Elapsed: 0:00:09.700186.\n",
      "  Batch 1,040  of  1,477.  Loss 0.1301  Elapsed: 0:00:10.086246.\n",
      "  Batch 1,080  of  1,477.  Loss 0.1131  Elapsed: 0:00:10.476832.\n",
      "  Batch 1,120  of  1,477.  Loss 0.2004  Elapsed: 0:00:10.865836.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0065  Elapsed: 0:00:11.253224.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0271  Elapsed: 0:00:11.640375.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0269  Elapsed: 0:00:12.029824.\n",
      "  Batch 1,280  of  1,477.  Loss 1.0049  Elapsed: 0:00:12.419356.\n",
      "  Batch 1,320  of  1,477.  Loss 0.5405  Elapsed: 0:00:12.809521.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0371  Elapsed: 0:00:13.197473.\n",
      "  Batch 1,400  of  1,477.  Loss 0.8141  Elapsed: 0:00:13.582867.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0040  Elapsed: 0:00:13.970069.\n",
      "Avg Validation Loss 0.3219, Completed in 0:00:14.316938 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.637376\n",
      "F1 Score (micro) =   0.645579\n",
      "F1 Score (macro) =   0.467653\n",
      "F1 Score (samples) =   0.606861\n",
      "Running Experiment GEMB_3000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    188.  Loss 0.1505  Elapsed: 0:00:25.101801.\n",
      "Avg Training Loss 0.2490, Completed in 0:00:46.859879 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1557  Elapsed: 0:00:03.731805.\n",
      "  Batch    80  of     93.  Loss 0.2799  Elapsed: 0:00:07.324036.\n",
      "Avg Validation Loss 0.2631, Completed in 0:00:08.345406 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.536806\n",
      "F1 Score (micro) =   0.602667\n",
      "F1 Score (macro) =   0.319522\n",
      "F1 Score (samples) =   0.535094\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    188.  Loss 0.1602  Elapsed: 0:00:25.107186.\n",
      "Avg Training Loss 0.1798, Completed in 0:00:46.864568 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2772  Elapsed: 0:00:03.721188.\n",
      "  Batch    80  of     93.  Loss 0.1790  Elapsed: 0:00:07.324284.\n",
      "Avg Validation Loss 0.2495, Completed in 0:00:08.342251 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.619299\n",
      "F1 Score (micro) =   0.656296\n",
      "F1 Score (macro) =   0.437199\n",
      "F1 Score (samples) =   0.598736\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    188.  Loss 0.1383  Elapsed: 0:00:25.107339.\n",
      "Avg Training Loss 0.0995, Completed in 0:00:46.863697 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1869  Elapsed: 0:00:03.733583.\n",
      "  Batch    80  of     93.  Loss 0.2171  Elapsed: 0:00:07.327918.\n",
      "Avg Validation Loss 0.2983, Completed in 0:00:08.349881 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.646525\n",
      "F1 Score (micro) =   0.674973\n",
      "F1 Score (macro) =   0.481587\n",
      "F1 Score (samples) =   0.640487\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    188.  Loss 0.0411  Elapsed: 0:00:25.110149.\n",
      "Avg Training Loss 0.0496, Completed in 0:00:46.866229 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1144  Elapsed: 0:00:03.734181.\n",
      "  Batch    80  of     93.  Loss 0.3884  Elapsed: 0:00:07.332499.\n",
      "Avg Validation Loss 0.3102, Completed in 0:00:08.354776 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.656911\n",
      "F1 Score (micro) =   0.662181\n",
      "F1 Score (macro) =   0.503855\n",
      "F1 Score (samples) =   0.630332\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  3,000.  Loss 0.0097  Elapsed: 0:00:00.400964.\n",
      "  Batch    80  of  3,000.  Loss 0.0211  Elapsed: 0:00:00.790941.\n",
      "  Batch   120  of  3,000.  Loss 0.0104  Elapsed: 0:00:01.181773.\n",
      "  Batch   160  of  3,000.  Loss 0.0152  Elapsed: 0:00:01.574321.\n",
      "  Batch   200  of  3,000.  Loss 0.0488  Elapsed: 0:00:01.965545.\n",
      "  Batch   240  of  3,000.  Loss 0.0112  Elapsed: 0:00:02.358063.\n",
      "  Batch   280  of  3,000.  Loss 0.0730  Elapsed: 0:00:02.747651.\n",
      "  Batch   320  of  3,000.  Loss 0.0031  Elapsed: 0:00:03.139162.\n",
      "  Batch   360  of  3,000.  Loss 0.0023  Elapsed: 0:00:03.529890.\n",
      "  Batch   400  of  3,000.  Loss 0.0020  Elapsed: 0:00:03.921552.\n",
      "  Batch   440  of  3,000.  Loss 0.0019  Elapsed: 0:00:04.313091.\n",
      "  Batch   480  of  3,000.  Loss 0.0084  Elapsed: 0:00:04.704319.\n",
      "  Batch   520  of  3,000.  Loss 0.0047  Elapsed: 0:00:05.096720.\n",
      "  Batch   560  of  3,000.  Loss 0.0026  Elapsed: 0:00:05.489149.\n",
      "  Batch   600  of  3,000.  Loss 0.0108  Elapsed: 0:00:05.879433.\n",
      "  Batch   640  of  3,000.  Loss 0.0027  Elapsed: 0:00:06.271117.\n",
      "  Batch   680  of  3,000.  Loss 0.0021  Elapsed: 0:00:06.665712.\n",
      "  Batch   720  of  3,000.  Loss 0.0284  Elapsed: 0:00:07.055627.\n",
      "  Batch   760  of  3,000.  Loss 0.0023  Elapsed: 0:00:07.446801.\n",
      "  Batch   800  of  3,000.  Loss 0.0019  Elapsed: 0:00:07.835402.\n",
      "  Batch   840  of  3,000.  Loss 0.0019  Elapsed: 0:00:08.224992.\n",
      "  Batch   880  of  3,000.  Loss 0.0024  Elapsed: 0:00:08.617721.\n",
      "  Batch   920  of  3,000.  Loss 0.0102  Elapsed: 0:00:09.007428.\n",
      "  Batch   960  of  3,000.  Loss 0.0020  Elapsed: 0:00:09.398235.\n",
      "  Batch 1,000  of  3,000.  Loss 0.0022  Elapsed: 0:00:09.791573.\n",
      "  Batch 1,040  of  3,000.  Loss 0.0030  Elapsed: 0:00:10.182631.\n",
      "  Batch 1,080  of  3,000.  Loss 0.0022  Elapsed: 0:00:10.583522.\n",
      "  Batch 1,120  of  3,000.  Loss 0.0023  Elapsed: 0:00:10.975122.\n",
      "  Batch 1,160  of  3,000.  Loss 0.0344  Elapsed: 0:00:11.367854.\n",
      "  Batch 1,200  of  3,000.  Loss 0.0024  Elapsed: 0:00:11.759689.\n",
      "  Batch 1,240  of  3,000.  Loss 0.0034  Elapsed: 0:00:12.154421.\n",
      "  Batch 1,280  of  3,000.  Loss 0.0034  Elapsed: 0:00:12.547732.\n",
      "  Batch 1,320  of  3,000.  Loss 0.0117  Elapsed: 0:00:12.938643.\n",
      "  Batch 1,360  of  3,000.  Loss 0.0093  Elapsed: 0:00:13.330324.\n",
      "  Batch 1,400  of  3,000.  Loss 0.0064  Elapsed: 0:00:13.721935.\n",
      "  Batch 1,440  of  3,000.  Loss 0.0021  Elapsed: 0:00:14.115980.\n",
      "  Batch 1,480  of  3,000.  Loss 0.0025  Elapsed: 0:00:14.506952.\n",
      "  Batch 1,520  of  3,000.  Loss 0.0102  Elapsed: 0:00:14.897962.\n",
      "  Batch 1,560  of  3,000.  Loss 0.0241  Elapsed: 0:00:15.289618.\n",
      "  Batch 1,600  of  3,000.  Loss 0.0147  Elapsed: 0:00:15.680110.\n",
      "  Batch 1,640  of  3,000.  Loss 0.0029  Elapsed: 0:00:16.071855.\n",
      "  Batch 1,680  of  3,000.  Loss 0.0250  Elapsed: 0:00:16.464356.\n",
      "  Batch 1,720  of  3,000.  Loss 0.0078  Elapsed: 0:00:16.854780.\n",
      "  Batch 1,760  of  3,000.  Loss 0.0018  Elapsed: 0:00:17.245378.\n",
      "  Batch 1,800  of  3,000.  Loss 0.0043  Elapsed: 0:00:17.634880.\n",
      "  Batch 1,840  of  3,000.  Loss 0.0391  Elapsed: 0:00:18.027493.\n",
      "  Batch 1,880  of  3,000.  Loss 0.0118  Elapsed: 0:00:18.418588.\n",
      "  Batch 1,920  of  3,000.  Loss 0.0020  Elapsed: 0:00:18.812275.\n",
      "  Batch 1,960  of  3,000.  Loss 0.0139  Elapsed: 0:00:19.203759.\n",
      "  Batch 2,000  of  3,000.  Loss 0.0041  Elapsed: 0:00:19.594956.\n",
      "  Batch 2,040  of  3,000.  Loss 0.0022  Elapsed: 0:00:19.986656.\n",
      "  Batch 2,080  of  3,000.  Loss 0.0031  Elapsed: 0:00:20.377358.\n",
      "  Batch 2,120  of  3,000.  Loss 0.0075  Elapsed: 0:00:20.770249.\n",
      "  Batch 2,160  of  3,000.  Loss 0.0019  Elapsed: 0:00:21.161234.\n",
      "  Batch 2,200  of  3,000.  Loss 0.0024  Elapsed: 0:00:21.553889.\n",
      "  Batch 2,240  of  3,000.  Loss 0.0109  Elapsed: 0:00:21.944911.\n",
      "  Batch 2,280  of  3,000.  Loss 0.0029  Elapsed: 0:00:22.335072.\n",
      "  Batch 2,320  of  3,000.  Loss 0.0027  Elapsed: 0:00:22.726550.\n",
      "  Batch 2,360  of  3,000.  Loss 0.0187  Elapsed: 0:00:23.116649.\n",
      "  Batch 2,400  of  3,000.  Loss 0.0023  Elapsed: 0:00:23.506994.\n",
      "  Batch 2,440  of  3,000.  Loss 0.0034  Elapsed: 0:00:23.897089.\n",
      "  Batch 2,480  of  3,000.  Loss 0.0019  Elapsed: 0:00:24.286947.\n",
      "  Batch 2,520  of  3,000.  Loss 0.0020  Elapsed: 0:00:24.678831.\n",
      "  Batch 2,560  of  3,000.  Loss 0.2922  Elapsed: 0:00:25.069182.\n",
      "  Batch 2,600  of  3,000.  Loss 0.0030  Elapsed: 0:00:25.459417.\n",
      "  Batch 2,640  of  3,000.  Loss 0.0506  Elapsed: 0:00:25.852955.\n",
      "  Batch 2,680  of  3,000.  Loss 0.0029  Elapsed: 0:00:26.242913.\n",
      "  Batch 2,720  of  3,000.  Loss 0.0038  Elapsed: 0:00:26.633000.\n",
      "  Batch 2,760  of  3,000.  Loss 0.0211  Elapsed: 0:00:27.024209.\n",
      "  Batch 2,800  of  3,000.  Loss 0.0040  Elapsed: 0:00:27.415624.\n",
      "  Batch 2,840  of  3,000.  Loss 0.0024  Elapsed: 0:00:27.808766.\n",
      "  Batch 2,880  of  3,000.  Loss 0.0041  Elapsed: 0:00:28.200449.\n",
      "  Batch 2,920  of  3,000.  Loss 0.0018  Elapsed: 0:00:28.589055.\n",
      "  Batch 2,960  of  3,000.  Loss 0.0364  Elapsed: 0:00:28.979711.\n",
      "Avg Validation Loss 0.0164, Completed in 0:00:29.361596 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.987890\n",
      "F1 Score (micro) =   0.987964\n",
      "F1 Score (macro) =   0.972875\n",
      "F1 Score (samples) =   0.984778\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0130  Elapsed: 0:00:00.403718.\n",
      "  Batch    80  of  1,477.  Loss 0.1617  Elapsed: 0:00:00.795343.\n",
      "  Batch   120  of  1,477.  Loss 1.4011  Elapsed: 0:00:01.186335.\n",
      "  Batch   160  of  1,477.  Loss 0.0092  Elapsed: 0:00:01.577427.\n",
      "  Batch   200  of  1,477.  Loss 0.0089  Elapsed: 0:00:01.968303.\n",
      "  Batch   240  of  1,477.  Loss 0.6052  Elapsed: 0:00:02.358956.\n",
      "  Batch   280  of  1,477.  Loss 1.0018  Elapsed: 0:00:02.749669.\n",
      "  Batch   320  of  1,477.  Loss 0.0334  Elapsed: 0:00:03.139361.\n",
      "  Batch   360  of  1,477.  Loss 1.5086  Elapsed: 0:00:03.529812.\n",
      "  Batch   400  of  1,477.  Loss 0.1090  Elapsed: 0:00:03.919807.\n",
      "  Batch   440  of  1,477.  Loss 0.0022  Elapsed: 0:00:04.311525.\n",
      "  Batch   480  of  1,477.  Loss 0.0273  Elapsed: 0:00:04.703053.\n",
      "  Batch   520  of  1,477.  Loss 0.0029  Elapsed: 0:00:05.095936.\n",
      "  Batch   560  of  1,477.  Loss 0.0023  Elapsed: 0:00:05.487386.\n",
      "  Batch   600  of  1,477.  Loss 0.0045  Elapsed: 0:00:05.880035.\n",
      "  Batch   640  of  1,477.  Loss 0.0032  Elapsed: 0:00:06.272485.\n",
      "  Batch   680  of  1,477.  Loss 0.0023  Elapsed: 0:00:06.662828.\n",
      "  Batch   720  of  1,477.  Loss 0.9571  Elapsed: 0:00:07.052495.\n",
      "  Batch   760  of  1,477.  Loss 0.1591  Elapsed: 0:00:07.441861.\n",
      "  Batch   800  of  1,477.  Loss 0.0159  Elapsed: 0:00:07.836559.\n",
      "  Batch   840  of  1,477.  Loss 0.0026  Elapsed: 0:00:08.230289.\n",
      "  Batch   880  of  1,477.  Loss 0.0019  Elapsed: 0:00:08.618277.\n",
      "  Batch   920  of  1,477.  Loss 0.0108  Elapsed: 0:00:09.010390.\n",
      "  Batch   960  of  1,477.  Loss 0.0121  Elapsed: 0:00:09.401773.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0051  Elapsed: 0:00:09.791787.\n",
      "  Batch 1,040  of  1,477.  Loss 0.8720  Elapsed: 0:00:10.182509.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0019  Elapsed: 0:00:10.574909.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0451  Elapsed: 0:00:10.967050.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0026  Elapsed: 0:00:11.358716.\n",
      "  Batch 1,200  of  1,477.  Loss 0.1454  Elapsed: 0:00:11.752852.\n",
      "  Batch 1,240  of  1,477.  Loss 1.3613  Elapsed: 0:00:12.144429.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0137  Elapsed: 0:00:12.535404.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0617  Elapsed: 0:00:12.927592.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0841  Elapsed: 0:00:13.319022.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0468  Elapsed: 0:00:13.710744.\n",
      "  Batch 1,440  of  1,477.  Loss 0.4566  Elapsed: 0:00:14.104032.\n",
      "Avg Validation Loss 0.2977, Completed in 0:00:14.455881 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.674577\n",
      "F1 Score (micro) =   0.675828\n",
      "F1 Score (macro) =   0.494490\n",
      "F1 Score (samples) =   0.645904\n",
      "Running Experiment GEMB_3500\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    219.  Loss 0.2931  Elapsed: 0:00:25.104802.\n",
      "Epoch  0  Batch   200  of    219.  Loss 0.4096  Elapsed: 0:00:50.109294.\n",
      "Avg Training Loss 0.2420, Completed in 0:00:54.650047 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2326  Elapsed: 0:00:03.740735.\n",
      "  Batch    80  of     93.  Loss 0.2788  Elapsed: 0:00:07.337726.\n",
      "Avg Validation Loss 0.2599, Completed in 0:00:08.365633 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.533197\n",
      "F1 Score (micro) =   0.607061\n",
      "F1 Score (macro) =   0.305089\n",
      "F1 Score (samples) =   0.534642\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    219.  Loss 0.1628  Elapsed: 0:00:25.103025.\n",
      "Epoch  1  Batch   200  of    219.  Loss 0.0615  Elapsed: 0:00:50.107091.\n",
      "Avg Training Loss 0.1732, Completed in 0:00:54.647164 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2078  Elapsed: 0:00:03.738702.\n",
      "  Batch    80  of     93.  Loss 0.1192  Elapsed: 0:00:07.333377.\n",
      "Avg Validation Loss 0.2372, Completed in 0:00:08.355426 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.627587\n",
      "F1 Score (micro) =   0.659451\n",
      "F1 Score (macro) =   0.439512\n",
      "F1 Score (samples) =   0.577071\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    219.  Loss 0.0742  Elapsed: 0:00:25.109499.\n",
      "Epoch  2  Batch   200  of    219.  Loss 0.0859  Elapsed: 0:00:50.119589.\n",
      "Avg Training Loss 0.0938, Completed in 0:00:54.661435 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1465  Elapsed: 0:00:03.746322.\n",
      "  Batch    80  of     93.  Loss 0.2068  Elapsed: 0:00:07.342972.\n",
      "Avg Validation Loss 0.2579, Completed in 0:00:08.365482 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.667838\n",
      "F1 Score (micro) =   0.676428\n",
      "F1 Score (macro) =   0.526688\n",
      "F1 Score (samples) =   0.624690\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    219.  Loss 0.0379  Elapsed: 0:00:25.104851.\n",
      "Epoch  3  Batch   200  of    219.  Loss 0.0065  Elapsed: 0:00:50.112117.\n",
      "Avg Training Loss 0.0481, Completed in 0:00:54.653307 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2312  Elapsed: 0:00:03.747256.\n",
      "  Batch    80  of     93.  Loss 0.2646  Elapsed: 0:00:07.348180.\n",
      "Avg Validation Loss 0.3039, Completed in 0:00:08.372285 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.686304\n",
      "F1 Score (micro) =   0.697593\n",
      "F1 Score (macro) =   0.540348\n",
      "F1 Score (samples) =   0.673663\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  3,500.  Loss 0.0097  Elapsed: 0:00:00.401509.\n",
      "  Batch    80  of  3,500.  Loss 0.0022  Elapsed: 0:00:00.793148.\n",
      "  Batch   120  of  3,500.  Loss 0.0071  Elapsed: 0:00:01.182488.\n",
      "  Batch   160  of  3,500.  Loss 0.0018  Elapsed: 0:00:01.574777.\n",
      "  Batch   200  of  3,500.  Loss 0.0025  Elapsed: 0:00:01.967826.\n",
      "  Batch   240  of  3,500.  Loss 0.0020  Elapsed: 0:00:02.359269.\n",
      "  Batch   280  of  3,500.  Loss 0.0025  Elapsed: 0:00:02.748623.\n",
      "  Batch   320  of  3,500.  Loss 0.0102  Elapsed: 0:00:03.141338.\n",
      "  Batch   360  of  3,500.  Loss 0.0019  Elapsed: 0:00:03.534570.\n",
      "  Batch   400  of  3,500.  Loss 0.0088  Elapsed: 0:00:03.923965.\n",
      "  Batch   440  of  3,500.  Loss 0.0018  Elapsed: 0:00:04.315036.\n",
      "  Batch   480  of  3,500.  Loss 0.0026  Elapsed: 0:00:04.705957.\n",
      "  Batch   520  of  3,500.  Loss 0.0019  Elapsed: 0:00:05.099313.\n",
      "  Batch   560  of  3,500.  Loss 0.0029  Elapsed: 0:00:05.490398.\n",
      "  Batch   600  of  3,500.  Loss 0.0029  Elapsed: 0:00:05.879877.\n",
      "  Batch   640  of  3,500.  Loss 0.0086  Elapsed: 0:00:06.271073.\n",
      "  Batch   680  of  3,500.  Loss 0.0074  Elapsed: 0:00:06.662319.\n",
      "  Batch   720  of  3,500.  Loss 0.1053  Elapsed: 0:00:07.053777.\n",
      "  Batch   760  of  3,500.  Loss 0.0025  Elapsed: 0:00:07.444589.\n",
      "  Batch   800  of  3,500.  Loss 0.0018  Elapsed: 0:00:07.833658.\n",
      "  Batch   840  of  3,500.  Loss 0.0018  Elapsed: 0:00:08.223507.\n",
      "  Batch   880  of  3,500.  Loss 0.0022  Elapsed: 0:00:08.613765.\n",
      "  Batch   920  of  3,500.  Loss 0.0023  Elapsed: 0:00:09.005024.\n",
      "  Batch   960  of  3,500.  Loss 0.0018  Elapsed: 0:00:09.396298.\n",
      "  Batch 1,000  of  3,500.  Loss 0.0087  Elapsed: 0:00:09.790121.\n",
      "  Batch 1,040  of  3,500.  Loss 0.1193  Elapsed: 0:00:10.179747.\n",
      "  Batch 1,080  of  3,500.  Loss 0.0142  Elapsed: 0:00:10.569646.\n",
      "  Batch 1,120  of  3,500.  Loss 0.0018  Elapsed: 0:00:10.961161.\n",
      "  Batch 1,160  of  3,500.  Loss 0.0017  Elapsed: 0:00:11.351031.\n",
      "  Batch 1,200  of  3,500.  Loss 0.0045  Elapsed: 0:00:11.742124.\n",
      "  Batch 1,240  of  3,500.  Loss 0.0535  Elapsed: 0:00:12.132349.\n",
      "  Batch 1,280  of  3,500.  Loss 0.0020  Elapsed: 0:00:12.523553.\n",
      "  Batch 1,320  of  3,500.  Loss 0.0019  Elapsed: 0:00:12.915491.\n",
      "  Batch 1,360  of  3,500.  Loss 0.0160  Elapsed: 0:00:13.307334.\n",
      "  Batch 1,400  of  3,500.  Loss 0.0023  Elapsed: 0:00:13.699844.\n",
      "  Batch 1,440  of  3,500.  Loss 0.0198  Elapsed: 0:00:14.090558.\n",
      "  Batch 1,480  of  3,500.  Loss 0.0018  Elapsed: 0:00:14.480258.\n",
      "  Batch 1,520  of  3,500.  Loss 0.0017  Elapsed: 0:00:14.872481.\n",
      "  Batch 1,560  of  3,500.  Loss 0.0092  Elapsed: 0:00:15.264347.\n",
      "  Batch 1,600  of  3,500.  Loss 0.0107  Elapsed: 0:00:15.655749.\n",
      "  Batch 1,640  of  3,500.  Loss 0.0019  Elapsed: 0:00:16.048434.\n",
      "  Batch 1,680  of  3,500.  Loss 0.0023  Elapsed: 0:00:16.441748.\n",
      "  Batch 1,720  of  3,500.  Loss 0.0102  Elapsed: 0:00:16.831941.\n",
      "  Batch 1,760  of  3,500.  Loss 0.0060  Elapsed: 0:00:17.222437.\n",
      "  Batch 1,800  of  3,500.  Loss 0.0032  Elapsed: 0:00:17.615379.\n",
      "  Batch 1,840  of  3,500.  Loss 0.0042  Elapsed: 0:00:18.006398.\n",
      "  Batch 1,880  of  3,500.  Loss 0.0021  Elapsed: 0:00:18.397696.\n",
      "  Batch 1,920  of  3,500.  Loss 0.0030  Elapsed: 0:00:18.788223.\n",
      "  Batch 1,960  of  3,500.  Loss 0.0020  Elapsed: 0:00:19.181538.\n",
      "  Batch 2,000  of  3,500.  Loss 0.0020  Elapsed: 0:00:19.575209.\n",
      "  Batch 2,040  of  3,500.  Loss 0.0168  Elapsed: 0:00:19.967585.\n",
      "  Batch 2,080  of  3,500.  Loss 0.0085  Elapsed: 0:00:20.356861.\n",
      "  Batch 2,120  of  3,500.  Loss 0.0030  Elapsed: 0:00:20.750398.\n",
      "  Batch 2,160  of  3,500.  Loss 0.0045  Elapsed: 0:00:21.139263.\n",
      "  Batch 2,200  of  3,500.  Loss 0.0030  Elapsed: 0:00:21.532812.\n",
      "  Batch 2,240  of  3,500.  Loss 0.0020  Elapsed: 0:00:21.924278.\n",
      "  Batch 2,280  of  3,500.  Loss 0.0030  Elapsed: 0:00:22.316167.\n",
      "  Batch 2,320  of  3,500.  Loss 0.0114  Elapsed: 0:00:22.705949.\n",
      "  Batch 2,360  of  3,500.  Loss 0.0272  Elapsed: 0:00:23.096239.\n",
      "  Batch 2,400  of  3,500.  Loss 0.0023  Elapsed: 0:00:23.486224.\n",
      "  Batch 2,440  of  3,500.  Loss 0.0021  Elapsed: 0:00:23.878567.\n",
      "  Batch 2,480  of  3,500.  Loss 0.0019  Elapsed: 0:00:24.273087.\n",
      "  Batch 2,520  of  3,500.  Loss 0.0042  Elapsed: 0:00:24.663597.\n",
      "  Batch 2,560  of  3,500.  Loss 0.0115  Elapsed: 0:00:25.056098.\n",
      "  Batch 2,600  of  3,500.  Loss 0.0029  Elapsed: 0:00:25.445584.\n",
      "  Batch 2,640  of  3,500.  Loss 0.0021  Elapsed: 0:00:25.836533.\n",
      "  Batch 2,680  of  3,500.  Loss 0.0020  Elapsed: 0:00:26.226943.\n",
      "  Batch 2,720  of  3,500.  Loss 0.0107  Elapsed: 0:00:26.617225.\n",
      "  Batch 2,760  of  3,500.  Loss 0.0029  Elapsed: 0:00:27.008669.\n",
      "  Batch 2,800  of  3,500.  Loss 0.0460  Elapsed: 0:00:27.401191.\n",
      "  Batch 2,840  of  3,500.  Loss 0.0093  Elapsed: 0:00:27.794801.\n",
      "  Batch 2,880  of  3,500.  Loss 0.0112  Elapsed: 0:00:28.187271.\n",
      "  Batch 2,920  of  3,500.  Loss 0.0094  Elapsed: 0:00:28.578147.\n",
      "  Batch 2,960  of  3,500.  Loss 0.0021  Elapsed: 0:00:28.969022.\n",
      "  Batch 3,000  of  3,500.  Loss 0.0101  Elapsed: 0:00:29.360437.\n",
      "  Batch 3,040  of  3,500.  Loss 0.0121  Elapsed: 0:00:29.752752.\n",
      "  Batch 3,080  of  3,500.  Loss 0.0046  Elapsed: 0:00:30.141876.\n",
      "  Batch 3,120  of  3,500.  Loss 0.0050  Elapsed: 0:00:30.532164.\n",
      "  Batch 3,160  of  3,500.  Loss 0.0164  Elapsed: 0:00:30.923706.\n",
      "  Batch 3,200  of  3,500.  Loss 0.0042  Elapsed: 0:00:31.312966.\n",
      "  Batch 3,240  of  3,500.  Loss 0.0095  Elapsed: 0:00:31.706244.\n",
      "  Batch 3,280  of  3,500.  Loss 0.0018  Elapsed: 0:00:32.097311.\n",
      "  Batch 3,320  of  3,500.  Loss 0.0038  Elapsed: 0:00:32.489150.\n",
      "  Batch 3,360  of  3,500.  Loss 0.0112  Elapsed: 0:00:32.881957.\n",
      "  Batch 3,400  of  3,500.  Loss 0.4478  Elapsed: 0:00:33.272955.\n",
      "  Batch 3,440  of  3,500.  Loss 0.0020  Elapsed: 0:00:33.664561.\n",
      "  Batch 3,480  of  3,500.  Loss 0.0095  Elapsed: 0:00:34.056954.\n",
      "Avg Validation Loss 0.0131, Completed in 0:00:34.243675 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.990007\n",
      "F1 Score (micro) =   0.989997\n",
      "F1 Score (macro) =   0.978916\n",
      "F1 Score (samples) =   0.988381\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 1.6935  Elapsed: 0:00:00.403578.\n",
      "  Batch    80  of  1,477.  Loss 0.0018  Elapsed: 0:00:00.795283.\n",
      "  Batch   120  of  1,477.  Loss 0.0102  Elapsed: 0:00:01.187979.\n",
      "  Batch   160  of  1,477.  Loss 0.0029  Elapsed: 0:00:01.578556.\n",
      "  Batch   200  of  1,477.  Loss 0.0387  Elapsed: 0:00:01.968734.\n",
      "  Batch   240  of  1,477.  Loss 0.6355  Elapsed: 0:00:02.358780.\n",
      "  Batch   280  of  1,477.  Loss 0.0876  Elapsed: 0:00:02.750872.\n",
      "  Batch   320  of  1,477.  Loss 1.2633  Elapsed: 0:00:03.144829.\n",
      "  Batch   360  of  1,477.  Loss 0.0020  Elapsed: 0:00:03.537103.\n",
      "  Batch   400  of  1,477.  Loss 0.1742  Elapsed: 0:00:03.926899.\n",
      "  Batch   440  of  1,477.  Loss 0.5150  Elapsed: 0:00:04.321214.\n",
      "  Batch   480  of  1,477.  Loss 1.2045  Elapsed: 0:00:04.711417.\n",
      "  Batch   520  of  1,477.  Loss 0.0105  Elapsed: 0:00:05.103341.\n",
      "  Batch   560  of  1,477.  Loss 0.6542  Elapsed: 0:00:05.493787.\n",
      "  Batch   600  of  1,477.  Loss 0.0061  Elapsed: 0:00:05.885547.\n",
      "  Batch   640  of  1,477.  Loss 0.1468  Elapsed: 0:00:06.275836.\n",
      "  Batch   680  of  1,477.  Loss 0.0047  Elapsed: 0:00:06.665473.\n",
      "  Batch   720  of  1,477.  Loss 0.0257  Elapsed: 0:00:07.057901.\n",
      "  Batch   760  of  1,477.  Loss 0.0026  Elapsed: 0:00:07.447896.\n",
      "  Batch   800  of  1,477.  Loss 0.0108  Elapsed: 0:00:07.840488.\n",
      "  Batch   840  of  1,477.  Loss 0.0021  Elapsed: 0:00:08.233609.\n",
      "  Batch   880  of  1,477.  Loss 0.0120  Elapsed: 0:00:08.627230.\n",
      "  Batch   920  of  1,477.  Loss 0.0018  Elapsed: 0:00:09.016104.\n",
      "  Batch   960  of  1,477.  Loss 1.6162  Elapsed: 0:00:09.408524.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0150  Elapsed: 0:00:09.799886.\n",
      "  Batch 1,040  of  1,477.  Loss 0.6327  Elapsed: 0:00:10.190110.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0017  Elapsed: 0:00:10.580137.\n",
      "  Batch 1,120  of  1,477.  Loss 0.7812  Elapsed: 0:00:10.969904.\n",
      "  Batch 1,160  of  1,477.  Loss 1.5339  Elapsed: 0:00:11.360603.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0100  Elapsed: 0:00:11.752144.\n",
      "  Batch 1,240  of  1,477.  Loss 1.4052  Elapsed: 0:00:12.142501.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0227  Elapsed: 0:00:12.532234.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0425  Elapsed: 0:00:12.921922.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0394  Elapsed: 0:00:13.314093.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0022  Elapsed: 0:00:13.704646.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0101  Elapsed: 0:00:14.095521.\n",
      "Avg Validation Loss 0.3062, Completed in 0:00:14.448848 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.682375\n",
      "F1 Score (micro) =   0.692924\n",
      "F1 Score (macro) =   0.505413\n",
      "F1 Score (samples) =   0.669601\n",
      "Running Experiment GEMB_4000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    250.  Loss 0.2345  Elapsed: 0:00:25.106416.\n",
      "Epoch  0  Batch   200  of    250.  Loss 0.2977  Elapsed: 0:00:50.114050.\n",
      "Avg Training Loss 0.2437, Completed in 0:01:02.448828 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1927  Elapsed: 0:00:03.757724.\n",
      "  Batch    80  of     93.  Loss 0.2563  Elapsed: 0:00:07.355214.\n",
      "Avg Validation Loss 0.2593, Completed in 0:00:08.376075 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.557380\n",
      "F1 Score (micro) =   0.622189\n",
      "F1 Score (macro) =   0.349249\n",
      "F1 Score (samples) =   0.569849\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    250.  Loss 0.2457  Elapsed: 0:00:25.105098.\n",
      "Epoch  1  Batch   200  of    250.  Loss 0.1545  Elapsed: 0:00:50.114515.\n",
      "Avg Training Loss 0.1738, Completed in 0:01:02.448293 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.4008  Elapsed: 0:00:03.760720.\n",
      "  Batch    80  of     93.  Loss 0.3050  Elapsed: 0:00:07.358039.\n",
      "Avg Validation Loss 0.2466, Completed in 0:00:08.378830 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.623329\n",
      "F1 Score (micro) =   0.662550\n",
      "F1 Score (macro) =   0.440242\n",
      "F1 Score (samples) =   0.616339\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    250.  Loss 0.2053  Elapsed: 0:00:25.109342.\n",
      "Epoch  2  Batch   200  of    250.  Loss 0.0532  Elapsed: 0:00:50.116753.\n",
      "Avg Training Loss 0.0967, Completed in 0:01:02.451618 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.5568  Elapsed: 0:00:03.759621.\n",
      "  Batch    80  of     93.  Loss 0.1039  Elapsed: 0:00:07.355765.\n",
      "Avg Validation Loss 0.2696, Completed in 0:00:08.376257 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.670498\n",
      "F1 Score (micro) =   0.676429\n",
      "F1 Score (macro) =   0.538538\n",
      "F1 Score (samples) =   0.640262\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    250.  Loss 0.0389  Elapsed: 0:00:25.105408.\n",
      "Epoch  3  Batch   200  of    250.  Loss 0.0149  Elapsed: 0:00:50.114009.\n",
      "Avg Training Loss 0.0475, Completed in 0:01:02.449153 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3781  Elapsed: 0:00:03.761859.\n",
      "  Batch    80  of     93.  Loss 0.4199  Elapsed: 0:00:07.356526.\n",
      "Avg Validation Loss 0.3057, Completed in 0:00:08.376486 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.653039\n",
      "F1 Score (micro) =   0.647977\n",
      "F1 Score (macro) =   0.517377\n",
      "F1 Score (samples) =   0.615211\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  4,000.  Loss 0.0018  Elapsed: 0:00:00.405693.\n",
      "  Batch    80  of  4,000.  Loss 0.0091  Elapsed: 0:00:00.794074.\n",
      "  Batch   120  of  4,000.  Loss 0.0498  Elapsed: 0:00:01.179488.\n",
      "  Batch   160  of  4,000.  Loss 0.0025  Elapsed: 0:00:01.567828.\n",
      "  Batch   200  of  4,000.  Loss 0.0021  Elapsed: 0:00:01.954262.\n",
      "  Batch   240  of  4,000.  Loss 0.0019  Elapsed: 0:00:02.342182.\n",
      "  Batch   280  of  4,000.  Loss 0.0187  Elapsed: 0:00:02.729964.\n",
      "  Batch   320  of  4,000.  Loss 0.0018  Elapsed: 0:00:03.119109.\n",
      "  Batch   360  of  4,000.  Loss 0.0454  Elapsed: 0:00:03.507521.\n",
      "  Batch   400  of  4,000.  Loss 0.0017  Elapsed: 0:00:03.895496.\n",
      "  Batch   440  of  4,000.  Loss 0.0053  Elapsed: 0:00:04.280581.\n",
      "  Batch   480  of  4,000.  Loss 0.0028  Elapsed: 0:00:04.667398.\n",
      "  Batch   520  of  4,000.  Loss 0.0019  Elapsed: 0:00:05.056183.\n",
      "  Batch   560  of  4,000.  Loss 0.0814  Elapsed: 0:00:05.446524.\n",
      "  Batch   600  of  4,000.  Loss 0.0018  Elapsed: 0:00:05.833377.\n",
      "  Batch   640  of  4,000.  Loss 0.0043  Elapsed: 0:00:06.220484.\n",
      "  Batch   680  of  4,000.  Loss 0.0126  Elapsed: 0:00:06.607877.\n",
      "  Batch   720  of  4,000.  Loss 0.0075  Elapsed: 0:00:06.995145.\n",
      "  Batch   760  of  4,000.  Loss 0.0108  Elapsed: 0:00:07.381705.\n",
      "  Batch   800  of  4,000.  Loss 0.0734  Elapsed: 0:00:07.768981.\n",
      "  Batch   840  of  4,000.  Loss 0.0277  Elapsed: 0:00:08.156865.\n",
      "  Batch   880  of  4,000.  Loss 0.0466  Elapsed: 0:00:08.544419.\n",
      "  Batch   920  of  4,000.  Loss 0.0020  Elapsed: 0:00:08.934299.\n",
      "  Batch   960  of  4,000.  Loss 0.0058  Elapsed: 0:00:09.320909.\n",
      "  Batch 1,000  of  4,000.  Loss 0.0159  Elapsed: 0:00:09.707415.\n",
      "  Batch 1,040  of  4,000.  Loss 0.0050  Elapsed: 0:00:10.096709.\n",
      "  Batch 1,080  of  4,000.  Loss 0.0590  Elapsed: 0:00:10.486657.\n",
      "  Batch 1,120  of  4,000.  Loss 0.0379  Elapsed: 0:00:10.874756.\n",
      "  Batch 1,160  of  4,000.  Loss 0.0117  Elapsed: 0:00:11.260910.\n",
      "  Batch 1,200  of  4,000.  Loss 0.0084  Elapsed: 0:00:11.649483.\n",
      "  Batch 1,240  of  4,000.  Loss 0.0065  Elapsed: 0:00:12.035816.\n",
      "  Batch 1,280  of  4,000.  Loss 0.0037  Elapsed: 0:00:12.424540.\n",
      "  Batch 1,320  of  4,000.  Loss 0.0029  Elapsed: 0:00:12.813090.\n",
      "  Batch 1,360  of  4,000.  Loss 0.0022  Elapsed: 0:00:13.201919.\n",
      "  Batch 1,400  of  4,000.  Loss 0.0098  Elapsed: 0:00:13.589540.\n",
      "  Batch 1,440  of  4,000.  Loss 0.0018  Elapsed: 0:00:13.977951.\n",
      "  Batch 1,480  of  4,000.  Loss 0.0042  Elapsed: 0:00:14.366731.\n",
      "  Batch 1,520  of  4,000.  Loss 0.0308  Elapsed: 0:00:14.755112.\n",
      "  Batch 1,560  of  4,000.  Loss 0.0021  Elapsed: 0:00:15.142449.\n",
      "  Batch 1,600  of  4,000.  Loss 0.0027  Elapsed: 0:00:15.531633.\n",
      "  Batch 1,640  of  4,000.  Loss 0.0122  Elapsed: 0:00:15.920075.\n",
      "  Batch 1,680  of  4,000.  Loss 0.0086  Elapsed: 0:00:16.309995.\n",
      "  Batch 1,720  of  4,000.  Loss 0.0017  Elapsed: 0:00:16.698499.\n",
      "  Batch 1,760  of  4,000.  Loss 0.0425  Elapsed: 0:00:17.085424.\n",
      "  Batch 1,800  of  4,000.  Loss 0.0089  Elapsed: 0:00:17.471012.\n",
      "  Batch 1,840  of  4,000.  Loss 0.0092  Elapsed: 0:00:17.860755.\n",
      "  Batch 1,880  of  4,000.  Loss 0.0020  Elapsed: 0:00:18.248824.\n",
      "  Batch 1,920  of  4,000.  Loss 0.0083  Elapsed: 0:00:18.636942.\n",
      "  Batch 1,960  of  4,000.  Loss 0.0019  Elapsed: 0:00:19.025390.\n",
      "  Batch 2,000  of  4,000.  Loss 0.0034  Elapsed: 0:00:19.412440.\n",
      "  Batch 2,040  of  4,000.  Loss 0.0018  Elapsed: 0:00:19.800405.\n",
      "  Batch 2,080  of  4,000.  Loss 0.0072  Elapsed: 0:00:20.189341.\n",
      "  Batch 2,120  of  4,000.  Loss 0.0022  Elapsed: 0:00:20.579843.\n",
      "  Batch 2,160  of  4,000.  Loss 0.0096  Elapsed: 0:00:20.969038.\n",
      "  Batch 2,200  of  4,000.  Loss 0.0032  Elapsed: 0:00:21.356360.\n",
      "  Batch 2,240  of  4,000.  Loss 0.0041  Elapsed: 0:00:21.744227.\n",
      "  Batch 2,280  of  4,000.  Loss 0.0021  Elapsed: 0:00:22.130831.\n",
      "  Batch 2,320  of  4,000.  Loss 0.0049  Elapsed: 0:00:22.516891.\n",
      "  Batch 2,360  of  4,000.  Loss 0.0020  Elapsed: 0:00:22.903571.\n",
      "  Batch 2,400  of  4,000.  Loss 0.0089  Elapsed: 0:00:23.290184.\n",
      "  Batch 2,440  of  4,000.  Loss 0.0156  Elapsed: 0:00:23.677668.\n",
      "  Batch 2,480  of  4,000.  Loss 0.1483  Elapsed: 0:00:24.065334.\n",
      "  Batch 2,520  of  4,000.  Loss 0.0079  Elapsed: 0:00:24.451844.\n",
      "  Batch 2,560  of  4,000.  Loss 0.0092  Elapsed: 0:00:24.838292.\n",
      "  Batch 2,600  of  4,000.  Loss 0.0338  Elapsed: 0:00:25.228689.\n",
      "  Batch 2,640  of  4,000.  Loss 0.0096  Elapsed: 0:00:25.617020.\n",
      "  Batch 2,680  of  4,000.  Loss 0.0129  Elapsed: 0:00:26.008604.\n",
      "  Batch 2,720  of  4,000.  Loss 0.0017  Elapsed: 0:00:26.393905.\n",
      "  Batch 2,760  of  4,000.  Loss 0.0098  Elapsed: 0:00:26.781085.\n",
      "  Batch 2,800  of  4,000.  Loss 0.0077  Elapsed: 0:00:27.167901.\n",
      "  Batch 2,840  of  4,000.  Loss 0.0081  Elapsed: 0:00:27.555676.\n",
      "  Batch 2,880  of  4,000.  Loss 0.0098  Elapsed: 0:00:27.944614.\n",
      "  Batch 2,920  of  4,000.  Loss 0.0018  Elapsed: 0:00:28.331644.\n",
      "  Batch 2,960  of  4,000.  Loss 0.3051  Elapsed: 0:00:28.718484.\n",
      "  Batch 3,000  of  4,000.  Loss 0.0103  Elapsed: 0:00:29.106426.\n",
      "  Batch 3,040  of  4,000.  Loss 0.0021  Elapsed: 0:00:29.492248.\n",
      "  Batch 3,080  of  4,000.  Loss 0.0130  Elapsed: 0:00:29.879201.\n",
      "  Batch 3,120  of  4,000.  Loss 0.0035  Elapsed: 0:00:30.267342.\n",
      "  Batch 3,160  of  4,000.  Loss 0.0081  Elapsed: 0:00:30.651910.\n",
      "  Batch 3,200  of  4,000.  Loss 0.0022  Elapsed: 0:00:31.038960.\n",
      "  Batch 3,240  of  4,000.  Loss 0.0362  Elapsed: 0:00:31.427146.\n",
      "  Batch 3,280  of  4,000.  Loss 0.0102  Elapsed: 0:00:31.816646.\n",
      "  Batch 3,320  of  4,000.  Loss 0.0113  Elapsed: 0:00:32.205895.\n",
      "  Batch 3,360  of  4,000.  Loss 0.0021  Elapsed: 0:00:32.594760.\n",
      "  Batch 3,400  of  4,000.  Loss 0.0019  Elapsed: 0:00:32.982892.\n",
      "  Batch 3,440  of  4,000.  Loss 0.3454  Elapsed: 0:00:33.371632.\n",
      "  Batch 3,480  of  4,000.  Loss 0.0018  Elapsed: 0:00:33.756593.\n",
      "  Batch 3,520  of  4,000.  Loss 0.0021  Elapsed: 0:00:34.143053.\n",
      "  Batch 3,560  of  4,000.  Loss 0.0024  Elapsed: 0:00:34.532173.\n",
      "  Batch 3,600  of  4,000.  Loss 0.0028  Elapsed: 0:00:34.921638.\n",
      "  Batch 3,640  of  4,000.  Loss 0.0076  Elapsed: 0:00:35.309057.\n",
      "  Batch 3,680  of  4,000.  Loss 0.0019  Elapsed: 0:00:35.697909.\n",
      "  Batch 3,720  of  4,000.  Loss 0.0249  Elapsed: 0:00:36.085372.\n",
      "  Batch 3,760  of  4,000.  Loss 0.0019  Elapsed: 0:00:36.472646.\n",
      "  Batch 3,800  of  4,000.  Loss 0.0017  Elapsed: 0:00:36.861535.\n",
      "  Batch 3,840  of  4,000.  Loss 0.0040  Elapsed: 0:00:37.251225.\n",
      "  Batch 3,880  of  4,000.  Loss 0.0196  Elapsed: 0:00:37.640642.\n",
      "  Batch 3,920  of  4,000.  Loss 0.0022  Elapsed: 0:00:38.030277.\n",
      "  Batch 3,960  of  4,000.  Loss 0.0019  Elapsed: 0:00:38.417075.\n",
      "Avg Validation Loss 0.0210, Completed in 0:00:38.792413 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.982312\n",
      "F1 Score (micro) =   0.982096\n",
      "F1 Score (macro) =   0.967408\n",
      "F1 Score (samples) =   0.978750\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.1592  Elapsed: 0:00:00.405380.\n",
      "  Batch    80  of  1,477.  Loss 0.0025  Elapsed: 0:00:00.791913.\n",
      "  Batch   120  of  1,477.  Loss 0.0044  Elapsed: 0:00:01.179542.\n",
      "  Batch   160  of  1,477.  Loss 0.0479  Elapsed: 0:00:01.567892.\n",
      "  Batch   200  of  1,477.  Loss 0.0020  Elapsed: 0:00:01.955613.\n",
      "  Batch   240  of  1,477.  Loss 0.0068  Elapsed: 0:00:02.341976.\n",
      "  Batch   280  of  1,477.  Loss 0.0264  Elapsed: 0:00:02.728662.\n",
      "  Batch   320  of  1,477.  Loss 0.0027  Elapsed: 0:00:03.117017.\n",
      "  Batch   360  of  1,477.  Loss 0.2290  Elapsed: 0:00:03.503080.\n",
      "  Batch   400  of  1,477.  Loss 0.0051  Elapsed: 0:00:03.888755.\n",
      "  Batch   440  of  1,477.  Loss 0.0191  Elapsed: 0:00:04.275055.\n",
      "  Batch   480  of  1,477.  Loss 0.0018  Elapsed: 0:00:04.664100.\n",
      "  Batch   520  of  1,477.  Loss 0.8435  Elapsed: 0:00:05.051355.\n",
      "  Batch   560  of  1,477.  Loss 0.0030  Elapsed: 0:00:05.439816.\n",
      "  Batch   600  of  1,477.  Loss 1.0110  Elapsed: 0:00:05.827289.\n",
      "  Batch   640  of  1,477.  Loss 1.6834  Elapsed: 0:00:06.214513.\n",
      "  Batch   680  of  1,477.  Loss 0.0165  Elapsed: 0:00:06.603323.\n",
      "  Batch   720  of  1,477.  Loss 0.0090  Elapsed: 0:00:06.992400.\n",
      "  Batch   760  of  1,477.  Loss 0.9576  Elapsed: 0:00:07.380291.\n",
      "  Batch   800  of  1,477.  Loss 0.8046  Elapsed: 0:00:07.766957.\n",
      "  Batch   840  of  1,477.  Loss 0.4404  Elapsed: 0:00:08.153441.\n",
      "  Batch   880  of  1,477.  Loss 0.0039  Elapsed: 0:00:08.541090.\n",
      "  Batch   920  of  1,477.  Loss 0.1820  Elapsed: 0:00:08.929024.\n",
      "  Batch   960  of  1,477.  Loss 1.4881  Elapsed: 0:00:09.315730.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0019  Elapsed: 0:00:09.703853.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0020  Elapsed: 0:00:10.090824.\n",
      "  Batch 1,080  of  1,477.  Loss 0.1939  Elapsed: 0:00:10.478458.\n",
      "  Batch 1,120  of  1,477.  Loss 1.2335  Elapsed: 0:00:10.865216.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0176  Elapsed: 0:00:11.252868.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0022  Elapsed: 0:00:11.640918.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0019  Elapsed: 0:00:12.028051.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0019  Elapsed: 0:00:12.416777.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0047  Elapsed: 0:00:12.806653.\n",
      "  Batch 1,360  of  1,477.  Loss 0.8521  Elapsed: 0:00:13.195554.\n",
      "  Batch 1,400  of  1,477.  Loss 1.4720  Elapsed: 0:00:13.580558.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0146  Elapsed: 0:00:13.969530.\n",
      "Avg Validation Loss 0.2977, Completed in 0:00:14.317518 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.667725\n",
      "F1 Score (micro) =   0.658632\n",
      "F1 Score (macro) =   0.512895\n",
      "F1 Score (samples) =   0.626495\n",
      "Running Experiment GEMB_4500\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    282.  Loss 0.2645  Elapsed: 0:00:25.108923.\n",
      "Epoch  0  Batch   200  of    282.  Loss 0.3117  Elapsed: 0:00:50.117418.\n",
      "Avg Training Loss 0.2438, Completed in 0:01:10.334957 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2673  Elapsed: 0:00:03.703073.\n",
      "  Batch    80  of     93.  Loss 0.2692  Elapsed: 0:00:07.299352.\n",
      "Avg Validation Loss 0.2555, Completed in 0:00:08.318969 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.564436\n",
      "F1 Score (micro) =   0.620584\n",
      "F1 Score (macro) =   0.350737\n",
      "F1 Score (samples) =   0.546152\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    282.  Loss 0.2497  Elapsed: 0:00:25.104762.\n",
      "Epoch  1  Batch   200  of    282.  Loss 0.2416  Elapsed: 0:00:50.110743.\n",
      "Avg Training Loss 0.1782, Completed in 0:01:10.326827 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2939  Elapsed: 0:00:03.706296.\n",
      "  Batch    80  of     93.  Loss 0.2390  Elapsed: 0:00:07.309377.\n",
      "Avg Validation Loss 0.2276, Completed in 0:00:08.330152 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.650751\n",
      "F1 Score (micro) =   0.682657\n",
      "F1 Score (macro) =   0.460566\n",
      "F1 Score (samples) =   0.624915\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    282.  Loss 0.1200  Elapsed: 0:00:25.103176.\n",
      "Epoch  2  Batch   200  of    282.  Loss 0.1115  Elapsed: 0:00:50.112708.\n",
      "Avg Training Loss 0.1011, Completed in 0:01:10.330335 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2867  Elapsed: 0:00:03.712541.\n",
      "  Batch    80  of     93.  Loss 0.3184  Elapsed: 0:00:07.318468.\n",
      "Avg Validation Loss 0.2440, Completed in 0:00:08.331035 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.679890\n",
      "F1 Score (micro) =   0.699612\n",
      "F1 Score (macro) =   0.564120\n",
      "F1 Score (samples) =   0.670503\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    282.  Loss 0.1336  Elapsed: 0:00:25.101963.\n",
      "Epoch  3  Batch   200  of    282.  Loss 0.0302  Elapsed: 0:00:50.114072.\n",
      "Avg Training Loss 0.0516, Completed in 0:01:10.328528 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2653  Elapsed: 0:00:03.707093.\n",
      "  Batch    80  of     93.  Loss 0.1879  Elapsed: 0:00:07.299907.\n",
      "Avg Validation Loss 0.2960, Completed in 0:00:08.317012 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.703801\n",
      "F1 Score (micro) =   0.714832\n",
      "F1 Score (macro) =   0.580865\n",
      "F1 Score (samples) =   0.695328\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  4,500.  Loss 0.0019  Elapsed: 0:00:00.401445.\n",
      "  Batch    80  of  4,500.  Loss 0.0028  Elapsed: 0:00:00.786865.\n",
      "  Batch   120  of  4,500.  Loss 0.0026  Elapsed: 0:00:01.172670.\n",
      "  Batch   160  of  4,500.  Loss 0.0020  Elapsed: 0:00:01.557865.\n",
      "  Batch   200  of  4,500.  Loss 0.0021  Elapsed: 0:00:01.946220.\n",
      "  Batch   240  of  4,500.  Loss 0.0106  Elapsed: 0:00:02.333371.\n",
      "  Batch   280  of  4,500.  Loss 0.0089  Elapsed: 0:00:02.719553.\n",
      "  Batch   320  of  4,500.  Loss 0.0114  Elapsed: 0:00:03.105645.\n",
      "  Batch   360  of  4,500.  Loss 0.0023  Elapsed: 0:00:03.491114.\n",
      "  Batch   400  of  4,500.  Loss 0.0325  Elapsed: 0:00:03.875226.\n",
      "  Batch   440  of  4,500.  Loss 0.0026  Elapsed: 0:00:04.262018.\n",
      "  Batch   480  of  4,500.  Loss 0.0047  Elapsed: 0:00:04.647837.\n",
      "  Batch   520  of  4,500.  Loss 0.0649  Elapsed: 0:00:05.033084.\n",
      "  Batch   560  of  4,500.  Loss 0.0022  Elapsed: 0:00:05.420652.\n",
      "  Batch   600  of  4,500.  Loss 0.0022  Elapsed: 0:00:05.804837.\n",
      "  Batch   640  of  4,500.  Loss 0.0037  Elapsed: 0:00:06.188732.\n",
      "  Batch   680  of  4,500.  Loss 0.0034  Elapsed: 0:00:06.572806.\n",
      "  Batch   720  of  4,500.  Loss 0.0018  Elapsed: 0:00:06.958647.\n",
      "  Batch   760  of  4,500.  Loss 0.0043  Elapsed: 0:00:07.343996.\n",
      "  Batch   800  of  4,500.  Loss 0.0019  Elapsed: 0:00:07.729399.\n",
      "  Batch   840  of  4,500.  Loss 0.0021  Elapsed: 0:00:08.116332.\n",
      "  Batch   880  of  4,500.  Loss 0.0020  Elapsed: 0:00:08.502993.\n",
      "  Batch   920  of  4,500.  Loss 0.0163  Elapsed: 0:00:08.888522.\n",
      "  Batch   960  of  4,500.  Loss 0.0037  Elapsed: 0:00:09.273749.\n",
      "  Batch 1,000  of  4,500.  Loss 0.0019  Elapsed: 0:00:09.659125.\n",
      "  Batch 1,040  of  4,500.  Loss 0.0038  Elapsed: 0:00:10.047500.\n",
      "  Batch 1,080  of  4,500.  Loss 0.0033  Elapsed: 0:00:10.432070.\n",
      "  Batch 1,120  of  4,500.  Loss 0.0024  Elapsed: 0:00:10.817625.\n",
      "  Batch 1,160  of  4,500.  Loss 0.2231  Elapsed: 0:00:11.202054.\n",
      "  Batch 1,200  of  4,500.  Loss 0.0517  Elapsed: 0:00:11.588400.\n",
      "  Batch 1,240  of  4,500.  Loss 0.0086  Elapsed: 0:00:11.973987.\n",
      "  Batch 1,280  of  4,500.  Loss 0.0037  Elapsed: 0:00:12.360530.\n",
      "  Batch 1,320  of  4,500.  Loss 0.0019  Elapsed: 0:00:12.744813.\n",
      "  Batch 1,360  of  4,500.  Loss 0.0023  Elapsed: 0:00:13.130357.\n",
      "  Batch 1,400  of  4,500.  Loss 0.0022  Elapsed: 0:00:13.516380.\n",
      "  Batch 1,440  of  4,500.  Loss 0.0226  Elapsed: 0:00:13.902587.\n",
      "  Batch 1,480  of  4,500.  Loss 0.0022  Elapsed: 0:00:14.288852.\n",
      "  Batch 1,520  of  4,500.  Loss 0.0017  Elapsed: 0:00:14.675674.\n",
      "  Batch 1,560  of  4,500.  Loss 0.0043  Elapsed: 0:00:15.060277.\n",
      "  Batch 1,600  of  4,500.  Loss 0.0426  Elapsed: 0:00:15.445184.\n",
      "  Batch 1,640  of  4,500.  Loss 0.0108  Elapsed: 0:00:15.834743.\n",
      "  Batch 1,680  of  4,500.  Loss 0.0048  Elapsed: 0:00:16.219335.\n",
      "  Batch 1,720  of  4,500.  Loss 0.0109  Elapsed: 0:00:16.606534.\n",
      "  Batch 1,760  of  4,500.  Loss 0.0025  Elapsed: 0:00:16.991692.\n",
      "  Batch 1,800  of  4,500.  Loss 0.0017  Elapsed: 0:00:17.374447.\n",
      "  Batch 1,840  of  4,500.  Loss 0.3048  Elapsed: 0:00:17.760767.\n",
      "  Batch 1,880  of  4,500.  Loss 0.0082  Elapsed: 0:00:18.146481.\n",
      "  Batch 1,920  of  4,500.  Loss 0.0020  Elapsed: 0:00:18.530312.\n",
      "  Batch 1,960  of  4,500.  Loss 0.0025  Elapsed: 0:00:18.916951.\n",
      "  Batch 2,000  of  4,500.  Loss 0.0019  Elapsed: 0:00:19.303437.\n",
      "  Batch 2,040  of  4,500.  Loss 0.0103  Elapsed: 0:00:19.689232.\n",
      "  Batch 2,080  of  4,500.  Loss 0.0119  Elapsed: 0:00:20.074935.\n",
      "  Batch 2,120  of  4,500.  Loss 0.0123  Elapsed: 0:00:20.460603.\n",
      "  Batch 2,160  of  4,500.  Loss 0.0030  Elapsed: 0:00:20.846833.\n",
      "  Batch 2,200  of  4,500.  Loss 0.0017  Elapsed: 0:00:21.233666.\n",
      "  Batch 2,240  of  4,500.  Loss 0.0022  Elapsed: 0:00:21.618195.\n",
      "  Batch 2,280  of  4,500.  Loss 0.0102  Elapsed: 0:00:22.001848.\n",
      "  Batch 2,320  of  4,500.  Loss 0.0038  Elapsed: 0:00:22.389981.\n",
      "  Batch 2,360  of  4,500.  Loss 0.0029  Elapsed: 0:00:22.775956.\n",
      "  Batch 2,400  of  4,500.  Loss 0.0021  Elapsed: 0:00:23.161491.\n",
      "  Batch 2,440  of  4,500.  Loss 0.0023  Elapsed: 0:00:23.549991.\n",
      "  Batch 2,480  of  4,500.  Loss 0.0100  Elapsed: 0:00:23.934558.\n",
      "  Batch 2,520  of  4,500.  Loss 0.0098  Elapsed: 0:00:24.320299.\n",
      "  Batch 2,560  of  4,500.  Loss 0.0088  Elapsed: 0:00:24.705667.\n",
      "  Batch 2,600  of  4,500.  Loss 0.0040  Elapsed: 0:00:25.090603.\n",
      "  Batch 2,640  of  4,500.  Loss 0.0091  Elapsed: 0:00:25.476929.\n",
      "  Batch 2,680  of  4,500.  Loss 0.0234  Elapsed: 0:00:25.863522.\n",
      "  Batch 2,720  of  4,500.  Loss 0.0037  Elapsed: 0:00:26.249183.\n",
      "  Batch 2,760  of  4,500.  Loss 0.0031  Elapsed: 0:00:26.632549.\n",
      "  Batch 2,800  of  4,500.  Loss 0.0075  Elapsed: 0:00:27.018553.\n",
      "  Batch 2,840  of  4,500.  Loss 0.0018  Elapsed: 0:00:27.404506.\n",
      "  Batch 2,880  of  4,500.  Loss 0.0101  Elapsed: 0:00:27.787733.\n",
      "  Batch 2,920  of  4,500.  Loss 0.0105  Elapsed: 0:00:28.173875.\n",
      "  Batch 2,960  of  4,500.  Loss 0.0151  Elapsed: 0:00:28.558093.\n",
      "  Batch 3,000  of  4,500.  Loss 0.0025  Elapsed: 0:00:28.943564.\n",
      "  Batch 3,040  of  4,500.  Loss 0.0031  Elapsed: 0:00:29.328199.\n",
      "  Batch 3,080  of  4,500.  Loss 0.0018  Elapsed: 0:00:29.714835.\n",
      "  Batch 3,120  of  4,500.  Loss 0.0035  Elapsed: 0:00:30.101114.\n",
      "  Batch 3,160  of  4,500.  Loss 0.0020  Elapsed: 0:00:30.486667.\n",
      "  Batch 3,200  of  4,500.  Loss 0.0137  Elapsed: 0:00:30.874469.\n",
      "  Batch 3,240  of  4,500.  Loss 0.0018  Elapsed: 0:00:31.262116.\n",
      "  Batch 3,280  of  4,500.  Loss 0.0093  Elapsed: 0:00:31.647067.\n",
      "  Batch 3,320  of  4,500.  Loss 0.0023  Elapsed: 0:00:32.033722.\n",
      "  Batch 3,360  of  4,500.  Loss 0.0026  Elapsed: 0:00:32.420110.\n",
      "  Batch 3,400  of  4,500.  Loss 0.0090  Elapsed: 0:00:32.807499.\n",
      "  Batch 3,440  of  4,500.  Loss 0.0115  Elapsed: 0:00:33.192929.\n",
      "  Batch 3,480  of  4,500.  Loss 0.0280  Elapsed: 0:00:33.577574.\n",
      "  Batch 3,520  of  4,500.  Loss 0.0017  Elapsed: 0:00:33.964441.\n",
      "  Batch 3,560  of  4,500.  Loss 0.0019  Elapsed: 0:00:34.351108.\n",
      "  Batch 3,600  of  4,500.  Loss 0.0018  Elapsed: 0:00:34.736509.\n",
      "  Batch 3,640  of  4,500.  Loss 0.0097  Elapsed: 0:00:35.122440.\n",
      "  Batch 3,680  of  4,500.  Loss 0.0023  Elapsed: 0:00:35.508792.\n",
      "  Batch 3,720  of  4,500.  Loss 0.0022  Elapsed: 0:00:35.894683.\n",
      "  Batch 3,760  of  4,500.  Loss 0.0026  Elapsed: 0:00:36.281216.\n",
      "  Batch 3,800  of  4,500.  Loss 0.0018  Elapsed: 0:00:36.665461.\n",
      "  Batch 3,840  of  4,500.  Loss 0.0027  Elapsed: 0:00:37.051931.\n",
      "  Batch 3,880  of  4,500.  Loss 0.0056  Elapsed: 0:00:37.440567.\n",
      "  Batch 3,920  of  4,500.  Loss 0.0024  Elapsed: 0:00:37.828081.\n",
      "  Batch 3,960  of  4,500.  Loss 0.0024  Elapsed: 0:00:38.214819.\n",
      "  Batch 4,000  of  4,500.  Loss 0.0018  Elapsed: 0:00:38.600392.\n",
      "  Batch 4,040  of  4,500.  Loss 0.0021  Elapsed: 0:00:38.985956.\n",
      "  Batch 4,080  of  4,500.  Loss 0.0046  Elapsed: 0:00:39.374753.\n",
      "  Batch 4,120  of  4,500.  Loss 0.0018  Elapsed: 0:00:39.760691.\n",
      "  Batch 4,160  of  4,500.  Loss 0.0019  Elapsed: 0:00:40.144745.\n",
      "  Batch 4,200  of  4,500.  Loss 0.0028  Elapsed: 0:00:40.528727.\n",
      "  Batch 4,240  of  4,500.  Loss 0.0017  Elapsed: 0:00:40.914731.\n",
      "  Batch 4,280  of  4,500.  Loss 0.0052  Elapsed: 0:00:41.298091.\n",
      "  Batch 4,320  of  4,500.  Loss 0.0040  Elapsed: 0:00:41.681698.\n",
      "  Batch 4,360  of  4,500.  Loss 0.0020  Elapsed: 0:00:42.065938.\n",
      "  Batch 4,400  of  4,500.  Loss 0.0026  Elapsed: 0:00:42.453422.\n",
      "  Batch 4,440  of  4,500.  Loss 0.0022  Elapsed: 0:00:42.840650.\n",
      "  Batch 4,480  of  4,500.  Loss 0.0021  Elapsed: 0:00:43.226642.\n",
      "Avg Validation Loss 0.0148, Completed in 0:00:43.411126 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.989158\n",
      "F1 Score (micro) =   0.989214\n",
      "F1 Score (macro) =   0.979476\n",
      "F1 Score (samples) =   0.987556\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.9868  Elapsed: 0:00:00.405666.\n",
      "  Batch    80  of  1,477.  Loss 0.0079  Elapsed: 0:00:00.793973.\n",
      "  Batch   120  of  1,477.  Loss 0.0873  Elapsed: 0:00:01.181577.\n",
      "  Batch   160  of  1,477.  Loss 0.2774  Elapsed: 0:00:01.568851.\n",
      "  Batch   200  of  1,477.  Loss 0.0016  Elapsed: 0:00:01.956751.\n",
      "  Batch   240  of  1,477.  Loss 0.7285  Elapsed: 0:00:02.343020.\n",
      "  Batch   280  of  1,477.  Loss 0.0712  Elapsed: 0:00:02.731912.\n",
      "  Batch   320  of  1,477.  Loss 0.0135  Elapsed: 0:00:03.120497.\n",
      "  Batch   360  of  1,477.  Loss 0.0183  Elapsed: 0:00:03.510120.\n",
      "  Batch   400  of  1,477.  Loss 0.0029  Elapsed: 0:00:03.899961.\n",
      "  Batch   440  of  1,477.  Loss 0.0487  Elapsed: 0:00:04.286823.\n",
      "  Batch   480  of  1,477.  Loss 0.0467  Elapsed: 0:00:04.677038.\n",
      "  Batch   520  of  1,477.  Loss 0.1224  Elapsed: 0:00:05.064714.\n",
      "  Batch   560  of  1,477.  Loss 0.0082  Elapsed: 0:00:05.452725.\n",
      "  Batch   600  of  1,477.  Loss 1.3773  Elapsed: 0:00:05.842183.\n",
      "  Batch   640  of  1,477.  Loss 1.0381  Elapsed: 0:00:06.230312.\n",
      "  Batch   680  of  1,477.  Loss 0.0092  Elapsed: 0:00:06.618734.\n",
      "  Batch   720  of  1,477.  Loss 0.0748  Elapsed: 0:00:07.007312.\n",
      "  Batch   760  of  1,477.  Loss 0.0121  Elapsed: 0:00:07.397033.\n",
      "  Batch   800  of  1,477.  Loss 1.2240  Elapsed: 0:00:07.784691.\n",
      "  Batch   840  of  1,477.  Loss 0.1302  Elapsed: 0:00:08.173146.\n",
      "  Batch   880  of  1,477.  Loss 0.2562  Elapsed: 0:00:08.559285.\n",
      "  Batch   920  of  1,477.  Loss 0.1069  Elapsed: 0:00:08.946791.\n",
      "  Batch   960  of  1,477.  Loss 0.0024  Elapsed: 0:00:09.335772.\n",
      "  Batch 1,000  of  1,477.  Loss 1.1358  Elapsed: 0:00:09.724570.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0021  Elapsed: 0:00:10.113584.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0113  Elapsed: 0:00:10.500911.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0021  Elapsed: 0:00:10.890352.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0020  Elapsed: 0:00:11.277178.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0034  Elapsed: 0:00:11.668242.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0093  Elapsed: 0:00:12.054564.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0391  Elapsed: 0:00:12.441879.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0025  Elapsed: 0:00:12.828134.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0093  Elapsed: 0:00:13.216059.\n",
      "  Batch 1,400  of  1,477.  Loss 0.1658  Elapsed: 0:00:13.604462.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0032  Elapsed: 0:00:13.991725.\n",
      "Avg Validation Loss 0.2749, Completed in 0:00:14.341081 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.711348\n",
      "F1 Score (micro) =   0.720833\n",
      "F1 Score (macro) =   0.564597\n",
      "F1 Score (samples) =   0.701196\n",
      "Running Experiment GEMB_5000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    313.  Loss 0.1689  Elapsed: 0:00:25.112700.\n",
      "Epoch  0  Batch   200  of    313.  Loss 0.2221  Elapsed: 0:00:50.118413.\n",
      "Epoch  0  Batch   300  of    313.  Loss 0.3101  Elapsed: 0:01:15.123255.\n",
      "Avg Training Loss 0.2369, Completed in 0:01:18.126016 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2602  Elapsed: 0:00:03.725575.\n",
      "  Batch    80  of     93.  Loss 0.2267  Elapsed: 0:00:07.317863.\n",
      "Avg Validation Loss 0.2526, Completed in 0:00:08.339387 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.549570\n",
      "F1 Score (micro) =   0.598865\n",
      "F1 Score (macro) =   0.354846\n",
      "F1 Score (samples) =   0.500339\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    313.  Loss 0.1301  Elapsed: 0:00:25.101786.\n",
      "Epoch  1  Batch   200  of    313.  Loss 0.2482  Elapsed: 0:00:50.107548.\n",
      "Epoch  1  Batch   300  of    313.  Loss 0.2131  Elapsed: 0:01:15.117120.\n",
      "Avg Training Loss 0.1694, Completed in 0:01:18.118141 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1086  Elapsed: 0:00:03.723761.\n",
      "  Batch    80  of     93.  Loss 0.2482  Elapsed: 0:00:07.316923.\n",
      "Avg Validation Loss 0.2269, Completed in 0:00:08.335842 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.645878\n",
      "F1 Score (micro) =   0.682362\n",
      "F1 Score (macro) =   0.466263\n",
      "F1 Score (samples) =   0.617694\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    313.  Loss 0.1108  Elapsed: 0:00:25.102923.\n",
      "Epoch  2  Batch   200  of    313.  Loss 0.1150  Elapsed: 0:00:50.110301.\n",
      "Epoch  2  Batch   300  of    313.  Loss 0.1311  Elapsed: 0:01:15.115713.\n",
      "Avg Training Loss 0.0965, Completed in 0:01:18.116414 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3379  Elapsed: 0:00:03.720349.\n",
      "  Batch    80  of     93.  Loss 0.1616  Elapsed: 0:00:07.319419.\n",
      "Avg Validation Loss 0.2372, Completed in 0:00:08.341006 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.699826\n",
      "F1 Score (micro) =   0.706304\n",
      "F1 Score (macro) =   0.603618\n",
      "F1 Score (samples) =   0.665313\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    313.  Loss 0.1475  Elapsed: 0:00:25.100028.\n",
      "Epoch  3  Batch   200  of    313.  Loss 0.0320  Elapsed: 0:00:50.105821.\n",
      "Epoch  3  Batch   300  of    313.  Loss 0.0209  Elapsed: 0:01:15.112271.\n",
      "Avg Training Loss 0.0497, Completed in 0:01:18.113213 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3377  Elapsed: 0:00:03.726524.\n",
      "  Batch    80  of     93.  Loss 0.3304  Elapsed: 0:00:07.318470.\n",
      "Avg Validation Loss 0.2744, Completed in 0:00:08.337847 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.706080\n",
      "F1 Score (micro) =   0.708905\n",
      "F1 Score (macro) =   0.619934\n",
      "F1 Score (samples) =   0.680659\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  5,000.  Loss 0.0061  Elapsed: 0:00:00.405102.\n",
      "  Batch    80  of  5,000.  Loss 0.0097  Elapsed: 0:00:00.795423.\n",
      "  Batch   120  of  5,000.  Loss 0.0575  Elapsed: 0:00:01.185482.\n",
      "  Batch   160  of  5,000.  Loss 0.0067  Elapsed: 0:00:01.572779.\n",
      "  Batch   200  of  5,000.  Loss 0.0042  Elapsed: 0:00:01.961925.\n",
      "  Batch   240  of  5,000.  Loss 0.0100  Elapsed: 0:00:02.350012.\n",
      "  Batch   280  of  5,000.  Loss 0.0678  Elapsed: 0:00:02.737385.\n",
      "  Batch   320  of  5,000.  Loss 0.0118  Elapsed: 0:00:03.124243.\n",
      "  Batch   360  of  5,000.  Loss 0.0223  Elapsed: 0:00:03.513349.\n",
      "  Batch   400  of  5,000.  Loss 0.0277  Elapsed: 0:00:03.900477.\n",
      "  Batch   440  of  5,000.  Loss 0.0178  Elapsed: 0:00:04.288655.\n",
      "  Batch   480  of  5,000.  Loss 0.0021  Elapsed: 0:00:04.677495.\n",
      "  Batch   520  of  5,000.  Loss 0.0557  Elapsed: 0:00:05.065097.\n",
      "  Batch   560  of  5,000.  Loss 0.0017  Elapsed: 0:00:05.451530.\n",
      "  Batch   600  of  5,000.  Loss 0.0016  Elapsed: 0:00:05.838336.\n",
      "  Batch   640  of  5,000.  Loss 0.0081  Elapsed: 0:00:06.225058.\n",
      "  Batch   680  of  5,000.  Loss 0.0055  Elapsed: 0:00:06.612028.\n",
      "  Batch   720  of  5,000.  Loss 0.0079  Elapsed: 0:00:06.999580.\n",
      "  Batch   760  of  5,000.  Loss 0.0017  Elapsed: 0:00:07.388815.\n",
      "  Batch   800  of  5,000.  Loss 0.0062  Elapsed: 0:00:07.776716.\n",
      "  Batch   840  of  5,000.  Loss 0.0017  Elapsed: 0:00:08.163257.\n",
      "  Batch   880  of  5,000.  Loss 0.0017  Elapsed: 0:00:08.549031.\n",
      "  Batch   920  of  5,000.  Loss 0.0032  Elapsed: 0:00:08.933753.\n",
      "  Batch   960  of  5,000.  Loss 0.0025  Elapsed: 0:00:09.320963.\n",
      "  Batch 1,000  of  5,000.  Loss 0.0124  Elapsed: 0:00:09.706883.\n",
      "  Batch 1,040  of  5,000.  Loss 0.0020  Elapsed: 0:00:10.093798.\n",
      "  Batch 1,080  of  5,000.  Loss 0.0041  Elapsed: 0:00:10.482172.\n",
      "  Batch 1,120  of  5,000.  Loss 0.0025  Elapsed: 0:00:10.870507.\n",
      "  Batch 1,160  of  5,000.  Loss 0.0027  Elapsed: 0:00:11.258274.\n",
      "  Batch 1,200  of  5,000.  Loss 0.0018  Elapsed: 0:00:11.645200.\n",
      "  Batch 1,240  of  5,000.  Loss 0.0017  Elapsed: 0:00:12.033957.\n",
      "  Batch 1,280  of  5,000.  Loss 0.0088  Elapsed: 0:00:12.419500.\n",
      "  Batch 1,320  of  5,000.  Loss 0.0082  Elapsed: 0:00:12.806981.\n",
      "  Batch 1,360  of  5,000.  Loss 0.0067  Elapsed: 0:00:13.194775.\n",
      "  Batch 1,400  of  5,000.  Loss 0.0508  Elapsed: 0:00:13.583453.\n",
      "  Batch 1,440  of  5,000.  Loss 0.0021  Elapsed: 0:00:13.972471.\n",
      "  Batch 1,480  of  5,000.  Loss 0.0122  Elapsed: 0:00:14.359831.\n",
      "  Batch 1,520  of  5,000.  Loss 0.1277  Elapsed: 0:00:14.747610.\n",
      "  Batch 1,560  of  5,000.  Loss 0.0021  Elapsed: 0:00:15.133900.\n",
      "  Batch 1,600  of  5,000.  Loss 0.0017  Elapsed: 0:00:15.518952.\n",
      "  Batch 1,640  of  5,000.  Loss 0.0121  Elapsed: 0:00:15.908010.\n",
      "  Batch 1,680  of  5,000.  Loss 0.0016  Elapsed: 0:00:16.294972.\n",
      "  Batch 1,720  of  5,000.  Loss 0.0020  Elapsed: 0:00:16.682441.\n",
      "  Batch 1,760  of  5,000.  Loss 0.0029  Elapsed: 0:00:17.069953.\n",
      "  Batch 1,800  of  5,000.  Loss 0.0173  Elapsed: 0:00:17.458320.\n",
      "  Batch 1,840  of  5,000.  Loss 0.0021  Elapsed: 0:00:17.846270.\n",
      "  Batch 1,880  of  5,000.  Loss 0.0097  Elapsed: 0:00:18.232752.\n",
      "  Batch 1,920  of  5,000.  Loss 0.0111  Elapsed: 0:00:18.624082.\n",
      "  Batch 1,960  of  5,000.  Loss 0.0017  Elapsed: 0:00:19.011482.\n",
      "  Batch 2,000  of  5,000.  Loss 0.0028  Elapsed: 0:00:19.399327.\n",
      "  Batch 2,040  of  5,000.  Loss 0.0017  Elapsed: 0:00:19.786397.\n",
      "  Batch 2,080  of  5,000.  Loss 0.0017  Elapsed: 0:00:20.173841.\n",
      "  Batch 2,120  of  5,000.  Loss 0.0175  Elapsed: 0:00:20.560190.\n",
      "  Batch 2,160  of  5,000.  Loss 0.0157  Elapsed: 0:00:20.947187.\n",
      "  Batch 2,200  of  5,000.  Loss 0.0179  Elapsed: 0:00:21.335449.\n",
      "  Batch 2,240  of  5,000.  Loss 0.0017  Elapsed: 0:00:21.721079.\n",
      "  Batch 2,280  of  5,000.  Loss 0.0054  Elapsed: 0:00:22.109560.\n",
      "  Batch 2,320  of  5,000.  Loss 0.0307  Elapsed: 0:00:22.497691.\n",
      "  Batch 2,360  of  5,000.  Loss 0.0135  Elapsed: 0:00:22.884198.\n",
      "  Batch 2,400  of  5,000.  Loss 0.0039  Elapsed: 0:00:23.272735.\n",
      "  Batch 2,440  of  5,000.  Loss 0.0133  Elapsed: 0:00:23.660354.\n",
      "  Batch 2,480  of  5,000.  Loss 0.0095  Elapsed: 0:00:24.047052.\n",
      "  Batch 2,520  of  5,000.  Loss 0.0516  Elapsed: 0:00:24.433359.\n",
      "  Batch 2,560  of  5,000.  Loss 0.0358  Elapsed: 0:00:24.819948.\n",
      "  Batch 2,600  of  5,000.  Loss 0.0096  Elapsed: 0:00:25.207493.\n",
      "  Batch 2,640  of  5,000.  Loss 0.0017  Elapsed: 0:00:25.594213.\n",
      "  Batch 2,680  of  5,000.  Loss 0.0264  Elapsed: 0:00:25.981543.\n",
      "  Batch 2,720  of  5,000.  Loss 0.2712  Elapsed: 0:00:26.368037.\n",
      "  Batch 2,760  of  5,000.  Loss 0.0517  Elapsed: 0:00:26.755220.\n",
      "  Batch 2,800  of  5,000.  Loss 0.0018  Elapsed: 0:00:27.144163.\n",
      "  Batch 2,840  of  5,000.  Loss 0.0021  Elapsed: 0:00:27.530837.\n",
      "  Batch 2,880  of  5,000.  Loss 0.0025  Elapsed: 0:00:27.919411.\n",
      "  Batch 2,920  of  5,000.  Loss 0.0030  Elapsed: 0:00:28.307398.\n",
      "  Batch 2,960  of  5,000.  Loss 0.0078  Elapsed: 0:00:28.695691.\n",
      "  Batch 3,000  of  5,000.  Loss 0.0082  Elapsed: 0:00:29.083557.\n",
      "  Batch 3,040  of  5,000.  Loss 0.0160  Elapsed: 0:00:29.471598.\n",
      "  Batch 3,080  of  5,000.  Loss 0.0018  Elapsed: 0:00:29.859140.\n",
      "  Batch 3,120  of  5,000.  Loss 0.0018  Elapsed: 0:00:30.246988.\n",
      "  Batch 3,160  of  5,000.  Loss 0.0068  Elapsed: 0:00:30.635132.\n",
      "  Batch 3,200  of  5,000.  Loss 0.0027  Elapsed: 0:00:31.023030.\n",
      "  Batch 3,240  of  5,000.  Loss 0.0158  Elapsed: 0:00:31.409190.\n",
      "  Batch 3,280  of  5,000.  Loss 0.6910  Elapsed: 0:00:31.799016.\n",
      "  Batch 3,320  of  5,000.  Loss 0.8139  Elapsed: 0:00:32.185824.\n",
      "  Batch 3,360  of  5,000.  Loss 0.0072  Elapsed: 0:00:32.572053.\n",
      "  Batch 3,400  of  5,000.  Loss 0.0016  Elapsed: 0:00:32.958102.\n",
      "  Batch 3,440  of  5,000.  Loss 0.0020  Elapsed: 0:00:33.346473.\n",
      "  Batch 3,480  of  5,000.  Loss 0.0516  Elapsed: 0:00:33.734192.\n",
      "  Batch 3,520  of  5,000.  Loss 0.0016  Elapsed: 0:00:34.121997.\n",
      "  Batch 3,560  of  5,000.  Loss 0.0032  Elapsed: 0:00:34.509108.\n",
      "  Batch 3,600  of  5,000.  Loss 0.0155  Elapsed: 0:00:34.894644.\n",
      "  Batch 3,640  of  5,000.  Loss 0.0071  Elapsed: 0:00:35.280071.\n",
      "  Batch 3,680  of  5,000.  Loss 0.0016  Elapsed: 0:00:35.667155.\n",
      "  Batch 3,720  of  5,000.  Loss 0.0143  Elapsed: 0:00:36.055731.\n",
      "  Batch 3,760  of  5,000.  Loss 0.0081  Elapsed: 0:00:36.443282.\n",
      "  Batch 3,800  of  5,000.  Loss 0.0020  Elapsed: 0:00:36.829696.\n",
      "  Batch 3,840  of  5,000.  Loss 0.9309  Elapsed: 0:00:37.219517.\n",
      "  Batch 3,880  of  5,000.  Loss 0.0123  Elapsed: 0:00:37.608100.\n",
      "  Batch 3,920  of  5,000.  Loss 0.0024  Elapsed: 0:00:37.998696.\n",
      "  Batch 3,960  of  5,000.  Loss 0.0021  Elapsed: 0:00:38.386776.\n",
      "  Batch 4,000  of  5,000.  Loss 0.0044  Elapsed: 0:00:38.774167.\n",
      "  Batch 4,040  of  5,000.  Loss 0.0110  Elapsed: 0:00:39.163053.\n",
      "  Batch 4,080  of  5,000.  Loss 0.0034  Elapsed: 0:00:39.551180.\n",
      "  Batch 4,120  of  5,000.  Loss 0.0165  Elapsed: 0:00:39.939731.\n",
      "  Batch 4,160  of  5,000.  Loss 0.0017  Elapsed: 0:00:40.327772.\n",
      "  Batch 4,200  of  5,000.  Loss 0.0018  Elapsed: 0:00:40.715552.\n",
      "  Batch 4,240  of  5,000.  Loss 0.0017  Elapsed: 0:00:41.103086.\n",
      "  Batch 4,280  of  5,000.  Loss 0.0020  Elapsed: 0:00:41.497411.\n",
      "  Batch 4,320  of  5,000.  Loss 0.0122  Elapsed: 0:00:41.887945.\n",
      "  Batch 4,360  of  5,000.  Loss 0.0094  Elapsed: 0:00:42.276457.\n",
      "  Batch 4,400  of  5,000.  Loss 0.0020  Elapsed: 0:00:42.662376.\n",
      "  Batch 4,440  of  5,000.  Loss 0.0021  Elapsed: 0:00:43.049588.\n",
      "  Batch 4,480  of  5,000.  Loss 0.0025  Elapsed: 0:00:43.435774.\n",
      "  Batch 4,520  of  5,000.  Loss 0.0621  Elapsed: 0:00:43.822966.\n",
      "  Batch 4,560  of  5,000.  Loss 0.0058  Elapsed: 0:00:44.210827.\n",
      "  Batch 4,600  of  5,000.  Loss 0.0042  Elapsed: 0:00:44.599247.\n",
      "  Batch 4,640  of  5,000.  Loss 0.0049  Elapsed: 0:00:44.986181.\n",
      "  Batch 4,680  of  5,000.  Loss 0.0017  Elapsed: 0:00:45.372410.\n",
      "  Batch 4,720  of  5,000.  Loss 0.0114  Elapsed: 0:00:45.762180.\n",
      "  Batch 4,760  of  5,000.  Loss 0.0027  Elapsed: 0:00:46.150088.\n",
      "  Batch 4,800  of  5,000.  Loss 0.2343  Elapsed: 0:00:46.539590.\n",
      "  Batch 4,840  of  5,000.  Loss 0.0021  Elapsed: 0:00:46.924690.\n",
      "  Batch 4,880  of  5,000.  Loss 0.0049  Elapsed: 0:00:47.311904.\n",
      "  Batch 4,920  of  5,000.  Loss 0.0021  Elapsed: 0:00:47.700784.\n",
      "  Batch 4,960  of  5,000.  Loss 0.0134  Elapsed: 0:00:48.088721.\n",
      "Avg Validation Loss 0.0181, Completed in 0:00:48.468338 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.985857\n",
      "F1 Score (micro) =   0.985972\n",
      "F1 Score (macro) =   0.970781\n",
      "F1 Score (samples) =   0.983267\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0277  Elapsed: 0:00:00.397868.\n",
      "  Batch    80  of  1,477.  Loss 0.0423  Elapsed: 0:00:00.786379.\n",
      "  Batch   120  of  1,477.  Loss 0.0620  Elapsed: 0:00:01.173946.\n",
      "  Batch   160  of  1,477.  Loss 0.0026  Elapsed: 0:00:01.561234.\n",
      "  Batch   200  of  1,477.  Loss 0.0086  Elapsed: 0:00:01.947209.\n",
      "  Batch   240  of  1,477.  Loss 0.1207  Elapsed: 0:00:02.334544.\n",
      "  Batch   280  of  1,477.  Loss 0.5026  Elapsed: 0:00:02.724968.\n",
      "  Batch   320  of  1,477.  Loss 0.0047  Elapsed: 0:00:03.113700.\n",
      "  Batch   360  of  1,477.  Loss 0.0068  Elapsed: 0:00:03.501239.\n",
      "  Batch   400  of  1,477.  Loss 0.2821  Elapsed: 0:00:03.887872.\n",
      "  Batch   440  of  1,477.  Loss 0.0094  Elapsed: 0:00:04.274835.\n",
      "  Batch   480  of  1,477.  Loss 0.0153  Elapsed: 0:00:04.661484.\n",
      "  Batch   520  of  1,477.  Loss 0.0026  Elapsed: 0:00:05.048603.\n",
      "  Batch   560  of  1,477.  Loss 0.0458  Elapsed: 0:00:05.433311.\n",
      "  Batch   600  of  1,477.  Loss 0.0022  Elapsed: 0:00:05.818298.\n",
      "  Batch   640  of  1,477.  Loss 0.2144  Elapsed: 0:00:06.203916.\n",
      "  Batch   680  of  1,477.  Loss 0.0290  Elapsed: 0:00:06.589315.\n",
      "  Batch   720  of  1,477.  Loss 0.0080  Elapsed: 0:00:06.973033.\n",
      "  Batch   760  of  1,477.  Loss 0.0073  Elapsed: 0:00:07.361809.\n",
      "  Batch   800  of  1,477.  Loss 0.4603  Elapsed: 0:00:07.749056.\n",
      "  Batch   840  of  1,477.  Loss 0.2177  Elapsed: 0:00:08.134831.\n",
      "  Batch   880  of  1,477.  Loss 0.0580  Elapsed: 0:00:08.522288.\n",
      "  Batch   920  of  1,477.  Loss 0.2517  Elapsed: 0:00:08.910326.\n",
      "  Batch   960  of  1,477.  Loss 0.0021  Elapsed: 0:00:09.297976.\n",
      "  Batch 1,000  of  1,477.  Loss 0.9966  Elapsed: 0:00:09.684671.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0021  Elapsed: 0:00:10.070896.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0093  Elapsed: 0:00:10.458017.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0021  Elapsed: 0:00:10.844457.\n",
      "  Batch 1,160  of  1,477.  Loss 0.9324  Elapsed: 0:00:11.229536.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0017  Elapsed: 0:00:11.615432.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0055  Elapsed: 0:00:11.999241.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0019  Elapsed: 0:00:12.386402.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0320  Elapsed: 0:00:12.771515.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0019  Elapsed: 0:00:13.159312.\n",
      "  Batch 1,400  of  1,477.  Loss 0.9913  Elapsed: 0:00:13.545674.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0088  Elapsed: 0:00:13.933312.\n",
      "Avg Validation Loss 0.2704, Completed in 0:00:14.281190 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.718684\n",
      "F1 Score (micro) =   0.720955\n",
      "F1 Score (macro) =   0.593066\n",
      "F1 Score (samples) =   0.694426\n",
      "Running Experiment GEMB_6000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    375.  Loss 0.2186  Elapsed: 0:00:25.104016.\n",
      "Epoch  0  Batch   200  of    375.  Loss 0.2538  Elapsed: 0:00:50.108953.\n",
      "Epoch  0  Batch   300  of    375.  Loss 0.1778  Elapsed: 0:01:15.111344.\n",
      "Avg Training Loss 0.2340, Completed in 0:01:33.693947 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2364  Elapsed: 0:00:03.754693.\n",
      "  Batch    80  of     93.  Loss 0.2312  Elapsed: 0:00:07.357195.\n",
      "Avg Validation Loss 0.2469, Completed in 0:00:08.377451 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.562171\n",
      "F1 Score (micro) =   0.629904\n",
      "F1 Score (macro) =   0.345398\n",
      "F1 Score (samples) =   0.575491\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    375.  Loss 0.1599  Elapsed: 0:00:25.100629.\n",
      "Epoch  1  Batch   200  of    375.  Loss 0.1181  Elapsed: 0:00:50.106756.\n",
      "Epoch  1  Batch   300  of    375.  Loss 0.1525  Elapsed: 0:01:15.114064.\n",
      "Avg Training Loss 0.1752, Completed in 0:01:33.700589 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3086  Elapsed: 0:00:03.763231.\n",
      "  Batch    80  of     93.  Loss 0.1600  Elapsed: 0:00:07.364950.\n",
      "Avg Validation Loss 0.2225, Completed in 0:00:08.388929 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.647617\n",
      "F1 Score (micro) =   0.691528\n",
      "F1 Score (macro) =   0.460295\n",
      "F1 Score (samples) =   0.645904\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    375.  Loss 0.2269  Elapsed: 0:00:25.100871.\n",
      "Epoch  2  Batch   200  of    375.  Loss 0.0762  Elapsed: 0:00:50.107250.\n",
      "Epoch  2  Batch   300  of    375.  Loss 0.0519  Elapsed: 0:01:15.114365.\n",
      "Avg Training Loss 0.1036, Completed in 0:01:33.709882 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1933  Elapsed: 0:00:03.765160.\n",
      "  Batch    80  of     93.  Loss 0.2820  Elapsed: 0:00:07.359544.\n",
      "Avg Validation Loss 0.2046, Completed in 0:00:08.378521 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.736858\n",
      "F1 Score (micro) =   0.746055\n",
      "F1 Score (macro) =   0.626389\n",
      "F1 Score (samples) =   0.702776\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    375.  Loss 0.0597  Elapsed: 0:00:25.098045.\n",
      "Epoch  3  Batch   200  of    375.  Loss 0.0202  Elapsed: 0:00:50.108001.\n",
      "Epoch  3  Batch   300  of    375.  Loss 0.0745  Elapsed: 0:01:15.110226.\n",
      "Avg Training Loss 0.0532, Completed in 0:01:33.695001 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3099  Elapsed: 0:00:03.758872.\n",
      "  Batch    80  of     93.  Loss 0.2146  Elapsed: 0:00:07.354021.\n",
      "Avg Validation Loss 0.2212, Completed in 0:00:08.376370 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.762885\n",
      "F1 Score (micro) =   0.765647\n",
      "F1 Score (macro) =   0.669006\n",
      "F1 Score (samples) =   0.743624\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  6,000.  Loss 0.0038  Elapsed: 0:00:00.403348.\n",
      "  Batch    80  of  6,000.  Loss 0.0049  Elapsed: 0:00:00.790857.\n",
      "  Batch   120  of  6,000.  Loss 0.0090  Elapsed: 0:00:01.179247.\n",
      "  Batch   160  of  6,000.  Loss 0.0021  Elapsed: 0:00:01.566586.\n",
      "  Batch   200  of  6,000.  Loss 0.0099  Elapsed: 0:00:01.955343.\n",
      "  Batch   240  of  6,000.  Loss 0.0021  Elapsed: 0:00:02.344650.\n",
      "  Batch   280  of  6,000.  Loss 0.0075  Elapsed: 0:00:02.733590.\n",
      "  Batch   320  of  6,000.  Loss 0.0015  Elapsed: 0:00:03.120591.\n",
      "  Batch   360  of  6,000.  Loss 0.0018  Elapsed: 0:00:03.510610.\n",
      "  Batch   400  of  6,000.  Loss 0.0104  Elapsed: 0:00:03.897794.\n",
      "  Batch   440  of  6,000.  Loss 0.0014  Elapsed: 0:00:04.284227.\n",
      "  Batch   480  of  6,000.  Loss 0.0021  Elapsed: 0:00:04.672821.\n",
      "  Batch   520  of  6,000.  Loss 0.0015  Elapsed: 0:00:05.064999.\n",
      "  Batch   560  of  6,000.  Loss 0.0036  Elapsed: 0:00:05.453772.\n",
      "  Batch   600  of  6,000.  Loss 0.0014  Elapsed: 0:00:05.841475.\n",
      "  Batch   640  of  6,000.  Loss 0.0015  Elapsed: 0:00:06.229107.\n",
      "  Batch   680  of  6,000.  Loss 0.0065  Elapsed: 0:00:06.618503.\n",
      "  Batch   720  of  6,000.  Loss 0.0131  Elapsed: 0:00:07.007244.\n",
      "  Batch   760  of  6,000.  Loss 0.0107  Elapsed: 0:00:07.397977.\n",
      "  Batch   800  of  6,000.  Loss 0.0014  Elapsed: 0:00:07.785976.\n",
      "  Batch   840  of  6,000.  Loss 0.0014  Elapsed: 0:00:08.175117.\n",
      "  Batch   880  of  6,000.  Loss 0.0019  Elapsed: 0:00:08.562788.\n",
      "  Batch   920  of  6,000.  Loss 0.0029  Elapsed: 0:00:08.950584.\n",
      "  Batch   960  of  6,000.  Loss 0.0015  Elapsed: 0:00:09.337416.\n",
      "  Batch 1,000  of  6,000.  Loss 0.0104  Elapsed: 0:00:09.726122.\n",
      "  Batch 1,040  of  6,000.  Loss 0.0089  Elapsed: 0:00:10.112981.\n",
      "  Batch 1,080  of  6,000.  Loss 0.0540  Elapsed: 0:00:10.499879.\n",
      "  Batch 1,120  of  6,000.  Loss 0.0015  Elapsed: 0:00:10.886752.\n",
      "  Batch 1,160  of  6,000.  Loss 0.0096  Elapsed: 0:00:11.272803.\n",
      "  Batch 1,200  of  6,000.  Loss 0.0016  Elapsed: 0:00:11.659554.\n",
      "  Batch 1,240  of  6,000.  Loss 0.0016  Elapsed: 0:00:12.048546.\n",
      "  Batch 1,280  of  6,000.  Loss 0.0015  Elapsed: 0:00:12.438912.\n",
      "  Batch 1,320  of  6,000.  Loss 0.0106  Elapsed: 0:00:12.826707.\n",
      "  Batch 1,360  of  6,000.  Loss 0.0099  Elapsed: 0:00:13.214774.\n",
      "  Batch 1,400  of  6,000.  Loss 0.0091  Elapsed: 0:00:13.605034.\n",
      "  Batch 1,440  of  6,000.  Loss 0.0088  Elapsed: 0:00:13.993532.\n",
      "  Batch 1,480  of  6,000.  Loss 0.0023  Elapsed: 0:00:14.385691.\n",
      "  Batch 1,520  of  6,000.  Loss 0.0046  Elapsed: 0:00:14.772617.\n",
      "  Batch 1,560  of  6,000.  Loss 0.0119  Elapsed: 0:00:15.160880.\n",
      "  Batch 1,600  of  6,000.  Loss 0.0026  Elapsed: 0:00:15.549240.\n",
      "  Batch 1,640  of  6,000.  Loss 0.0014  Elapsed: 0:00:15.939777.\n",
      "  Batch 1,680  of  6,000.  Loss 0.0015  Elapsed: 0:00:16.330592.\n",
      "  Batch 1,720  of  6,000.  Loss 0.0017  Elapsed: 0:00:16.719987.\n",
      "  Batch 1,760  of  6,000.  Loss 0.0125  Elapsed: 0:00:17.109526.\n",
      "  Batch 1,800  of  6,000.  Loss 0.0074  Elapsed: 0:00:17.496706.\n",
      "  Batch 1,840  of  6,000.  Loss 0.2277  Elapsed: 0:00:17.883212.\n",
      "  Batch 1,880  of  6,000.  Loss 0.0017  Elapsed: 0:00:18.269441.\n",
      "  Batch 1,920  of  6,000.  Loss 0.0017  Elapsed: 0:00:18.656789.\n",
      "  Batch 1,960  of  6,000.  Loss 0.0021  Elapsed: 0:00:19.046159.\n",
      "  Batch 2,000  of  6,000.  Loss 0.0116  Elapsed: 0:00:19.434230.\n",
      "  Batch 2,040  of  6,000.  Loss 0.0048  Elapsed: 0:00:19.824771.\n",
      "  Batch 2,080  of  6,000.  Loss 0.0155  Elapsed: 0:00:20.212995.\n",
      "  Batch 2,120  of  6,000.  Loss 0.0017  Elapsed: 0:00:20.600142.\n",
      "  Batch 2,160  of  6,000.  Loss 0.0018  Elapsed: 0:00:20.987088.\n",
      "  Batch 2,200  of  6,000.  Loss 0.5426  Elapsed: 0:00:21.376981.\n",
      "  Batch 2,240  of  6,000.  Loss 0.0017  Elapsed: 0:00:21.764942.\n",
      "  Batch 2,280  of  6,000.  Loss 0.0014  Elapsed: 0:00:22.152150.\n",
      "  Batch 2,320  of  6,000.  Loss 0.0099  Elapsed: 0:00:22.542294.\n",
      "  Batch 2,360  of  6,000.  Loss 0.0082  Elapsed: 0:00:22.928680.\n",
      "  Batch 2,400  of  6,000.  Loss 0.0040  Elapsed: 0:00:23.314044.\n",
      "  Batch 2,440  of  6,000.  Loss 0.0015  Elapsed: 0:00:23.702071.\n",
      "  Batch 2,480  of  6,000.  Loss 0.0034  Elapsed: 0:00:24.092343.\n",
      "  Batch 2,520  of  6,000.  Loss 0.0032  Elapsed: 0:00:24.479770.\n",
      "  Batch 2,560  of  6,000.  Loss 0.0130  Elapsed: 0:00:24.867575.\n",
      "  Batch 2,600  of  6,000.  Loss 0.0023  Elapsed: 0:00:25.252779.\n",
      "  Batch 2,640  of  6,000.  Loss 0.0014  Elapsed: 0:00:25.641156.\n",
      "  Batch 2,680  of  6,000.  Loss 0.0366  Elapsed: 0:00:26.029861.\n",
      "  Batch 2,720  of  6,000.  Loss 0.0108  Elapsed: 0:00:26.418133.\n",
      "  Batch 2,760  of  6,000.  Loss 0.0015  Elapsed: 0:00:26.807178.\n",
      "  Batch 2,800  of  6,000.  Loss 0.0016  Elapsed: 0:00:27.193562.\n",
      "  Batch 2,840  of  6,000.  Loss 0.0317  Elapsed: 0:00:27.582069.\n",
      "  Batch 2,880  of  6,000.  Loss 0.0083  Elapsed: 0:00:27.968868.\n",
      "  Batch 2,920  of  6,000.  Loss 0.0106  Elapsed: 0:00:28.355790.\n",
      "  Batch 2,960  of  6,000.  Loss 0.0195  Elapsed: 0:00:28.743379.\n",
      "  Batch 3,000  of  6,000.  Loss 0.0178  Elapsed: 0:00:29.132835.\n",
      "  Batch 3,040  of  6,000.  Loss 0.0021  Elapsed: 0:00:29.520682.\n",
      "  Batch 3,080  of  6,000.  Loss 0.0109  Elapsed: 0:00:29.907512.\n",
      "  Batch 3,120  of  6,000.  Loss 0.0019  Elapsed: 0:00:30.294761.\n",
      "  Batch 3,160  of  6,000.  Loss 0.0017  Elapsed: 0:00:30.681612.\n",
      "  Batch 3,200  of  6,000.  Loss 0.0084  Elapsed: 0:00:31.070963.\n",
      "  Batch 3,240  of  6,000.  Loss 0.0066  Elapsed: 0:00:31.457686.\n",
      "  Batch 3,280  of  6,000.  Loss 0.0018  Elapsed: 0:00:31.844158.\n",
      "  Batch 3,320  of  6,000.  Loss 0.0023  Elapsed: 0:00:32.230608.\n",
      "  Batch 3,360  of  6,000.  Loss 0.0015  Elapsed: 0:00:32.620363.\n",
      "  Batch 3,400  of  6,000.  Loss 0.0048  Elapsed: 0:00:33.013155.\n",
      "  Batch 3,440  of  6,000.  Loss 0.0150  Elapsed: 0:00:33.401024.\n",
      "  Batch 3,480  of  6,000.  Loss 0.0015  Elapsed: 0:00:33.788899.\n",
      "  Batch 3,520  of  6,000.  Loss 0.0015  Elapsed: 0:00:34.177380.\n",
      "  Batch 3,560  of  6,000.  Loss 0.0015  Elapsed: 0:00:34.566385.\n",
      "  Batch 3,600  of  6,000.  Loss 0.0015  Elapsed: 0:00:34.954336.\n",
      "  Batch 3,640  of  6,000.  Loss 0.0241  Elapsed: 0:00:35.342911.\n",
      "  Batch 3,680  of  6,000.  Loss 0.0130  Elapsed: 0:00:35.731280.\n",
      "  Batch 3,720  of  6,000.  Loss 0.0177  Elapsed: 0:00:36.118815.\n",
      "  Batch 3,760  of  6,000.  Loss 0.0016  Elapsed: 0:00:36.505219.\n",
      "  Batch 3,800  of  6,000.  Loss 0.0087  Elapsed: 0:00:36.892328.\n",
      "  Batch 3,840  of  6,000.  Loss 0.0018  Elapsed: 0:00:37.279681.\n",
      "  Batch 3,880  of  6,000.  Loss 0.0088  Elapsed: 0:00:37.666064.\n",
      "  Batch 3,920  of  6,000.  Loss 0.0021  Elapsed: 0:00:38.054657.\n",
      "  Batch 3,960  of  6,000.  Loss 0.0014  Elapsed: 0:00:38.442306.\n",
      "  Batch 4,000  of  6,000.  Loss 0.0363  Elapsed: 0:00:38.830570.\n",
      "  Batch 4,040  of  6,000.  Loss 0.0148  Elapsed: 0:00:39.217150.\n",
      "  Batch 4,080  of  6,000.  Loss 0.0199  Elapsed: 0:00:39.604218.\n",
      "  Batch 4,120  of  6,000.  Loss 0.0021  Elapsed: 0:00:39.993036.\n",
      "  Batch 4,160  of  6,000.  Loss 0.0021  Elapsed: 0:00:40.384162.\n",
      "  Batch 4,200  of  6,000.  Loss 0.0018  Elapsed: 0:00:40.773334.\n",
      "  Batch 4,240  of  6,000.  Loss 0.0018  Elapsed: 0:00:41.161309.\n",
      "  Batch 4,280  of  6,000.  Loss 0.0030  Elapsed: 0:00:41.547750.\n",
      "  Batch 4,320  of  6,000.  Loss 0.0015  Elapsed: 0:00:41.935997.\n",
      "  Batch 4,360  of  6,000.  Loss 0.0034  Elapsed: 0:00:42.321879.\n",
      "  Batch 4,400  of  6,000.  Loss 0.0021  Elapsed: 0:00:42.710822.\n",
      "  Batch 4,440  of  6,000.  Loss 0.0014  Elapsed: 0:00:43.097970.\n",
      "  Batch 4,480  of  6,000.  Loss 0.0021  Elapsed: 0:00:43.487682.\n",
      "  Batch 4,520  of  6,000.  Loss 0.0101  Elapsed: 0:00:43.874422.\n",
      "  Batch 4,560  of  6,000.  Loss 0.0015  Elapsed: 0:00:44.262985.\n",
      "  Batch 4,600  of  6,000.  Loss 0.0014  Elapsed: 0:00:44.651582.\n",
      "  Batch 4,640  of  6,000.  Loss 0.0019  Elapsed: 0:00:45.041208.\n",
      "  Batch 4,680  of  6,000.  Loss 0.0036  Elapsed: 0:00:45.431638.\n",
      "  Batch 4,720  of  6,000.  Loss 0.0045  Elapsed: 0:00:45.820337.\n",
      "  Batch 4,760  of  6,000.  Loss 0.0522  Elapsed: 0:00:46.208917.\n",
      "  Batch 4,800  of  6,000.  Loss 0.0018  Elapsed: 0:00:46.596917.\n",
      "  Batch 4,840  of  6,000.  Loss 0.0015  Elapsed: 0:00:46.986316.\n",
      "  Batch 4,880  of  6,000.  Loss 0.0054  Elapsed: 0:00:47.373405.\n",
      "  Batch 4,920  of  6,000.  Loss 0.0025  Elapsed: 0:00:47.763230.\n",
      "  Batch 4,960  of  6,000.  Loss 0.0351  Elapsed: 0:00:48.150029.\n",
      "  Batch 5,000  of  6,000.  Loss 0.0087  Elapsed: 0:00:48.538062.\n",
      "  Batch 5,040  of  6,000.  Loss 0.0117  Elapsed: 0:00:48.924138.\n",
      "  Batch 5,080  of  6,000.  Loss 0.0019  Elapsed: 0:00:49.311980.\n",
      "  Batch 5,120  of  6,000.  Loss 0.0025  Elapsed: 0:00:49.700702.\n",
      "  Batch 5,160  of  6,000.  Loss 0.0015  Elapsed: 0:00:50.089194.\n",
      "  Batch 5,200  of  6,000.  Loss 0.0014  Elapsed: 0:00:50.478044.\n",
      "  Batch 5,240  of  6,000.  Loss 0.0047  Elapsed: 0:00:50.863189.\n",
      "  Batch 5,280  of  6,000.  Loss 0.0077  Elapsed: 0:00:51.252025.\n",
      "  Batch 5,320  of  6,000.  Loss 0.0023  Elapsed: 0:00:51.639108.\n",
      "  Batch 5,360  of  6,000.  Loss 0.0078  Elapsed: 0:00:52.027944.\n",
      "  Batch 5,400  of  6,000.  Loss 0.0020  Elapsed: 0:00:52.415051.\n",
      "  Batch 5,440  of  6,000.  Loss 0.0021  Elapsed: 0:00:52.803700.\n",
      "  Batch 5,480  of  6,000.  Loss 0.0020  Elapsed: 0:00:53.192414.\n",
      "  Batch 5,520  of  6,000.  Loss 0.0030  Elapsed: 0:00:53.579756.\n",
      "  Batch 5,560  of  6,000.  Loss 0.0153  Elapsed: 0:00:53.967988.\n",
      "  Batch 5,600  of  6,000.  Loss 0.0040  Elapsed: 0:00:54.355577.\n",
      "  Batch 5,640  of  6,000.  Loss 0.0029  Elapsed: 0:00:54.743315.\n",
      "  Batch 5,680  of  6,000.  Loss 0.0016  Elapsed: 0:00:55.132121.\n",
      "  Batch 5,720  of  6,000.  Loss 0.0017  Elapsed: 0:00:55.521136.\n",
      "  Batch 5,760  of  6,000.  Loss 0.0016  Elapsed: 0:00:55.910630.\n",
      "  Batch 5,800  of  6,000.  Loss 0.0020  Elapsed: 0:00:56.301545.\n",
      "  Batch 5,840  of  6,000.  Loss 0.0018  Elapsed: 0:00:56.688641.\n",
      "  Batch 5,880  of  6,000.  Loss 0.0016  Elapsed: 0:00:57.077236.\n",
      "  Batch 5,920  of  6,000.  Loss 0.0171  Elapsed: 0:00:57.467262.\n",
      "  Batch 5,960  of  6,000.  Loss 0.0078  Elapsed: 0:00:57.853357.\n",
      "Avg Validation Loss 0.0165, Completed in 0:00:58.231839 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.984423\n",
      "F1 Score (micro) =   0.984464\n",
      "F1 Score (macro) =   0.966140\n",
      "F1 Score (samples) =   0.981500\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0015  Elapsed: 0:00:00.399974.\n",
      "  Batch    80  of  1,477.  Loss 0.0028  Elapsed: 0:00:00.787079.\n",
      "  Batch   120  of  1,477.  Loss 0.0071  Elapsed: 0:00:01.176591.\n",
      "  Batch   160  of  1,477.  Loss 0.0019  Elapsed: 0:00:01.565462.\n",
      "  Batch   200  of  1,477.  Loss 0.0020  Elapsed: 0:00:01.954089.\n",
      "  Batch   240  of  1,477.  Loss 0.0022  Elapsed: 0:00:02.342031.\n",
      "  Batch   280  of  1,477.  Loss 0.0209  Elapsed: 0:00:02.729990.\n",
      "  Batch   320  of  1,477.  Loss 0.2105  Elapsed: 0:00:03.120493.\n",
      "  Batch   360  of  1,477.  Loss 0.0020  Elapsed: 0:00:03.509756.\n",
      "  Batch   400  of  1,477.  Loss 0.0026  Elapsed: 0:00:03.896722.\n",
      "  Batch   440  of  1,477.  Loss 0.9366  Elapsed: 0:00:04.283909.\n",
      "  Batch   480  of  1,477.  Loss 0.0014  Elapsed: 0:00:04.674857.\n",
      "  Batch   520  of  1,477.  Loss 0.2220  Elapsed: 0:00:05.063471.\n",
      "  Batch   560  of  1,477.  Loss 0.0097  Elapsed: 0:00:05.450953.\n",
      "  Batch   600  of  1,477.  Loss 0.0016  Elapsed: 0:00:05.839924.\n",
      "  Batch   640  of  1,477.  Loss 0.1183  Elapsed: 0:00:06.229744.\n",
      "  Batch   680  of  1,477.  Loss 0.0043  Elapsed: 0:00:06.615550.\n",
      "  Batch   720  of  1,477.  Loss 0.0026  Elapsed: 0:00:07.002485.\n",
      "  Batch   760  of  1,477.  Loss 0.0086  Elapsed: 0:00:07.390383.\n",
      "  Batch   800  of  1,477.  Loss 0.0042  Elapsed: 0:00:07.777692.\n",
      "  Batch   840  of  1,477.  Loss 0.3011  Elapsed: 0:00:08.165243.\n",
      "  Batch   880  of  1,477.  Loss 0.1175  Elapsed: 0:00:08.553422.\n",
      "  Batch   920  of  1,477.  Loss 0.0091  Elapsed: 0:00:08.940248.\n",
      "  Batch   960  of  1,477.  Loss 0.2257  Elapsed: 0:00:09.326438.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0038  Elapsed: 0:00:09.713168.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0406  Elapsed: 0:00:10.102125.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0037  Elapsed: 0:00:10.489275.\n",
      "  Batch 1,120  of  1,477.  Loss 0.1226  Elapsed: 0:00:10.876501.\n",
      "  Batch 1,160  of  1,477.  Loss 0.4055  Elapsed: 0:00:11.263772.\n",
      "  Batch 1,200  of  1,477.  Loss 0.8148  Elapsed: 0:00:11.650336.\n",
      "  Batch 1,240  of  1,477.  Loss 1.2228  Elapsed: 0:00:12.039346.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0229  Elapsed: 0:00:12.429841.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0904  Elapsed: 0:00:12.819411.\n",
      "  Batch 1,360  of  1,477.  Loss 0.6661  Elapsed: 0:00:13.208175.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0040  Elapsed: 0:00:13.595251.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0016  Elapsed: 0:00:13.983182.\n",
      "Avg Validation Loss 0.2406, Completed in 0:00:14.331751 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.745486\n",
      "F1 Score (micro) =   0.749827\n",
      "F1 Score (macro) =   0.621843\n",
      "F1 Score (samples) =   0.730761\n",
      "Running Experiment GEMB_7000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    438.  Loss 0.2469  Elapsed: 0:00:25.113866.\n",
      "Epoch  0  Batch   200  of    438.  Loss 0.1981  Elapsed: 0:00:50.120916.\n",
      "Epoch  0  Batch   300  of    438.  Loss 0.2216  Elapsed: 0:01:15.128858.\n",
      "Epoch  0  Batch   400  of    438.  Loss 0.1945  Elapsed: 0:01:40.138610.\n",
      "Avg Training Loss 0.2331, Completed in 0:01:49.391202 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2153  Elapsed: 0:00:03.721123.\n",
      "  Batch    80  of     93.  Loss 0.2357  Elapsed: 0:00:07.319711.\n",
      "Avg Validation Loss 0.2509, Completed in 0:00:08.343711 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.557823\n",
      "F1 Score (micro) =   0.630267\n",
      "F1 Score (macro) =   0.345602\n",
      "F1 Score (samples) =   0.581584\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    438.  Loss 0.1844  Elapsed: 0:00:25.106645.\n",
      "Epoch  1  Batch   200  of    438.  Loss 0.2484  Elapsed: 0:00:50.116703.\n",
      "Epoch  1  Batch   300  of    438.  Loss 0.1975  Elapsed: 0:01:15.128820.\n",
      "Epoch  1  Batch   400  of    438.  Loss 0.1607  Elapsed: 0:01:40.138515.\n",
      "Avg Training Loss 0.1697, Completed in 0:01:49.390859 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.0740  Elapsed: 0:00:03.715663.\n",
      "  Batch    80  of     93.  Loss 0.2716  Elapsed: 0:00:07.320168.\n",
      "Avg Validation Loss 0.1934, Completed in 0:00:08.339994 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.696305\n",
      "F1 Score (micro) =   0.720673\n",
      "F1 Score (macro) =   0.531000\n",
      "F1 Score (samples) =   0.638456\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    438.  Loss 0.1640  Elapsed: 0:00:25.105914.\n",
      "Epoch  2  Batch   200  of    438.  Loss 0.1456  Elapsed: 0:00:50.114516.\n",
      "Epoch  2  Batch   300  of    438.  Loss 0.0352  Elapsed: 0:01:15.119660.\n",
      "Epoch  2  Batch   400  of    438.  Loss 0.1190  Elapsed: 0:01:40.127289.\n",
      "Avg Training Loss 0.1014, Completed in 0:01:49.379912 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3958  Elapsed: 0:00:03.725490.\n",
      "  Batch    80  of     93.  Loss 0.3139  Elapsed: 0:00:07.320034.\n",
      "Avg Validation Loss 0.1949, Completed in 0:00:08.340514 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.769855\n",
      "F1 Score (micro) =   0.779979\n",
      "F1 Score (macro) =   0.665588\n",
      "F1 Score (samples) =   0.758294\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    438.  Loss 0.0303  Elapsed: 0:00:25.105973.\n",
      "Epoch  3  Batch   200  of    438.  Loss 0.1393  Elapsed: 0:00:50.117567.\n",
      "Epoch  3  Batch   300  of    438.  Loss 0.0428  Elapsed: 0:01:15.124494.\n",
      "Epoch  3  Batch   400  of    438.  Loss 0.0413  Elapsed: 0:01:40.130348.\n",
      "Avg Training Loss 0.0530, Completed in 0:01:49.382713 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1880  Elapsed: 0:00:03.721616.\n",
      "  Batch    80  of     93.  Loss 0.1628  Elapsed: 0:00:07.324106.\n",
      "Avg Validation Loss 0.2130, Completed in 0:00:08.339864 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.778543\n",
      "F1 Score (micro) =   0.780152\n",
      "F1 Score (macro) =   0.690012\n",
      "F1 Score (samples) =   0.764162\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  7,000.  Loss 0.0035  Elapsed: 0:00:00.403660.\n",
      "  Batch    80  of  7,000.  Loss 0.0236  Elapsed: 0:00:00.790742.\n",
      "  Batch   120  of  7,000.  Loss 0.0158  Elapsed: 0:00:01.176105.\n",
      "  Batch   160  of  7,000.  Loss 0.0021  Elapsed: 0:00:01.562812.\n",
      "  Batch   200  of  7,000.  Loss 0.0083  Elapsed: 0:00:01.951350.\n",
      "  Batch   240  of  7,000.  Loss 0.0014  Elapsed: 0:00:02.338997.\n",
      "  Batch   280  of  7,000.  Loss 0.0052  Elapsed: 0:00:02.724490.\n",
      "  Batch   320  of  7,000.  Loss 0.0018  Elapsed: 0:00:03.117568.\n",
      "  Batch   360  of  7,000.  Loss 0.0074  Elapsed: 0:00:03.503196.\n",
      "  Batch   400  of  7,000.  Loss 0.0064  Elapsed: 0:00:03.890378.\n",
      "  Batch   440  of  7,000.  Loss 0.0823  Elapsed: 0:00:04.277403.\n",
      "  Batch   480  of  7,000.  Loss 0.0015  Elapsed: 0:00:04.662900.\n",
      "  Batch   520  of  7,000.  Loss 0.0014  Elapsed: 0:00:05.048351.\n",
      "  Batch   560  of  7,000.  Loss 0.0014  Elapsed: 0:00:05.436503.\n",
      "  Batch   600  of  7,000.  Loss 0.1829  Elapsed: 0:00:05.824075.\n",
      "  Batch   640  of  7,000.  Loss 0.0177  Elapsed: 0:00:06.210980.\n",
      "  Batch   680  of  7,000.  Loss 0.0020  Elapsed: 0:00:06.598573.\n",
      "  Batch   720  of  7,000.  Loss 0.0068  Elapsed: 0:00:06.983880.\n",
      "  Batch   760  of  7,000.  Loss 0.0080  Elapsed: 0:00:07.369777.\n",
      "  Batch   800  of  7,000.  Loss 0.0015  Elapsed: 0:00:07.755592.\n",
      "  Batch   840  of  7,000.  Loss 0.0047  Elapsed: 0:00:08.141422.\n",
      "  Batch   880  of  7,000.  Loss 0.0015  Elapsed: 0:00:08.528416.\n",
      "  Batch   920  of  7,000.  Loss 0.4783  Elapsed: 0:00:08.917284.\n",
      "  Batch   960  of  7,000.  Loss 0.0014  Elapsed: 0:00:09.304156.\n",
      "  Batch 1,000  of  7,000.  Loss 0.0019  Elapsed: 0:00:09.689851.\n",
      "  Batch 1,040  of  7,000.  Loss 0.0353  Elapsed: 0:00:10.076149.\n",
      "  Batch 1,080  of  7,000.  Loss 0.0733  Elapsed: 0:00:10.463398.\n",
      "  Batch 1,120  of  7,000.  Loss 0.0107  Elapsed: 0:00:10.849055.\n",
      "  Batch 1,160  of  7,000.  Loss 0.2453  Elapsed: 0:00:11.237016.\n",
      "  Batch 1,200  of  7,000.  Loss 0.0041  Elapsed: 0:00:11.623967.\n",
      "  Batch 1,240  of  7,000.  Loss 0.0031  Elapsed: 0:00:12.012861.\n",
      "  Batch 1,280  of  7,000.  Loss 0.0014  Elapsed: 0:00:12.398924.\n",
      "  Batch 1,320  of  7,000.  Loss 0.0160  Elapsed: 0:00:12.788747.\n",
      "  Batch 1,360  of  7,000.  Loss 0.0121  Elapsed: 0:00:13.175070.\n",
      "  Batch 1,400  of  7,000.  Loss 0.0014  Elapsed: 0:00:13.561857.\n",
      "  Batch 1,440  of  7,000.  Loss 0.0038  Elapsed: 0:00:13.948545.\n",
      "  Batch 1,480  of  7,000.  Loss 0.0097  Elapsed: 0:00:14.338299.\n",
      "  Batch 1,520  of  7,000.  Loss 0.0013  Elapsed: 0:00:14.724648.\n",
      "  Batch 1,560  of  7,000.  Loss 0.0014  Elapsed: 0:00:15.109727.\n",
      "  Batch 1,600  of  7,000.  Loss 0.0015  Elapsed: 0:00:15.495608.\n",
      "  Batch 1,640  of  7,000.  Loss 0.0024  Elapsed: 0:00:15.883013.\n",
      "  Batch 1,680  of  7,000.  Loss 0.0089  Elapsed: 0:00:16.269031.\n",
      "  Batch 1,720  of  7,000.  Loss 0.0019  Elapsed: 0:00:16.654561.\n",
      "  Batch 1,760  of  7,000.  Loss 0.0015  Elapsed: 0:00:17.043585.\n",
      "  Batch 1,800  of  7,000.  Loss 0.0019  Elapsed: 0:00:17.430309.\n",
      "  Batch 1,840  of  7,000.  Loss 0.0200  Elapsed: 0:00:17.814802.\n",
      "  Batch 1,880  of  7,000.  Loss 0.0022  Elapsed: 0:00:18.202703.\n",
      "  Batch 1,920  of  7,000.  Loss 0.0030  Elapsed: 0:00:18.589519.\n",
      "  Batch 1,960  of  7,000.  Loss 0.0146  Elapsed: 0:00:18.975677.\n",
      "  Batch 2,000  of  7,000.  Loss 0.0093  Elapsed: 0:00:19.364343.\n",
      "  Batch 2,040  of  7,000.  Loss 0.0027  Elapsed: 0:00:19.751210.\n",
      "  Batch 2,080  of  7,000.  Loss 0.0083  Elapsed: 0:00:20.136673.\n",
      "  Batch 2,120  of  7,000.  Loss 0.0025  Elapsed: 0:00:20.523588.\n",
      "  Batch 2,160  of  7,000.  Loss 0.0060  Elapsed: 0:00:20.910331.\n",
      "  Batch 2,200  of  7,000.  Loss 0.0018  Elapsed: 0:00:21.298315.\n",
      "  Batch 2,240  of  7,000.  Loss 0.0037  Elapsed: 0:00:21.683086.\n",
      "  Batch 2,280  of  7,000.  Loss 0.0015  Elapsed: 0:00:22.069886.\n",
      "  Batch 2,320  of  7,000.  Loss 0.0090  Elapsed: 0:00:22.458289.\n",
      "  Batch 2,360  of  7,000.  Loss 0.0050  Elapsed: 0:00:22.843408.\n",
      "  Batch 2,400  of  7,000.  Loss 0.0029  Elapsed: 0:00:23.228652.\n",
      "  Batch 2,440  of  7,000.  Loss 0.0023  Elapsed: 0:00:23.615980.\n",
      "  Batch 2,480  of  7,000.  Loss 0.0105  Elapsed: 0:00:24.002578.\n",
      "  Batch 2,520  of  7,000.  Loss 0.0016  Elapsed: 0:00:24.390982.\n",
      "  Batch 2,560  of  7,000.  Loss 0.0346  Elapsed: 0:00:24.780280.\n",
      "  Batch 2,600  of  7,000.  Loss 0.0026  Elapsed: 0:00:25.168654.\n",
      "  Batch 2,640  of  7,000.  Loss 0.0017  Elapsed: 0:00:25.556002.\n",
      "  Batch 2,680  of  7,000.  Loss 0.0015  Elapsed: 0:00:25.941444.\n",
      "  Batch 2,720  of  7,000.  Loss 0.0068  Elapsed: 0:00:26.329807.\n",
      "  Batch 2,760  of  7,000.  Loss 0.0116  Elapsed: 0:00:26.717914.\n",
      "  Batch 2,800  of  7,000.  Loss 0.0087  Elapsed: 0:00:27.101977.\n",
      "  Batch 2,840  of  7,000.  Loss 0.0017  Elapsed: 0:00:27.488165.\n",
      "  Batch 2,880  of  7,000.  Loss 0.0017  Elapsed: 0:00:27.874871.\n",
      "  Batch 2,920  of  7,000.  Loss 0.0067  Elapsed: 0:00:28.263825.\n",
      "  Batch 2,960  of  7,000.  Loss 0.0025  Elapsed: 0:00:28.649656.\n",
      "  Batch 3,000  of  7,000.  Loss 0.0023  Elapsed: 0:00:29.037272.\n",
      "  Batch 3,040  of  7,000.  Loss 0.0022  Elapsed: 0:00:29.421253.\n",
      "  Batch 3,080  of  7,000.  Loss 0.0137  Elapsed: 0:00:29.812717.\n",
      "  Batch 3,120  of  7,000.  Loss 0.0017  Elapsed: 0:00:30.199000.\n",
      "  Batch 3,160  of  7,000.  Loss 0.0025  Elapsed: 0:00:30.586156.\n",
      "  Batch 3,200  of  7,000.  Loss 0.0023  Elapsed: 0:00:30.972365.\n",
      "  Batch 3,240  of  7,000.  Loss 0.0082  Elapsed: 0:00:31.360807.\n",
      "  Batch 3,280  of  7,000.  Loss 0.0020  Elapsed: 0:00:31.748956.\n",
      "  Batch 3,320  of  7,000.  Loss 0.0025  Elapsed: 0:00:32.134929.\n",
      "  Batch 3,360  of  7,000.  Loss 0.1489  Elapsed: 0:00:32.522284.\n",
      "  Batch 3,400  of  7,000.  Loss 0.0016  Elapsed: 0:00:32.909212.\n",
      "  Batch 3,440  of  7,000.  Loss 0.0335  Elapsed: 0:00:33.295945.\n",
      "  Batch 3,480  of  7,000.  Loss 0.0050  Elapsed: 0:00:33.682281.\n",
      "  Batch 3,520  of  7,000.  Loss 0.0096  Elapsed: 0:00:34.068290.\n",
      "  Batch 3,560  of  7,000.  Loss 0.0015  Elapsed: 0:00:34.453822.\n",
      "  Batch 3,600  of  7,000.  Loss 0.0024  Elapsed: 0:00:34.840482.\n",
      "  Batch 3,640  of  7,000.  Loss 0.0021  Elapsed: 0:00:35.230637.\n",
      "  Batch 3,680  of  7,000.  Loss 0.0018  Elapsed: 0:00:35.618659.\n",
      "  Batch 3,720  of  7,000.  Loss 0.0027  Elapsed: 0:00:36.007745.\n",
      "  Batch 3,760  of  7,000.  Loss 0.0209  Elapsed: 0:00:36.395686.\n",
      "  Batch 3,800  of  7,000.  Loss 0.0015  Elapsed: 0:00:36.781056.\n",
      "  Batch 3,840  of  7,000.  Loss 0.0528  Elapsed: 0:00:37.170825.\n",
      "  Batch 3,880  of  7,000.  Loss 0.0127  Elapsed: 0:00:37.557937.\n",
      "  Batch 3,920  of  7,000.  Loss 0.0016  Elapsed: 0:00:37.947268.\n",
      "  Batch 3,960  of  7,000.  Loss 0.0015  Elapsed: 0:00:38.333061.\n",
      "  Batch 4,000  of  7,000.  Loss 0.0054  Elapsed: 0:00:38.719071.\n",
      "  Batch 4,040  of  7,000.  Loss 0.0019  Elapsed: 0:00:39.105813.\n",
      "  Batch 4,080  of  7,000.  Loss 0.0016  Elapsed: 0:00:39.492620.\n",
      "  Batch 4,120  of  7,000.  Loss 0.0024  Elapsed: 0:00:39.880071.\n",
      "  Batch 4,160  of  7,000.  Loss 0.0072  Elapsed: 0:00:40.268169.\n",
      "  Batch 4,200  of  7,000.  Loss 0.0081  Elapsed: 0:00:40.656364.\n",
      "  Batch 4,240  of  7,000.  Loss 0.0413  Elapsed: 0:00:41.044420.\n",
      "  Batch 4,280  of  7,000.  Loss 0.0014  Elapsed: 0:00:41.433517.\n",
      "  Batch 4,320  of  7,000.  Loss 0.0688  Elapsed: 0:00:41.818546.\n",
      "  Batch 4,360  of  7,000.  Loss 0.0080  Elapsed: 0:00:42.204811.\n",
      "  Batch 4,400  of  7,000.  Loss 0.0015  Elapsed: 0:00:42.592500.\n",
      "  Batch 4,440  of  7,000.  Loss 0.0099  Elapsed: 0:00:42.979913.\n",
      "  Batch 4,480  of  7,000.  Loss 0.0015  Elapsed: 0:00:43.369341.\n",
      "  Batch 4,520  of  7,000.  Loss 0.0020  Elapsed: 0:00:43.755166.\n",
      "  Batch 4,560  of  7,000.  Loss 0.0255  Elapsed: 0:00:44.141758.\n",
      "  Batch 4,600  of  7,000.  Loss 0.0028  Elapsed: 0:00:44.528654.\n",
      "  Batch 4,640  of  7,000.  Loss 0.0061  Elapsed: 0:00:44.915219.\n",
      "  Batch 4,680  of  7,000.  Loss 0.0024  Elapsed: 0:00:45.301182.\n",
      "  Batch 4,720  of  7,000.  Loss 0.0019  Elapsed: 0:00:45.688846.\n",
      "  Batch 4,760  of  7,000.  Loss 0.0508  Elapsed: 0:00:46.075847.\n",
      "  Batch 4,800  of  7,000.  Loss 0.0020  Elapsed: 0:00:46.463920.\n",
      "  Batch 4,840  of  7,000.  Loss 0.0033  Elapsed: 0:00:46.850344.\n",
      "  Batch 4,880  of  7,000.  Loss 0.0017  Elapsed: 0:00:47.238166.\n",
      "  Batch 4,920  of  7,000.  Loss 0.0018  Elapsed: 0:00:47.625871.\n",
      "  Batch 4,960  of  7,000.  Loss 0.0015  Elapsed: 0:00:48.013154.\n",
      "  Batch 5,000  of  7,000.  Loss 0.0015  Elapsed: 0:00:48.400969.\n",
      "  Batch 5,040  of  7,000.  Loss 0.0021  Elapsed: 0:00:48.786544.\n",
      "  Batch 5,080  of  7,000.  Loss 0.0024  Elapsed: 0:00:49.173692.\n",
      "  Batch 5,120  of  7,000.  Loss 0.0024  Elapsed: 0:00:49.560415.\n",
      "  Batch 5,160  of  7,000.  Loss 0.0020  Elapsed: 0:00:49.946593.\n",
      "  Batch 5,200  of  7,000.  Loss 0.0016  Elapsed: 0:00:50.334377.\n",
      "  Batch 5,240  of  7,000.  Loss 0.0029  Elapsed: 0:00:50.722380.\n",
      "  Batch 5,280  of  7,000.  Loss 0.0040  Elapsed: 0:00:51.109972.\n",
      "  Batch 5,320  of  7,000.  Loss 0.0184  Elapsed: 0:00:51.497221.\n",
      "  Batch 5,360  of  7,000.  Loss 0.0034  Elapsed: 0:00:51.883056.\n",
      "  Batch 5,400  of  7,000.  Loss 0.0076  Elapsed: 0:00:52.271670.\n",
      "  Batch 5,440  of  7,000.  Loss 0.0149  Elapsed: 0:00:52.658379.\n",
      "  Batch 5,480  of  7,000.  Loss 0.0014  Elapsed: 0:00:53.045492.\n",
      "  Batch 5,520  of  7,000.  Loss 0.0219  Elapsed: 0:00:53.431717.\n",
      "  Batch 5,560  of  7,000.  Loss 0.0059  Elapsed: 0:00:53.817094.\n",
      "  Batch 5,600  of  7,000.  Loss 0.0021  Elapsed: 0:00:54.202092.\n",
      "  Batch 5,640  of  7,000.  Loss 0.0019  Elapsed: 0:00:54.588493.\n",
      "  Batch 5,680  of  7,000.  Loss 0.0015  Elapsed: 0:00:54.975822.\n",
      "  Batch 5,720  of  7,000.  Loss 0.0029  Elapsed: 0:00:55.362016.\n",
      "  Batch 5,760  of  7,000.  Loss 0.0128  Elapsed: 0:00:55.747096.\n",
      "  Batch 5,800  of  7,000.  Loss 0.0014  Elapsed: 0:00:56.137280.\n",
      "  Batch 5,840  of  7,000.  Loss 0.0025  Elapsed: 0:00:56.523591.\n",
      "  Batch 5,880  of  7,000.  Loss 0.0358  Elapsed: 0:00:56.910474.\n",
      "  Batch 5,920  of  7,000.  Loss 0.0016  Elapsed: 0:00:57.296014.\n",
      "  Batch 5,960  of  7,000.  Loss 0.0139  Elapsed: 0:00:57.682927.\n",
      "  Batch 6,000  of  7,000.  Loss 0.0043  Elapsed: 0:00:58.070774.\n",
      "  Batch 6,040  of  7,000.  Loss 0.0050  Elapsed: 0:00:58.455154.\n",
      "  Batch 6,080  of  7,000.  Loss 0.0013  Elapsed: 0:00:58.841234.\n",
      "  Batch 6,120  of  7,000.  Loss 0.0020  Elapsed: 0:00:59.227504.\n",
      "  Batch 6,160  of  7,000.  Loss 0.0024  Elapsed: 0:00:59.612893.\n",
      "  Batch 6,200  of  7,000.  Loss 0.0269  Elapsed: 0:01:00.002393.\n",
      "  Batch 6,240  of  7,000.  Loss 0.0138  Elapsed: 0:01:00.391263.\n",
      "  Batch 6,280  of  7,000.  Loss 0.0015  Elapsed: 0:01:00.777974.\n",
      "  Batch 6,320  of  7,000.  Loss 0.0021  Elapsed: 0:01:01.165836.\n",
      "  Batch 6,360  of  7,000.  Loss 0.0900  Elapsed: 0:01:01.550018.\n",
      "  Batch 6,400  of  7,000.  Loss 0.0138  Elapsed: 0:01:01.935706.\n",
      "  Batch 6,440  of  7,000.  Loss 0.0017  Elapsed: 0:01:02.323200.\n",
      "  Batch 6,480  of  7,000.  Loss 0.0018  Elapsed: 0:01:02.708314.\n",
      "  Batch 6,520  of  7,000.  Loss 0.0015  Elapsed: 0:01:03.095373.\n",
      "  Batch 6,560  of  7,000.  Loss 0.0033  Elapsed: 0:01:03.482167.\n",
      "  Batch 6,600  of  7,000.  Loss 0.0023  Elapsed: 0:01:03.865672.\n",
      "  Batch 6,640  of  7,000.  Loss 0.0093  Elapsed: 0:01:04.252559.\n",
      "  Batch 6,680  of  7,000.  Loss 0.0019  Elapsed: 0:01:04.640671.\n",
      "  Batch 6,720  of  7,000.  Loss 0.0016  Elapsed: 0:01:05.027637.\n",
      "  Batch 6,760  of  7,000.  Loss 0.0025  Elapsed: 0:01:05.413706.\n",
      "  Batch 6,800  of  7,000.  Loss 0.0086  Elapsed: 0:01:05.799742.\n",
      "  Batch 6,840  of  7,000.  Loss 0.0084  Elapsed: 0:01:06.187681.\n",
      "  Batch 6,880  of  7,000.  Loss 0.0044  Elapsed: 0:01:06.573931.\n",
      "  Batch 6,920  of  7,000.  Loss 0.7859  Elapsed: 0:01:06.959771.\n",
      "  Batch 6,960  of  7,000.  Loss 0.0050  Elapsed: 0:01:07.347184.\n",
      "Avg Validation Loss 0.0179, Completed in 0:01:07.726044 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.983495\n",
      "F1 Score (micro) =   0.983482\n",
      "F1 Score (macro) =   0.965991\n",
      "F1 Score (samples) =   0.981524\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0075  Elapsed: 0:00:00.400222.\n",
      "  Batch    80  of  1,477.  Loss 0.0129  Elapsed: 0:00:00.787033.\n",
      "  Batch   120  of  1,477.  Loss 0.0014  Elapsed: 0:00:01.173681.\n",
      "  Batch   160  of  1,477.  Loss 0.0084  Elapsed: 0:00:01.559777.\n",
      "  Batch   200  of  1,477.  Loss 0.0017  Elapsed: 0:00:01.947799.\n",
      "  Batch   240  of  1,477.  Loss 0.0013  Elapsed: 0:00:02.334196.\n",
      "  Batch   280  of  1,477.  Loss 0.0143  Elapsed: 0:00:02.720991.\n",
      "  Batch   320  of  1,477.  Loss 0.0112  Elapsed: 0:00:03.107567.\n",
      "  Batch   360  of  1,477.  Loss 0.0853  Elapsed: 0:00:03.492371.\n",
      "  Batch   400  of  1,477.  Loss 0.0017  Elapsed: 0:00:03.879879.\n",
      "  Batch   440  of  1,477.  Loss 0.0120  Elapsed: 0:00:04.263312.\n",
      "  Batch   480  of  1,477.  Loss 0.0130  Elapsed: 0:00:04.650092.\n",
      "  Batch   520  of  1,477.  Loss 0.0083  Elapsed: 0:00:05.037934.\n",
      "  Batch   560  of  1,477.  Loss 0.0014  Elapsed: 0:00:05.422419.\n",
      "  Batch   600  of  1,477.  Loss 1.1390  Elapsed: 0:00:05.808278.\n",
      "  Batch   640  of  1,477.  Loss 0.0118  Elapsed: 0:00:06.193779.\n",
      "  Batch   680  of  1,477.  Loss 0.0070  Elapsed: 0:00:06.579027.\n",
      "  Batch   720  of  1,477.  Loss 0.1733  Elapsed: 0:00:06.966038.\n",
      "  Batch   760  of  1,477.  Loss 0.0221  Elapsed: 0:00:07.353664.\n",
      "  Batch   800  of  1,477.  Loss 0.0025  Elapsed: 0:00:07.740934.\n",
      "  Batch   840  of  1,477.  Loss 0.0080  Elapsed: 0:00:08.126455.\n",
      "  Batch   880  of  1,477.  Loss 0.7284  Elapsed: 0:00:08.512898.\n",
      "  Batch   920  of  1,477.  Loss 0.0025  Elapsed: 0:00:08.898874.\n",
      "  Batch   960  of  1,477.  Loss 0.0023  Elapsed: 0:00:09.287352.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0114  Elapsed: 0:00:09.672725.\n",
      "  Batch 1,040  of  1,477.  Loss 1.1304  Elapsed: 0:00:10.057907.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0038  Elapsed: 0:00:10.446775.\n",
      "  Batch 1,120  of  1,477.  Loss 0.4898  Elapsed: 0:00:10.835078.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0076  Elapsed: 0:00:11.222260.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0016  Elapsed: 0:00:11.608085.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0497  Elapsed: 0:00:11.994680.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0015  Elapsed: 0:00:12.378580.\n",
      "  Batch 1,320  of  1,477.  Loss 1.2218  Elapsed: 0:00:12.767125.\n",
      "  Batch 1,360  of  1,477.  Loss 0.7206  Elapsed: 0:00:13.153961.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0018  Elapsed: 0:00:13.540022.\n",
      "  Batch 1,440  of  1,477.  Loss 0.3023  Elapsed: 0:00:13.929685.\n",
      "Avg Validation Loss 0.2107, Completed in 0:00:14.278248 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.781954\n",
      "F1 Score (micro) =   0.783270\n",
      "F1 Score (macro) =   0.675617\n",
      "F1 Score (samples) =   0.765741\n",
      "Running Experiment GEMB_8000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    500.  Loss 0.3193  Elapsed: 0:00:25.104120.\n",
      "Epoch  0  Batch   200  of    500.  Loss 0.1099  Elapsed: 0:00:50.120245.\n",
      "Epoch  0  Batch   300  of    500.  Loss 0.1577  Elapsed: 0:01:15.127813.\n",
      "Epoch  0  Batch   400  of    500.  Loss 0.2144  Elapsed: 0:01:40.132797.\n",
      "Avg Training Loss 0.2292, Completed in 0:02:04.969103 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3134  Elapsed: 0:00:03.769805.\n",
      "  Batch    80  of     93.  Loss 0.2001  Elapsed: 0:00:07.377205.\n",
      "Avg Validation Loss 0.2383, Completed in 0:00:08.393831 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.560377\n",
      "F1 Score (micro) =   0.621094\n",
      "F1 Score (macro) =   0.345202\n",
      "F1 Score (samples) =   0.538253\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    500.  Loss 0.1874  Elapsed: 0:00:25.108636.\n",
      "Epoch  1  Batch   200  of    500.  Loss 0.1748  Elapsed: 0:00:50.115059.\n",
      "Epoch  1  Batch   300  of    500.  Loss 0.1467  Elapsed: 0:01:15.123689.\n",
      "Epoch  1  Batch   400  of    500.  Loss 0.1350  Elapsed: 0:01:40.133942.\n",
      "Avg Training Loss 0.1726, Completed in 0:02:04.971299 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2340  Elapsed: 0:00:03.759003.\n",
      "  Batch    80  of     93.  Loss 0.3057  Elapsed: 0:00:07.359211.\n",
      "Avg Validation Loss 0.1844, Completed in 0:00:08.377787 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.719830\n",
      "F1 Score (micro) =   0.740549\n",
      "F1 Score (macro) =   0.559886\n",
      "F1 Score (samples) =   0.675920\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    500.  Loss 0.1228  Elapsed: 0:00:25.099245.\n",
      "Epoch  2  Batch   200  of    500.  Loss 0.1491  Elapsed: 0:00:50.111534.\n",
      "Epoch  2  Batch   300  of    500.  Loss 0.1063  Elapsed: 0:01:15.116587.\n",
      "Epoch  2  Batch   400  of    500.  Loss 0.0639  Elapsed: 0:01:40.123299.\n",
      "Avg Training Loss 0.1040, Completed in 0:02:04.956094 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.1695  Elapsed: 0:00:03.760994.\n",
      "  Batch    80  of     93.  Loss 0.1070  Elapsed: 0:00:07.364157.\n",
      "Avg Validation Loss 0.1815, Completed in 0:00:08.375066 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.769272\n",
      "F1 Score (micro) =   0.779733\n",
      "F1 Score (macro) =   0.695521\n",
      "F1 Score (samples) =   0.748589\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    500.  Loss 0.0633  Elapsed: 0:00:25.101377.\n",
      "Epoch  3  Batch   200  of    500.  Loss 0.0929  Elapsed: 0:00:50.106887.\n",
      "Epoch  3  Batch   300  of    500.  Loss 0.1248  Elapsed: 0:01:15.115065.\n",
      "Epoch  3  Batch   400  of    500.  Loss 0.0470  Elapsed: 0:01:40.123054.\n",
      "Avg Training Loss 0.0570, Completed in 0:02:04.960463 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.0941  Elapsed: 0:00:03.763845.\n",
      "  Batch    80  of     93.  Loss 0.0929  Elapsed: 0:00:07.358370.\n",
      "Avg Validation Loss 0.1905, Completed in 0:00:08.383186 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.799115\n",
      "F1 Score (micro) =   0.801521\n",
      "F1 Score (macro) =   0.734911\n",
      "F1 Score (samples) =   0.783119\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  8,000.  Loss 0.0020  Elapsed: 0:00:00.396714.\n",
      "  Batch    80  of  8,000.  Loss 0.0020  Elapsed: 0:00:00.783573.\n",
      "  Batch   120  of  8,000.  Loss 0.0014  Elapsed: 0:00:01.170515.\n",
      "  Batch   160  of  8,000.  Loss 0.0180  Elapsed: 0:00:01.556178.\n",
      "  Batch   200  of  8,000.  Loss 0.0017  Elapsed: 0:00:01.944009.\n",
      "  Batch   240  of  8,000.  Loss 0.0015  Elapsed: 0:00:02.329943.\n",
      "  Batch   280  of  8,000.  Loss 0.0014  Elapsed: 0:00:02.717529.\n",
      "  Batch   320  of  8,000.  Loss 0.0015  Elapsed: 0:00:03.106684.\n",
      "  Batch   360  of  8,000.  Loss 0.0046  Elapsed: 0:00:03.493209.\n",
      "  Batch   400  of  8,000.  Loss 0.0040  Elapsed: 0:00:03.878685.\n",
      "  Batch   440  of  8,000.  Loss 0.0050  Elapsed: 0:00:04.265467.\n",
      "  Batch   480  of  8,000.  Loss 0.0710  Elapsed: 0:00:04.653893.\n",
      "  Batch   520  of  8,000.  Loss 0.0021  Elapsed: 0:00:05.043137.\n",
      "  Batch   560  of  8,000.  Loss 0.0034  Elapsed: 0:00:05.429476.\n",
      "  Batch   600  of  8,000.  Loss 0.0013  Elapsed: 0:00:05.814471.\n",
      "  Batch   640  of  8,000.  Loss 0.0074  Elapsed: 0:00:06.201789.\n",
      "  Batch   680  of  8,000.  Loss 0.0015  Elapsed: 0:00:06.588689.\n",
      "  Batch   720  of  8,000.  Loss 0.0257  Elapsed: 0:00:06.974870.\n",
      "  Batch   760  of  8,000.  Loss 0.0025  Elapsed: 0:00:07.362455.\n",
      "  Batch   800  of  8,000.  Loss 0.0040  Elapsed: 0:00:07.748633.\n",
      "  Batch   840  of  8,000.  Loss 0.0090  Elapsed: 0:00:08.135634.\n",
      "  Batch   880  of  8,000.  Loss 0.0060  Elapsed: 0:00:08.522713.\n",
      "  Batch   920  of  8,000.  Loss 0.0057  Elapsed: 0:00:08.909638.\n",
      "  Batch   960  of  8,000.  Loss 0.0144  Elapsed: 0:00:09.294638.\n",
      "  Batch 1,000  of  8,000.  Loss 0.0013  Elapsed: 0:00:09.681618.\n",
      "  Batch 1,040  of  8,000.  Loss 0.0019  Elapsed: 0:00:10.066950.\n",
      "  Batch 1,080  of  8,000.  Loss 0.0014  Elapsed: 0:00:10.451479.\n",
      "  Batch 1,120  of  8,000.  Loss 0.0016  Elapsed: 0:00:10.838720.\n",
      "  Batch 1,160  of  8,000.  Loss 0.0021  Elapsed: 0:00:11.223506.\n",
      "  Batch 1,200  of  8,000.  Loss 0.0023  Elapsed: 0:00:11.609141.\n",
      "  Batch 1,240  of  8,000.  Loss 0.0186  Elapsed: 0:00:11.996510.\n",
      "  Batch 1,280  of  8,000.  Loss 0.0098  Elapsed: 0:00:12.383260.\n",
      "  Batch 1,320  of  8,000.  Loss 0.0024  Elapsed: 0:00:12.771602.\n",
      "  Batch 1,360  of  8,000.  Loss 0.0037  Elapsed: 0:00:13.158807.\n",
      "  Batch 1,400  of  8,000.  Loss 0.0027  Elapsed: 0:00:13.544174.\n",
      "  Batch 1,440  of  8,000.  Loss 0.0013  Elapsed: 0:00:13.929672.\n",
      "  Batch 1,480  of  8,000.  Loss 0.0060  Elapsed: 0:00:14.315606.\n",
      "  Batch 1,520  of  8,000.  Loss 0.0067  Elapsed: 0:00:14.705342.\n",
      "  Batch 1,560  of  8,000.  Loss 0.0096  Elapsed: 0:00:15.090631.\n",
      "  Batch 1,600  of  8,000.  Loss 0.0013  Elapsed: 0:00:15.477029.\n",
      "  Batch 1,640  of  8,000.  Loss 0.0023  Elapsed: 0:00:15.864443.\n",
      "  Batch 1,680  of  8,000.  Loss 0.0073  Elapsed: 0:00:16.250176.\n",
      "  Batch 1,720  of  8,000.  Loss 0.0027  Elapsed: 0:00:16.637814.\n",
      "  Batch 1,760  of  8,000.  Loss 0.0014  Elapsed: 0:00:17.024732.\n",
      "  Batch 1,800  of  8,000.  Loss 0.0019  Elapsed: 0:00:17.413009.\n",
      "  Batch 1,840  of  8,000.  Loss 0.6715  Elapsed: 0:00:17.799028.\n",
      "  Batch 1,880  of  8,000.  Loss 0.0533  Elapsed: 0:00:18.186217.\n",
      "  Batch 1,920  of  8,000.  Loss 0.1496  Elapsed: 0:00:18.572932.\n",
      "  Batch 1,960  of  8,000.  Loss 0.0073  Elapsed: 0:00:18.959171.\n",
      "  Batch 2,000  of  8,000.  Loss 0.0020  Elapsed: 0:00:19.345831.\n",
      "  Batch 2,040  of  8,000.  Loss 0.0064  Elapsed: 0:00:19.734498.\n",
      "  Batch 2,080  of  8,000.  Loss 0.0014  Elapsed: 0:00:20.122901.\n",
      "  Batch 2,120  of  8,000.  Loss 0.1597  Elapsed: 0:00:20.507751.\n",
      "  Batch 2,160  of  8,000.  Loss 0.0378  Elapsed: 0:00:20.895694.\n",
      "  Batch 2,200  of  8,000.  Loss 0.0017  Elapsed: 0:00:21.282715.\n",
      "  Batch 2,240  of  8,000.  Loss 0.0017  Elapsed: 0:00:21.669845.\n",
      "  Batch 2,280  of  8,000.  Loss 0.0014  Elapsed: 0:00:22.056638.\n",
      "  Batch 2,320  of  8,000.  Loss 0.0034  Elapsed: 0:00:22.442072.\n",
      "  Batch 2,360  of  8,000.  Loss 0.0063  Elapsed: 0:00:22.829169.\n",
      "  Batch 2,400  of  8,000.  Loss 0.0013  Elapsed: 0:00:23.216996.\n",
      "  Batch 2,440  of  8,000.  Loss 0.0028  Elapsed: 0:00:23.604167.\n",
      "  Batch 2,480  of  8,000.  Loss 0.0468  Elapsed: 0:00:23.988924.\n",
      "  Batch 2,520  of  8,000.  Loss 0.0102  Elapsed: 0:00:24.375053.\n",
      "  Batch 2,560  of  8,000.  Loss 0.0018  Elapsed: 0:00:24.762220.\n",
      "  Batch 2,600  of  8,000.  Loss 0.0013  Elapsed: 0:00:25.150113.\n",
      "  Batch 2,640  of  8,000.  Loss 0.0046  Elapsed: 0:00:25.538320.\n",
      "  Batch 2,680  of  8,000.  Loss 0.0129  Elapsed: 0:00:25.923701.\n",
      "  Batch 2,720  of  8,000.  Loss 0.0091  Elapsed: 0:00:26.309788.\n",
      "  Batch 2,760  of  8,000.  Loss 0.0020  Elapsed: 0:00:26.693469.\n",
      "  Batch 2,800  of  8,000.  Loss 0.0012  Elapsed: 0:00:27.077671.\n",
      "  Batch 2,840  of  8,000.  Loss 0.0021  Elapsed: 0:00:27.466162.\n",
      "  Batch 2,880  of  8,000.  Loss 0.0021  Elapsed: 0:00:27.851351.\n",
      "  Batch 2,920  of  8,000.  Loss 0.1515  Elapsed: 0:00:28.237865.\n",
      "  Batch 2,960  of  8,000.  Loss 0.0036  Elapsed: 0:00:28.625777.\n",
      "  Batch 3,000  of  8,000.  Loss 0.0130  Elapsed: 0:00:29.011855.\n",
      "  Batch 3,040  of  8,000.  Loss 0.0013  Elapsed: 0:00:29.398453.\n",
      "  Batch 3,080  of  8,000.  Loss 0.0099  Elapsed: 0:00:29.784346.\n",
      "  Batch 3,120  of  8,000.  Loss 0.0718  Elapsed: 0:00:30.171817.\n",
      "  Batch 3,160  of  8,000.  Loss 0.0082  Elapsed: 0:00:30.557104.\n",
      "  Batch 3,200  of  8,000.  Loss 0.0017  Elapsed: 0:00:30.943812.\n",
      "  Batch 3,240  of  8,000.  Loss 0.0522  Elapsed: 0:00:31.329667.\n",
      "  Batch 3,280  of  8,000.  Loss 0.0035  Elapsed: 0:00:31.716515.\n",
      "  Batch 3,320  of  8,000.  Loss 0.0013  Elapsed: 0:00:32.101312.\n",
      "  Batch 3,360  of  8,000.  Loss 0.0290  Elapsed: 0:00:32.487454.\n",
      "  Batch 3,400  of  8,000.  Loss 0.0174  Elapsed: 0:00:32.874301.\n",
      "  Batch 3,440  of  8,000.  Loss 0.0019  Elapsed: 0:00:33.260928.\n",
      "  Batch 3,480  of  8,000.  Loss 0.0013  Elapsed: 0:00:33.647433.\n",
      "  Batch 3,520  of  8,000.  Loss 0.0019  Elapsed: 0:00:34.032765.\n",
      "  Batch 3,560  of  8,000.  Loss 0.0145  Elapsed: 0:00:34.418628.\n",
      "  Batch 3,600  of  8,000.  Loss 0.0092  Elapsed: 0:00:34.807520.\n",
      "  Batch 3,640  of  8,000.  Loss 0.0214  Elapsed: 0:00:35.193788.\n",
      "  Batch 3,680  of  8,000.  Loss 0.0338  Elapsed: 0:00:35.581198.\n",
      "  Batch 3,720  of  8,000.  Loss 0.0020  Elapsed: 0:00:35.967666.\n",
      "  Batch 3,760  of  8,000.  Loss 0.0286  Elapsed: 0:00:36.354921.\n",
      "  Batch 3,800  of  8,000.  Loss 0.0119  Elapsed: 0:00:36.742249.\n",
      "  Batch 3,840  of  8,000.  Loss 0.0027  Elapsed: 0:00:37.129111.\n",
      "  Batch 3,880  of  8,000.  Loss 0.0016  Elapsed: 0:00:37.514643.\n",
      "  Batch 3,920  of  8,000.  Loss 0.0013  Elapsed: 0:00:37.899464.\n",
      "  Batch 3,960  of  8,000.  Loss 0.0273  Elapsed: 0:00:38.284569.\n",
      "  Batch 4,000  of  8,000.  Loss 0.0013  Elapsed: 0:00:38.671875.\n",
      "  Batch 4,040  of  8,000.  Loss 0.0017  Elapsed: 0:00:39.057592.\n",
      "  Batch 4,080  of  8,000.  Loss 0.0014  Elapsed: 0:00:39.445249.\n",
      "  Batch 4,120  of  8,000.  Loss 0.0013  Elapsed: 0:00:39.831678.\n",
      "  Batch 4,160  of  8,000.  Loss 0.0015  Elapsed: 0:00:40.217077.\n",
      "  Batch 4,200  of  8,000.  Loss 0.0271  Elapsed: 0:00:40.602327.\n",
      "  Batch 4,240  of  8,000.  Loss 0.0017  Elapsed: 0:00:40.989836.\n",
      "  Batch 4,280  of  8,000.  Loss 0.0019  Elapsed: 0:00:41.375232.\n",
      "  Batch 4,320  of  8,000.  Loss 0.0015  Elapsed: 0:00:41.762422.\n",
      "  Batch 4,360  of  8,000.  Loss 0.0144  Elapsed: 0:00:42.149434.\n",
      "  Batch 4,400  of  8,000.  Loss 0.0017  Elapsed: 0:00:42.537155.\n",
      "  Batch 4,440  of  8,000.  Loss 0.0019  Elapsed: 0:00:42.920838.\n",
      "  Batch 4,480  of  8,000.  Loss 0.0015  Elapsed: 0:00:43.305234.\n",
      "  Batch 4,520  of  8,000.  Loss 0.0079  Elapsed: 0:00:43.692706.\n",
      "  Batch 4,560  of  8,000.  Loss 0.0064  Elapsed: 0:00:44.079381.\n",
      "  Batch 4,600  of  8,000.  Loss 0.0015  Elapsed: 0:00:44.464757.\n",
      "  Batch 4,640  of  8,000.  Loss 0.0013  Elapsed: 0:00:44.850354.\n",
      "  Batch 4,680  of  8,000.  Loss 0.0015  Elapsed: 0:00:45.237662.\n",
      "  Batch 4,720  of  8,000.  Loss 0.0345  Elapsed: 0:00:45.624397.\n",
      "  Batch 4,760  of  8,000.  Loss 0.0977  Elapsed: 0:00:46.011471.\n",
      "  Batch 4,800  of  8,000.  Loss 0.0030  Elapsed: 0:00:46.397676.\n",
      "  Batch 4,840  of  8,000.  Loss 0.1498  Elapsed: 0:00:46.784704.\n",
      "  Batch 4,880  of  8,000.  Loss 0.0026  Elapsed: 0:00:47.172630.\n",
      "  Batch 4,920  of  8,000.  Loss 0.0109  Elapsed: 0:00:47.563349.\n",
      "  Batch 4,960  of  8,000.  Loss 0.0023  Elapsed: 0:00:47.950794.\n",
      "  Batch 5,000  of  8,000.  Loss 0.0040  Elapsed: 0:00:48.340384.\n",
      "  Batch 5,040  of  8,000.  Loss 0.0028  Elapsed: 0:00:48.725543.\n",
      "  Batch 5,080  of  8,000.  Loss 0.0014  Elapsed: 0:00:49.113472.\n",
      "  Batch 5,120  of  8,000.  Loss 0.0636  Elapsed: 0:00:49.501675.\n",
      "  Batch 5,160  of  8,000.  Loss 0.0075  Elapsed: 0:00:49.887534.\n",
      "  Batch 5,200  of  8,000.  Loss 0.0015  Elapsed: 0:00:50.274861.\n",
      "  Batch 5,240  of  8,000.  Loss 0.0027  Elapsed: 0:00:50.662711.\n",
      "  Batch 5,280  of  8,000.  Loss 0.0031  Elapsed: 0:00:51.051552.\n",
      "  Batch 5,320  of  8,000.  Loss 0.0078  Elapsed: 0:00:51.440132.\n",
      "  Batch 5,360  of  8,000.  Loss 0.0082  Elapsed: 0:00:51.827566.\n",
      "  Batch 5,400  of  8,000.  Loss 0.0122  Elapsed: 0:00:52.216844.\n",
      "  Batch 5,440  of  8,000.  Loss 0.0019  Elapsed: 0:00:52.601674.\n",
      "  Batch 5,480  of  8,000.  Loss 0.0017  Elapsed: 0:00:52.989513.\n",
      "  Batch 5,520  of  8,000.  Loss 0.0070  Elapsed: 0:00:53.378898.\n",
      "  Batch 5,560  of  8,000.  Loss 0.0015  Elapsed: 0:00:53.765246.\n",
      "  Batch 5,600  of  8,000.  Loss 0.0013  Elapsed: 0:00:54.153514.\n",
      "  Batch 5,640  of  8,000.  Loss 0.0015  Elapsed: 0:00:54.538869.\n",
      "  Batch 5,680  of  8,000.  Loss 0.0013  Elapsed: 0:00:54.926383.\n",
      "  Batch 5,720  of  8,000.  Loss 0.0014  Elapsed: 0:00:55.314601.\n",
      "  Batch 5,760  of  8,000.  Loss 0.0058  Elapsed: 0:00:55.702286.\n",
      "  Batch 5,800  of  8,000.  Loss 0.0013  Elapsed: 0:00:56.087914.\n",
      "  Batch 5,840  of  8,000.  Loss 0.0072  Elapsed: 0:00:56.474426.\n",
      "  Batch 5,880  of  8,000.  Loss 0.0753  Elapsed: 0:00:56.861131.\n",
      "  Batch 5,920  of  8,000.  Loss 0.0034  Elapsed: 0:00:57.246693.\n",
      "  Batch 5,960  of  8,000.  Loss 0.0101  Elapsed: 0:00:57.632828.\n",
      "  Batch 6,000  of  8,000.  Loss 0.0155  Elapsed: 0:00:58.019432.\n",
      "  Batch 6,040  of  8,000.  Loss 0.0018  Elapsed: 0:00:58.404184.\n",
      "  Batch 6,080  of  8,000.  Loss 0.0034  Elapsed: 0:00:58.791985.\n",
      "  Batch 6,120  of  8,000.  Loss 0.0014  Elapsed: 0:00:59.178928.\n",
      "  Batch 6,160  of  8,000.  Loss 0.0076  Elapsed: 0:00:59.566633.\n",
      "  Batch 6,200  of  8,000.  Loss 0.0020  Elapsed: 0:00:59.954839.\n",
      "  Batch 6,240  of  8,000.  Loss 0.0021  Elapsed: 0:01:00.342435.\n",
      "  Batch 6,280  of  8,000.  Loss 0.0013  Elapsed: 0:01:00.728212.\n",
      "  Batch 6,320  of  8,000.  Loss 0.0074  Elapsed: 0:01:01.115610.\n",
      "  Batch 6,360  of  8,000.  Loss 0.0794  Elapsed: 0:01:01.503398.\n",
      "  Batch 6,400  of  8,000.  Loss 0.0040  Elapsed: 0:01:01.890842.\n",
      "  Batch 6,440  of  8,000.  Loss 0.0027  Elapsed: 0:01:02.277446.\n",
      "  Batch 6,480  of  8,000.  Loss 0.0015  Elapsed: 0:01:02.664349.\n",
      "  Batch 6,520  of  8,000.  Loss 0.0078  Elapsed: 0:01:03.050840.\n",
      "  Batch 6,560  of  8,000.  Loss 0.0061  Elapsed: 0:01:03.437565.\n",
      "  Batch 6,600  of  8,000.  Loss 0.0015  Elapsed: 0:01:03.823415.\n",
      "  Batch 6,640  of  8,000.  Loss 0.0719  Elapsed: 0:01:04.208944.\n",
      "  Batch 6,680  of  8,000.  Loss 0.0586  Elapsed: 0:01:04.598723.\n",
      "  Batch 6,720  of  8,000.  Loss 0.0021  Elapsed: 0:01:04.983865.\n",
      "  Batch 6,760  of  8,000.  Loss 0.0137  Elapsed: 0:01:05.369092.\n",
      "  Batch 6,800  of  8,000.  Loss 0.0015  Elapsed: 0:01:05.754976.\n",
      "  Batch 6,840  of  8,000.  Loss 0.0013  Elapsed: 0:01:06.142781.\n",
      "  Batch 6,880  of  8,000.  Loss 0.4993  Elapsed: 0:01:06.528419.\n",
      "  Batch 6,920  of  8,000.  Loss 0.0014  Elapsed: 0:01:06.914957.\n",
      "  Batch 6,960  of  8,000.  Loss 0.0096  Elapsed: 0:01:07.300019.\n",
      "  Batch 7,000  of  8,000.  Loss 0.0014  Elapsed: 0:01:07.686204.\n",
      "  Batch 7,040  of  8,000.  Loss 0.0013  Elapsed: 0:01:08.070920.\n",
      "  Batch 7,080  of  8,000.  Loss 0.0087  Elapsed: 0:01:08.457721.\n",
      "  Batch 7,120  of  8,000.  Loss 0.0013  Elapsed: 0:01:08.845093.\n",
      "  Batch 7,160  of  8,000.  Loss 0.0012  Elapsed: 0:01:09.230635.\n",
      "  Batch 7,200  of  8,000.  Loss 0.0098  Elapsed: 0:01:09.615881.\n",
      "  Batch 7,240  of  8,000.  Loss 0.0018  Elapsed: 0:01:10.002538.\n",
      "  Batch 7,280  of  8,000.  Loss 0.0019  Elapsed: 0:01:10.389683.\n",
      "  Batch 7,320  of  8,000.  Loss 0.0018  Elapsed: 0:01:10.775766.\n",
      "  Batch 7,360  of  8,000.  Loss 0.0024  Elapsed: 0:01:11.161191.\n",
      "  Batch 7,400  of  8,000.  Loss 0.0084  Elapsed: 0:01:11.548611.\n",
      "  Batch 7,440  of  8,000.  Loss 0.0013  Elapsed: 0:01:11.933547.\n",
      "  Batch 7,480  of  8,000.  Loss 0.0099  Elapsed: 0:01:12.319765.\n",
      "  Batch 7,520  of  8,000.  Loss 0.0128  Elapsed: 0:01:12.707510.\n",
      "  Batch 7,560  of  8,000.  Loss 0.0019  Elapsed: 0:01:13.094741.\n",
      "  Batch 7,600  of  8,000.  Loss 0.0014  Elapsed: 0:01:13.480247.\n",
      "  Batch 7,640  of  8,000.  Loss 0.0032  Elapsed: 0:01:13.865058.\n",
      "  Batch 7,680  of  8,000.  Loss 0.0092  Elapsed: 0:01:14.252114.\n",
      "  Batch 7,720  of  8,000.  Loss 0.0266  Elapsed: 0:01:14.639809.\n",
      "  Batch 7,760  of  8,000.  Loss 0.0014  Elapsed: 0:01:15.027584.\n",
      "  Batch 7,800  of  8,000.  Loss 0.0019  Elapsed: 0:01:15.411943.\n",
      "  Batch 7,840  of  8,000.  Loss 0.0066  Elapsed: 0:01:15.801481.\n",
      "  Batch 7,880  of  8,000.  Loss 0.0082  Elapsed: 0:01:16.189024.\n",
      "  Batch 7,920  of  8,000.  Loss 0.0119  Elapsed: 0:01:16.574999.\n",
      "  Batch 7,960  of  8,000.  Loss 0.0022  Elapsed: 0:01:16.959979.\n",
      "Avg Validation Loss 0.0199, Completed in 0:01:17.337251 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.981845\n",
      "F1 Score (micro) =   0.981896\n",
      "F1 Score (macro) =   0.966748\n",
      "F1 Score (samples) =   0.978917\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0014  Elapsed: 0:00:00.405484.\n",
      "  Batch    80  of  1,477.  Loss 1.1726  Elapsed: 0:00:00.791961.\n",
      "  Batch   120  of  1,477.  Loss 0.0031  Elapsed: 0:00:01.178885.\n",
      "  Batch   160  of  1,477.  Loss 1.4328  Elapsed: 0:00:01.565656.\n",
      "  Batch   200  of  1,477.  Loss 0.0014  Elapsed: 0:00:01.953153.\n",
      "  Batch   240  of  1,477.  Loss 0.0013  Elapsed: 0:00:02.340204.\n",
      "  Batch   280  of  1,477.  Loss 0.0073  Elapsed: 0:00:02.725289.\n",
      "  Batch   320  of  1,477.  Loss 0.0020  Elapsed: 0:00:03.113061.\n",
      "  Batch   360  of  1,477.  Loss 0.0804  Elapsed: 0:00:03.499808.\n",
      "  Batch   400  of  1,477.  Loss 0.0014  Elapsed: 0:00:03.889385.\n",
      "  Batch   440  of  1,477.  Loss 0.0030  Elapsed: 0:00:04.277118.\n",
      "  Batch   480  of  1,477.  Loss 0.0219  Elapsed: 0:00:04.665363.\n",
      "  Batch   520  of  1,477.  Loss 0.0074  Elapsed: 0:00:05.052909.\n",
      "  Batch   560  of  1,477.  Loss 0.1875  Elapsed: 0:00:05.441487.\n",
      "  Batch   600  of  1,477.  Loss 1.7998  Elapsed: 0:00:05.828254.\n",
      "  Batch   640  of  1,477.  Loss 0.1648  Elapsed: 0:00:06.215767.\n",
      "  Batch   680  of  1,477.  Loss 0.0013  Elapsed: 0:00:06.600939.\n",
      "  Batch   720  of  1,477.  Loss 0.0600  Elapsed: 0:00:06.989002.\n",
      "  Batch   760  of  1,477.  Loss 0.0046  Elapsed: 0:00:07.378246.\n",
      "  Batch   800  of  1,477.  Loss 0.0020  Elapsed: 0:00:07.765004.\n",
      "  Batch   840  of  1,477.  Loss 0.0080  Elapsed: 0:00:08.151128.\n",
      "  Batch   880  of  1,477.  Loss 0.0021  Elapsed: 0:00:08.539487.\n",
      "  Batch   920  of  1,477.  Loss 0.3119  Elapsed: 0:00:08.926743.\n",
      "  Batch   960  of  1,477.  Loss 0.0021  Elapsed: 0:00:09.313202.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0167  Elapsed: 0:00:09.698801.\n",
      "  Batch 1,040  of  1,477.  Loss 0.1045  Elapsed: 0:00:10.085876.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0125  Elapsed: 0:00:10.469903.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0016  Elapsed: 0:00:10.855594.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0018  Elapsed: 0:00:11.241736.\n",
      "  Batch 1,200  of  1,477.  Loss 0.3067  Elapsed: 0:00:11.627420.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0114  Elapsed: 0:00:12.014356.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0027  Elapsed: 0:00:12.399585.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0013  Elapsed: 0:00:12.785582.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0016  Elapsed: 0:00:13.171235.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0022  Elapsed: 0:00:13.557379.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0015  Elapsed: 0:00:13.942596.\n",
      "Avg Validation Loss 0.1874, Completed in 0:00:14.290322 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.812378\n",
      "F1 Score (micro) =   0.813430\n",
      "F1 Score (macro) =   0.726667\n",
      "F1 Score (samples) =   0.794177\n",
      "Running Experiment GEMB_ALL\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    739.  Loss 0.2020  Elapsed: 0:00:25.101310.\n",
      "Epoch  0  Batch   200  of    739.  Loss 0.1999  Elapsed: 0:00:50.111287.\n",
      "Epoch  0  Batch   300  of    739.  Loss 0.2362  Elapsed: 0:01:15.116254.\n",
      "Epoch  0  Batch   400  of    739.  Loss 0.1145  Elapsed: 0:01:40.127280.\n",
      "Epoch  0  Batch   500  of    739.  Loss 0.1349  Elapsed: 0:02:05.133141.\n",
      "Epoch  0  Batch   600  of    739.  Loss 0.2341  Elapsed: 0:02:30.143055.\n",
      "Epoch  0  Batch   700  of    739.  Loss 0.3227  Elapsed: 0:02:55.153601.\n",
      "Avg Training Loss 0.2266, Completed in 0:03:04.666078 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2309  Elapsed: 0:00:03.726892.\n",
      "  Batch    80  of     93.  Loss 0.2755  Elapsed: 0:00:07.329604.\n",
      "Avg Validation Loss 0.2218, Completed in 0:00:08.347240 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.622181\n",
      "F1 Score (micro) =   0.659616\n",
      "F1 Score (macro) =   0.442002\n",
      "F1 Score (samples) =   0.570074\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    739.  Loss 0.1633  Elapsed: 0:00:25.107251.\n",
      "Epoch  1  Batch   200  of    739.  Loss 0.1735  Elapsed: 0:00:50.115827.\n",
      "Epoch  1  Batch   300  of    739.  Loss 0.1844  Elapsed: 0:01:15.125690.\n",
      "Epoch  1  Batch   400  of    739.  Loss 0.2606  Elapsed: 0:01:40.136516.\n",
      "Epoch  1  Batch   500  of    739.  Loss 0.2369  Elapsed: 0:02:05.145231.\n",
      "Epoch  1  Batch   600  of    739.  Loss 0.2943  Elapsed: 0:02:30.155631.\n",
      "Epoch  1  Batch   700  of    739.  Loss 0.1872  Elapsed: 0:02:55.163088.\n",
      "Avg Training Loss 0.1726, Completed in 0:03:04.674823 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2647  Elapsed: 0:00:03.720872.\n",
      "  Batch    80  of     93.  Loss 0.2905  Elapsed: 0:00:07.324082.\n",
      "Avg Validation Loss 0.1563, Completed in 0:00:08.343584 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.751104\n",
      "F1 Score (micro) =   0.781307\n",
      "F1 Score (macro) =   0.579205\n",
      "F1 Score (samples) =   0.724441\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    739.  Loss 0.1534  Elapsed: 0:00:25.108813.\n",
      "Epoch  2  Batch   200  of    739.  Loss 0.1298  Elapsed: 0:00:50.119998.\n",
      "Epoch  2  Batch   300  of    739.  Loss 0.0304  Elapsed: 0:01:15.132254.\n",
      "Epoch  2  Batch   400  of    739.  Loss 0.1848  Elapsed: 0:01:40.139966.\n",
      "Epoch  2  Batch   500  of    739.  Loss 0.0785  Elapsed: 0:02:05.160010.\n",
      "Epoch  2  Batch   600  of    739.  Loss 0.0846  Elapsed: 0:02:30.167227.\n",
      "Epoch  2  Batch   700  of    739.  Loss 0.0311  Elapsed: 0:02:55.174647.\n",
      "Avg Training Loss 0.1108, Completed in 0:03:04.686394 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.0777  Elapsed: 0:00:03.725520.\n",
      "  Batch    80  of     93.  Loss 0.1276  Elapsed: 0:00:07.328333.\n",
      "Avg Validation Loss 0.1043, Completed in 0:00:08.350929 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.874705\n",
      "F1 Score (micro) =   0.877997\n",
      "F1 Score (macro) =   0.802876\n",
      "F1 Score (samples) =   0.842699\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    739.  Loss 0.0475  Elapsed: 0:00:25.102193.\n",
      "Epoch  3  Batch   200  of    739.  Loss 0.0533  Elapsed: 0:00:50.114642.\n",
      "Epoch  3  Batch   300  of    739.  Loss 0.0313  Elapsed: 0:01:15.122308.\n",
      "Epoch  3  Batch   400  of    739.  Loss 0.1284  Elapsed: 0:01:40.131719.\n",
      "Epoch  3  Batch   500  of    739.  Loss 0.0690  Elapsed: 0:02:05.143384.\n",
      "Epoch  3  Batch   600  of    739.  Loss 0.1242  Elapsed: 0:02:30.150800.\n",
      "Epoch  3  Batch   700  of    739.  Loss 0.0319  Elapsed: 0:02:55.163647.\n",
      "Avg Training Loss 0.0660, Completed in 0:03:04.680194 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.0160  Elapsed: 0:00:03.750688.\n",
      "  Batch    80  of     93.  Loss 0.0152  Elapsed: 0:00:07.352549.\n",
      "Avg Validation Loss 0.0922, Completed in 0:00:08.366877 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.899951\n",
      "F1 Score (micro) =   0.900513\n",
      "F1 Score (macro) =   0.845829\n",
      "F1 Score (samples) =   0.890995\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  11,817.  Loss 0.0073  Elapsed: 0:00:00.399962.\n",
      "  Batch    80  of  11,817.  Loss 0.4523  Elapsed: 0:00:00.795481.\n",
      "  Batch   120  of  11,817.  Loss 0.0088  Elapsed: 0:00:01.186768.\n",
      "  Batch   160  of  11,817.  Loss 0.0012  Elapsed: 0:00:01.577870.\n",
      "  Batch   200  of  11,817.  Loss 0.0057  Elapsed: 0:00:01.968595.\n",
      "  Batch   240  of  11,817.  Loss 0.0017  Elapsed: 0:00:02.357420.\n",
      "  Batch   280  of  11,817.  Loss 0.0013  Elapsed: 0:00:02.744913.\n",
      "  Batch   320  of  11,817.  Loss 0.0025  Elapsed: 0:00:03.138393.\n",
      "  Batch   360  of  11,817.  Loss 0.0013  Elapsed: 0:00:03.530782.\n",
      "  Batch   400  of  11,817.  Loss 0.0019  Elapsed: 0:00:03.922837.\n",
      "  Batch   440  of  11,817.  Loss 0.0013  Elapsed: 0:00:04.313115.\n",
      "  Batch   480  of  11,817.  Loss 0.0014  Elapsed: 0:00:04.702538.\n",
      "  Batch   520  of  11,817.  Loss 0.0022  Elapsed: 0:00:05.093513.\n",
      "  Batch   560  of  11,817.  Loss 0.0017  Elapsed: 0:00:05.485258.\n",
      "  Batch   600  of  11,817.  Loss 0.0100  Elapsed: 0:00:05.873775.\n",
      "  Batch   640  of  11,817.  Loss 0.0022  Elapsed: 0:00:06.262431.\n",
      "  Batch   680  of  11,817.  Loss 0.0038  Elapsed: 0:00:06.652704.\n",
      "  Batch   720  of  11,817.  Loss 0.0111  Elapsed: 0:00:07.041837.\n",
      "  Batch   760  of  11,817.  Loss 0.0149  Elapsed: 0:00:07.431721.\n",
      "  Batch   800  of  11,817.  Loss 0.0115  Elapsed: 0:00:07.821271.\n",
      "  Batch   840  of  11,817.  Loss 0.0012  Elapsed: 0:00:08.212757.\n",
      "  Batch   880  of  11,817.  Loss 0.0013  Elapsed: 0:00:08.603271.\n",
      "  Batch   920  of  11,817.  Loss 0.0756  Elapsed: 0:00:08.995891.\n",
      "  Batch   960  of  11,817.  Loss 0.0020  Elapsed: 0:00:09.384777.\n",
      "  Batch 1,000  of  11,817.  Loss 0.0021  Elapsed: 0:00:09.773454.\n",
      "  Batch 1,040  of  11,817.  Loss 0.0103  Elapsed: 0:00:10.165533.\n",
      "  Batch 1,080  of  11,817.  Loss 0.0013  Elapsed: 0:00:10.556943.\n",
      "  Batch 1,120  of  11,817.  Loss 0.0012  Elapsed: 0:00:10.949070.\n",
      "  Batch 1,160  of  11,817.  Loss 0.0018  Elapsed: 0:00:11.338164.\n",
      "  Batch 1,200  of  11,817.  Loss 0.0650  Elapsed: 0:00:11.727137.\n",
      "  Batch 1,240  of  11,817.  Loss 0.0077  Elapsed: 0:00:12.120427.\n",
      "  Batch 1,280  of  11,817.  Loss 0.0065  Elapsed: 0:00:12.512665.\n",
      "  Batch 1,320  of  11,817.  Loss 0.0125  Elapsed: 0:00:12.904136.\n",
      "  Batch 1,360  of  11,817.  Loss 0.0953  Elapsed: 0:00:13.293936.\n",
      "  Batch 1,400  of  11,817.  Loss 0.0140  Elapsed: 0:00:13.686185.\n",
      "  Batch 1,440  of  11,817.  Loss 0.0052  Elapsed: 0:00:14.078601.\n",
      "  Batch 1,480  of  11,817.  Loss 0.0013  Elapsed: 0:00:14.468414.\n",
      "  Batch 1,520  of  11,817.  Loss 0.0466  Elapsed: 0:00:14.858455.\n",
      "  Batch 1,560  of  11,817.  Loss 0.1331  Elapsed: 0:00:15.246292.\n",
      "  Batch 1,600  of  11,817.  Loss 0.0013  Elapsed: 0:00:15.635860.\n",
      "  Batch 1,640  of  11,817.  Loss 0.0015  Elapsed: 0:00:16.027288.\n",
      "  Batch 1,680  of  11,817.  Loss 0.0105  Elapsed: 0:00:16.419500.\n",
      "  Batch 1,720  of  11,817.  Loss 0.0021  Elapsed: 0:00:16.811386.\n",
      "  Batch 1,760  of  11,817.  Loss 0.0093  Elapsed: 0:00:17.200889.\n",
      "  Batch 1,800  of  11,817.  Loss 0.0019  Elapsed: 0:00:17.593531.\n",
      "  Batch 1,840  of  11,817.  Loss 0.0014  Elapsed: 0:00:17.985585.\n",
      "  Batch 1,880  of  11,817.  Loss 0.0013  Elapsed: 0:00:18.377245.\n",
      "  Batch 1,920  of  11,817.  Loss 0.0014  Elapsed: 0:00:18.768326.\n",
      "  Batch 1,960  of  11,817.  Loss 0.0315  Elapsed: 0:00:19.160920.\n",
      "  Batch 2,000  of  11,817.  Loss 0.0170  Elapsed: 0:00:19.553077.\n",
      "  Batch 2,040  of  11,817.  Loss 0.0020  Elapsed: 0:00:19.944724.\n",
      "  Batch 2,080  of  11,817.  Loss 0.0030  Elapsed: 0:00:20.334619.\n",
      "  Batch 2,120  of  11,817.  Loss 0.0017  Elapsed: 0:00:20.723012.\n",
      "  Batch 2,160  of  11,817.  Loss 0.0086  Elapsed: 0:00:21.113254.\n",
      "  Batch 2,200  of  11,817.  Loss 0.0131  Elapsed: 0:00:21.501720.\n",
      "  Batch 2,240  of  11,817.  Loss 0.0013  Elapsed: 0:00:21.890510.\n",
      "  Batch 2,280  of  11,817.  Loss 0.0297  Elapsed: 0:00:22.280121.\n",
      "  Batch 2,320  of  11,817.  Loss 0.0015  Elapsed: 0:00:22.671055.\n",
      "  Batch 2,360  of  11,817.  Loss 0.0020  Elapsed: 0:00:23.059195.\n",
      "  Batch 2,400  of  11,817.  Loss 0.0013  Elapsed: 0:00:23.447313.\n",
      "  Batch 2,440  of  11,817.  Loss 0.0027  Elapsed: 0:00:23.835294.\n",
      "  Batch 2,480  of  11,817.  Loss 0.0015  Elapsed: 0:00:24.225857.\n",
      "  Batch 2,520  of  11,817.  Loss 0.0059  Elapsed: 0:00:24.617780.\n",
      "  Batch 2,560  of  11,817.  Loss 0.0013  Elapsed: 0:00:25.008474.\n",
      "  Batch 2,600  of  11,817.  Loss 0.0014  Elapsed: 0:00:25.398945.\n",
      "  Batch 2,640  of  11,817.  Loss 0.0121  Elapsed: 0:00:25.789549.\n",
      "  Batch 2,680  of  11,817.  Loss 0.0072  Elapsed: 0:00:26.179978.\n",
      "  Batch 2,720  of  11,817.  Loss 0.0126  Elapsed: 0:00:26.571091.\n",
      "  Batch 2,760  of  11,817.  Loss 0.0192  Elapsed: 0:00:26.962687.\n",
      "  Batch 2,800  of  11,817.  Loss 0.0043  Elapsed: 0:00:27.357831.\n",
      "  Batch 2,840  of  11,817.  Loss 0.0026  Elapsed: 0:00:27.745899.\n",
      "  Batch 2,880  of  11,817.  Loss 0.0015  Elapsed: 0:00:28.136950.\n",
      "  Batch 2,920  of  11,817.  Loss 0.0013  Elapsed: 0:00:28.528669.\n",
      "  Batch 2,960  of  11,817.  Loss 0.0086  Elapsed: 0:00:28.918580.\n",
      "  Batch 3,000  of  11,817.  Loss 0.0014  Elapsed: 0:00:29.308145.\n",
      "  Batch 3,040  of  11,817.  Loss 0.0014  Elapsed: 0:00:29.698320.\n",
      "  Batch 3,080  of  11,817.  Loss 0.0281  Elapsed: 0:00:30.090838.\n",
      "  Batch 3,120  of  11,817.  Loss 0.2693  Elapsed: 0:00:30.480189.\n",
      "  Batch 3,160  of  11,817.  Loss 0.0031  Elapsed: 0:00:30.867909.\n",
      "  Batch 3,200  of  11,817.  Loss 0.0327  Elapsed: 0:00:31.260488.\n",
      "  Batch 3,240  of  11,817.  Loss 0.0243  Elapsed: 0:00:31.651406.\n",
      "  Batch 3,280  of  11,817.  Loss 0.0310  Elapsed: 0:00:32.041369.\n",
      "  Batch 3,320  of  11,817.  Loss 0.0026  Elapsed: 0:00:32.434305.\n",
      "  Batch 3,360  of  11,817.  Loss 0.0014  Elapsed: 0:00:32.823042.\n",
      "  Batch 3,400  of  11,817.  Loss 0.0013  Elapsed: 0:00:33.213619.\n",
      "  Batch 3,440  of  11,817.  Loss 0.0013  Elapsed: 0:00:33.602708.\n",
      "  Batch 3,480  of  11,817.  Loss 0.0313  Elapsed: 0:00:33.993584.\n",
      "  Batch 3,520  of  11,817.  Loss 0.0014  Elapsed: 0:00:34.384428.\n",
      "  Batch 3,560  of  11,817.  Loss 0.0014  Elapsed: 0:00:34.774717.\n",
      "  Batch 3,600  of  11,817.  Loss 0.0015  Elapsed: 0:00:35.164679.\n",
      "  Batch 3,640  of  11,817.  Loss 0.0019  Elapsed: 0:00:35.557227.\n",
      "  Batch 3,680  of  11,817.  Loss 0.0021  Elapsed: 0:00:35.947900.\n",
      "  Batch 3,720  of  11,817.  Loss 0.0450  Elapsed: 0:00:36.339068.\n",
      "  Batch 3,760  of  11,817.  Loss 0.0013  Elapsed: 0:00:36.730805.\n",
      "  Batch 3,800  of  11,817.  Loss 0.0013  Elapsed: 0:00:37.122942.\n",
      "  Batch 3,840  of  11,817.  Loss 0.0013  Elapsed: 0:00:37.516472.\n",
      "  Batch 3,880  of  11,817.  Loss 0.0017  Elapsed: 0:00:37.908690.\n",
      "  Batch 3,920  of  11,817.  Loss 0.0019  Elapsed: 0:00:38.298473.\n",
      "  Batch 3,960  of  11,817.  Loss 0.0015  Elapsed: 0:00:38.688613.\n",
      "  Batch 4,000  of  11,817.  Loss 0.0016  Elapsed: 0:00:39.080741.\n",
      "  Batch 4,040  of  11,817.  Loss 0.0014  Elapsed: 0:00:39.471408.\n",
      "  Batch 4,080  of  11,817.  Loss 0.2182  Elapsed: 0:00:39.861979.\n",
      "  Batch 4,120  of  11,817.  Loss 0.0104  Elapsed: 0:00:40.251933.\n",
      "  Batch 4,160  of  11,817.  Loss 0.0016  Elapsed: 0:00:40.641463.\n",
      "  Batch 4,200  of  11,817.  Loss 0.0134  Elapsed: 0:00:41.030307.\n",
      "  Batch 4,240  of  11,817.  Loss 0.0015  Elapsed: 0:00:41.419998.\n",
      "  Batch 4,280  of  11,817.  Loss 0.0169  Elapsed: 0:00:41.812467.\n",
      "  Batch 4,320  of  11,817.  Loss 0.0040  Elapsed: 0:00:42.201900.\n",
      "  Batch 4,360  of  11,817.  Loss 0.0014  Elapsed: 0:00:42.592339.\n",
      "  Batch 4,400  of  11,817.  Loss 0.0208  Elapsed: 0:00:42.981000.\n",
      "  Batch 4,440  of  11,817.  Loss 0.0022  Elapsed: 0:00:43.367152.\n",
      "  Batch 4,480  of  11,817.  Loss 0.0092  Elapsed: 0:00:43.754409.\n",
      "  Batch 4,520  of  11,817.  Loss 0.0013  Elapsed: 0:00:44.142220.\n",
      "  Batch 4,560  of  11,817.  Loss 0.0197  Elapsed: 0:00:44.533381.\n",
      "  Batch 4,600  of  11,817.  Loss 0.0013  Elapsed: 0:00:44.921661.\n",
      "  Batch 4,640  of  11,817.  Loss 0.0017  Elapsed: 0:00:45.309812.\n",
      "  Batch 4,680  of  11,817.  Loss 0.0057  Elapsed: 0:00:45.699317.\n",
      "  Batch 4,720  of  11,817.  Loss 0.0161  Elapsed: 0:00:46.086102.\n",
      "  Batch 4,760  of  11,817.  Loss 0.0026  Elapsed: 0:00:46.474617.\n",
      "  Batch 4,800  of  11,817.  Loss 0.0034  Elapsed: 0:00:46.861216.\n",
      "  Batch 4,840  of  11,817.  Loss 0.0014  Elapsed: 0:00:47.249711.\n",
      "  Batch 4,880  of  11,817.  Loss 0.0013  Elapsed: 0:00:47.637364.\n",
      "  Batch 4,920  of  11,817.  Loss 0.0078  Elapsed: 0:00:48.023877.\n",
      "  Batch 4,960  of  11,817.  Loss 0.0013  Elapsed: 0:00:48.411565.\n",
      "  Batch 5,000  of  11,817.  Loss 0.0017  Elapsed: 0:00:48.799244.\n",
      "  Batch 5,040  of  11,817.  Loss 0.0078  Elapsed: 0:00:49.187828.\n",
      "  Batch 5,080  of  11,817.  Loss 0.0023  Elapsed: 0:00:49.575582.\n",
      "  Batch 5,120  of  11,817.  Loss 0.0014  Elapsed: 0:00:49.963216.\n",
      "  Batch 5,160  of  11,817.  Loss 0.0013  Elapsed: 0:00:50.350506.\n",
      "  Batch 5,200  of  11,817.  Loss 0.0084  Elapsed: 0:00:50.737329.\n",
      "  Batch 5,240  of  11,817.  Loss 0.0096  Elapsed: 0:00:51.126569.\n",
      "  Batch 5,280  of  11,817.  Loss 0.0104  Elapsed: 0:00:51.514621.\n",
      "  Batch 5,320  of  11,817.  Loss 0.0134  Elapsed: 0:00:51.901514.\n",
      "  Batch 5,360  of  11,817.  Loss 0.1638  Elapsed: 0:00:52.287897.\n",
      "  Batch 5,400  of  11,817.  Loss 0.0013  Elapsed: 0:00:52.674081.\n",
      "  Batch 5,440  of  11,817.  Loss 0.0015  Elapsed: 0:00:53.060901.\n",
      "  Batch 5,480  of  11,817.  Loss 0.0015  Elapsed: 0:00:53.450497.\n",
      "  Batch 5,520  of  11,817.  Loss 0.0017  Elapsed: 0:00:53.837930.\n",
      "  Batch 5,560  of  11,817.  Loss 0.0559  Elapsed: 0:00:54.224869.\n",
      "  Batch 5,600  of  11,817.  Loss 0.0019  Elapsed: 0:00:54.612373.\n",
      "  Batch 5,640  of  11,817.  Loss 0.0085  Elapsed: 0:00:54.998870.\n",
      "  Batch 5,680  of  11,817.  Loss 0.0057  Elapsed: 0:00:55.386488.\n",
      "  Batch 5,720  of  11,817.  Loss 0.0012  Elapsed: 0:00:55.773883.\n",
      "  Batch 5,760  of  11,817.  Loss 0.0035  Elapsed: 0:00:56.163459.\n",
      "  Batch 5,800  of  11,817.  Loss 0.0013  Elapsed: 0:00:56.551530.\n",
      "  Batch 5,840  of  11,817.  Loss 0.7125  Elapsed: 0:00:56.938810.\n",
      "  Batch 5,880  of  11,817.  Loss 0.0024  Elapsed: 0:00:57.326044.\n",
      "  Batch 5,920  of  11,817.  Loss 0.0363  Elapsed: 0:00:57.713700.\n",
      "  Batch 5,960  of  11,817.  Loss 0.0014  Elapsed: 0:00:58.102379.\n",
      "  Batch 6,000  of  11,817.  Loss 0.0013  Elapsed: 0:00:58.488971.\n",
      "  Batch 6,040  of  11,817.  Loss 0.0017  Elapsed: 0:00:58.877783.\n",
      "  Batch 6,080  of  11,817.  Loss 0.0042  Elapsed: 0:00:59.265768.\n",
      "  Batch 6,120  of  11,817.  Loss 0.0023  Elapsed: 0:00:59.651571.\n",
      "  Batch 6,160  of  11,817.  Loss 0.0013  Elapsed: 0:01:00.039181.\n",
      "  Batch 6,200  of  11,817.  Loss 0.0320  Elapsed: 0:01:00.427114.\n",
      "  Batch 6,240  of  11,817.  Loss 0.0013  Elapsed: 0:01:00.811865.\n",
      "  Batch 6,280  of  11,817.  Loss 0.0063  Elapsed: 0:01:01.199012.\n",
      "  Batch 6,320  of  11,817.  Loss 0.0013  Elapsed: 0:01:01.586778.\n",
      "  Batch 6,360  of  11,817.  Loss 0.0046  Elapsed: 0:01:01.975147.\n",
      "  Batch 6,400  of  11,817.  Loss 0.0012  Elapsed: 0:01:02.362165.\n",
      "  Batch 6,440  of  11,817.  Loss 0.0741  Elapsed: 0:01:02.749542.\n",
      "  Batch 6,480  of  11,817.  Loss 0.0013  Elapsed: 0:01:03.137736.\n",
      "  Batch 6,520  of  11,817.  Loss 0.0876  Elapsed: 0:01:03.524793.\n",
      "  Batch 6,560  of  11,817.  Loss 0.0030  Elapsed: 0:01:03.913119.\n",
      "  Batch 6,600  of  11,817.  Loss 0.0014  Elapsed: 0:01:04.301154.\n",
      "  Batch 6,640  of  11,817.  Loss 0.0013  Elapsed: 0:01:04.689486.\n",
      "  Batch 6,680  of  11,817.  Loss 0.0013  Elapsed: 0:01:05.076935.\n",
      "  Batch 6,720  of  11,817.  Loss 0.0013  Elapsed: 0:01:05.464896.\n",
      "  Batch 6,760  of  11,817.  Loss 0.0029  Elapsed: 0:01:05.853736.\n",
      "  Batch 6,800  of  11,817.  Loss 0.0014  Elapsed: 0:01:06.244490.\n",
      "  Batch 6,840  of  11,817.  Loss 0.0587  Elapsed: 0:01:06.632174.\n",
      "  Batch 6,880  of  11,817.  Loss 0.0198  Elapsed: 0:01:07.020589.\n",
      "  Batch 6,920  of  11,817.  Loss 0.0073  Elapsed: 0:01:07.407128.\n",
      "  Batch 6,960  of  11,817.  Loss 0.0013  Elapsed: 0:01:07.792428.\n",
      "  Batch 7,000  of  11,817.  Loss 0.3238  Elapsed: 0:01:08.178417.\n",
      "  Batch 7,040  of  11,817.  Loss 0.0031  Elapsed: 0:01:08.565108.\n",
      "  Batch 7,080  of  11,817.  Loss 0.0013  Elapsed: 0:01:08.951946.\n",
      "  Batch 7,120  of  11,817.  Loss 0.0275  Elapsed: 0:01:09.339195.\n",
      "  Batch 7,160  of  11,817.  Loss 0.0029  Elapsed: 0:01:09.727020.\n",
      "  Batch 7,200  of  11,817.  Loss 0.0174  Elapsed: 0:01:10.114171.\n",
      "  Batch 7,240  of  11,817.  Loss 0.4705  Elapsed: 0:01:10.505049.\n",
      "  Batch 7,280  of  11,817.  Loss 0.0015  Elapsed: 0:01:10.893024.\n",
      "  Batch 7,320  of  11,817.  Loss 0.0025  Elapsed: 0:01:11.282371.\n",
      "  Batch 7,360  of  11,817.  Loss 0.0013  Elapsed: 0:01:11.672727.\n",
      "  Batch 7,400  of  11,817.  Loss 0.0014  Elapsed: 0:01:12.060659.\n",
      "  Batch 7,440  of  11,817.  Loss 0.0041  Elapsed: 0:01:12.446312.\n",
      "  Batch 7,480  of  11,817.  Loss 0.0022  Elapsed: 0:01:12.831963.\n",
      "  Batch 7,520  of  11,817.  Loss 0.0013  Elapsed: 0:01:13.218002.\n",
      "  Batch 7,560  of  11,817.  Loss 0.0013  Elapsed: 0:01:13.607241.\n",
      "  Batch 7,600  of  11,817.  Loss 0.0694  Elapsed: 0:01:13.993418.\n",
      "  Batch 7,640  of  11,817.  Loss 0.0013  Elapsed: 0:01:14.381736.\n",
      "  Batch 7,680  of  11,817.  Loss 0.0015  Elapsed: 0:01:14.766971.\n",
      "  Batch 7,720  of  11,817.  Loss 0.0018  Elapsed: 0:01:15.156572.\n",
      "  Batch 7,760  of  11,817.  Loss 0.0015  Elapsed: 0:01:15.545614.\n",
      "  Batch 7,800  of  11,817.  Loss 0.0025  Elapsed: 0:01:15.934028.\n",
      "  Batch 7,840  of  11,817.  Loss 0.0426  Elapsed: 0:01:16.321592.\n",
      "  Batch 7,880  of  11,817.  Loss 0.0015  Elapsed: 0:01:16.710341.\n",
      "  Batch 7,920  of  11,817.  Loss 0.0013  Elapsed: 0:01:17.095735.\n",
      "  Batch 7,960  of  11,817.  Loss 0.0014  Elapsed: 0:01:17.481710.\n",
      "  Batch 8,000  of  11,817.  Loss 0.0015  Elapsed: 0:01:17.869466.\n",
      "  Batch 8,040  of  11,817.  Loss 0.0026  Elapsed: 0:01:18.256083.\n",
      "  Batch 8,080  of  11,817.  Loss 0.0021  Elapsed: 0:01:18.644191.\n",
      "  Batch 8,120  of  11,817.  Loss 0.0037  Elapsed: 0:01:19.030518.\n",
      "  Batch 8,160  of  11,817.  Loss 0.0012  Elapsed: 0:01:19.418432.\n",
      "  Batch 8,200  of  11,817.  Loss 0.0070  Elapsed: 0:01:19.806718.\n",
      "  Batch 8,240  of  11,817.  Loss 0.0087  Elapsed: 0:01:20.194552.\n",
      "  Batch 8,280  of  11,817.  Loss 0.0013  Elapsed: 0:01:20.583646.\n",
      "  Batch 8,320  of  11,817.  Loss 0.0406  Elapsed: 0:01:20.972403.\n",
      "  Batch 8,360  of  11,817.  Loss 0.0015  Elapsed: 0:01:21.361823.\n",
      "  Batch 8,400  of  11,817.  Loss 0.0042  Elapsed: 0:01:21.748570.\n",
      "  Batch 8,440  of  11,817.  Loss 0.1588  Elapsed: 0:01:22.136334.\n",
      "  Batch 8,480  of  11,817.  Loss 0.0015  Elapsed: 0:01:22.525637.\n",
      "  Batch 8,520  of  11,817.  Loss 0.0552  Elapsed: 0:01:22.910382.\n",
      "  Batch 8,560  of  11,817.  Loss 0.1556  Elapsed: 0:01:23.298639.\n",
      "  Batch 8,600  of  11,817.  Loss 0.0023  Elapsed: 0:01:23.686287.\n",
      "  Batch 8,640  of  11,817.  Loss 0.0014  Elapsed: 0:01:24.073661.\n",
      "  Batch 8,680  of  11,817.  Loss 0.0083  Elapsed: 0:01:24.463285.\n",
      "  Batch 8,720  of  11,817.  Loss 0.0312  Elapsed: 0:01:24.850578.\n",
      "  Batch 8,760  of  11,817.  Loss 0.0013  Elapsed: 0:01:25.235979.\n",
      "  Batch 8,800  of  11,817.  Loss 0.0018  Elapsed: 0:01:25.622104.\n",
      "  Batch 8,840  of  11,817.  Loss 0.0013  Elapsed: 0:01:26.009060.\n",
      "  Batch 8,880  of  11,817.  Loss 0.0025  Elapsed: 0:01:26.398719.\n",
      "  Batch 8,920  of  11,817.  Loss 0.0014  Elapsed: 0:01:26.785026.\n",
      "  Batch 8,960  of  11,817.  Loss 0.0044  Elapsed: 0:01:27.171064.\n",
      "  Batch 9,000  of  11,817.  Loss 0.0082  Elapsed: 0:01:27.770620.\n",
      "  Batch 9,040  of  11,817.  Loss 0.0140  Elapsed: 0:01:28.160947.\n",
      "  Batch 9,080  of  11,817.  Loss 0.0018  Elapsed: 0:01:28.550303.\n",
      "  Batch 9,120  of  11,817.  Loss 0.0082  Elapsed: 0:01:28.940108.\n",
      "  Batch 9,160  of  11,817.  Loss 0.0012  Elapsed: 0:01:29.330385.\n",
      "  Batch 9,200  of  11,817.  Loss 0.0059  Elapsed: 0:01:29.720448.\n",
      "  Batch 9,240  of  11,817.  Loss 0.0014  Elapsed: 0:01:30.107412.\n",
      "  Batch 9,280  of  11,817.  Loss 0.0013  Elapsed: 0:01:30.495777.\n",
      "  Batch 9,320  of  11,817.  Loss 0.0201  Elapsed: 0:01:30.884754.\n",
      "  Batch 9,360  of  11,817.  Loss 0.0013  Elapsed: 0:01:31.272564.\n",
      "  Batch 9,400  of  11,817.  Loss 0.0045  Elapsed: 0:01:31.661756.\n",
      "  Batch 9,440  of  11,817.  Loss 0.0014  Elapsed: 0:01:32.052457.\n",
      "  Batch 9,480  of  11,817.  Loss 0.0012  Elapsed: 0:01:32.444636.\n",
      "  Batch 9,520  of  11,817.  Loss 0.0115  Elapsed: 0:01:32.831862.\n",
      "  Batch 9,560  of  11,817.  Loss 0.0076  Elapsed: 0:01:33.222075.\n",
      "  Batch 9,600  of  11,817.  Loss 0.0020  Elapsed: 0:01:33.610351.\n",
      "  Batch 9,640  of  11,817.  Loss 0.0015  Elapsed: 0:01:33.996923.\n",
      "  Batch 9,680  of  11,817.  Loss 0.0021  Elapsed: 0:01:34.387183.\n",
      "  Batch 9,720  of  11,817.  Loss 0.0031  Elapsed: 0:01:34.776994.\n",
      "  Batch 9,760  of  11,817.  Loss 0.0080  Elapsed: 0:01:35.167645.\n",
      "  Batch 9,800  of  11,817.  Loss 0.0021  Elapsed: 0:01:35.555648.\n",
      "  Batch 9,840  of  11,817.  Loss 0.0013  Elapsed: 0:01:35.944674.\n",
      "  Batch 9,880  of  11,817.  Loss 0.0079  Elapsed: 0:01:36.335269.\n",
      "  Batch 9,920  of  11,817.  Loss 0.0727  Elapsed: 0:01:36.721559.\n",
      "  Batch 9,960  of  11,817.  Loss 0.0018  Elapsed: 0:01:37.111663.\n",
      "  Batch 10,000  of  11,817.  Loss 0.0013  Elapsed: 0:01:37.500481.\n",
      "  Batch 10,040  of  11,817.  Loss 0.0093  Elapsed: 0:01:37.889410.\n",
      "  Batch 10,080  of  11,817.  Loss 0.0013  Elapsed: 0:01:38.276819.\n",
      "  Batch 10,120  of  11,817.  Loss 0.0023  Elapsed: 0:01:38.666400.\n",
      "  Batch 10,160  of  11,817.  Loss 0.0028  Elapsed: 0:01:39.057300.\n",
      "  Batch 10,200  of  11,817.  Loss 0.0212  Elapsed: 0:01:39.445307.\n",
      "  Batch 10,240  of  11,817.  Loss 0.0071  Elapsed: 0:01:39.834399.\n",
      "  Batch 10,280  of  11,817.  Loss 0.0257  Elapsed: 0:01:40.223913.\n",
      "  Batch 10,320  of  11,817.  Loss 0.0013  Elapsed: 0:01:40.611781.\n",
      "  Batch 10,360  of  11,817.  Loss 0.0013  Elapsed: 0:01:41.001832.\n",
      "  Batch 10,400  of  11,817.  Loss 0.0013  Elapsed: 0:01:41.389215.\n",
      "  Batch 10,440  of  11,817.  Loss 0.0028  Elapsed: 0:01:41.776925.\n",
      "  Batch 10,480  of  11,817.  Loss 0.2623  Elapsed: 0:01:42.167859.\n",
      "  Batch 10,520  of  11,817.  Loss 0.0094  Elapsed: 0:01:42.554829.\n",
      "  Batch 10,560  of  11,817.  Loss 0.0021  Elapsed: 0:01:42.942700.\n",
      "  Batch 10,600  of  11,817.  Loss 0.0060  Elapsed: 0:01:43.331791.\n",
      "  Batch 10,640  of  11,817.  Loss 0.0013  Elapsed: 0:01:43.718301.\n",
      "  Batch 10,680  of  11,817.  Loss 0.0916  Elapsed: 0:01:44.109263.\n",
      "  Batch 10,720  of  11,817.  Loss 0.0134  Elapsed: 0:01:44.497312.\n",
      "  Batch 10,760  of  11,817.  Loss 0.0013  Elapsed: 0:01:44.888493.\n",
      "  Batch 10,800  of  11,817.  Loss 0.0094  Elapsed: 0:01:45.276076.\n",
      "  Batch 10,840  of  11,817.  Loss 0.0107  Elapsed: 0:01:45.662776.\n",
      "  Batch 10,880  of  11,817.  Loss 0.0064  Elapsed: 0:01:46.051096.\n",
      "  Batch 10,920  of  11,817.  Loss 0.0014  Elapsed: 0:01:46.439259.\n",
      "  Batch 10,960  of  11,817.  Loss 0.0063  Elapsed: 0:01:46.827883.\n",
      "  Batch 11,000  of  11,817.  Loss 0.0086  Elapsed: 0:01:47.216622.\n",
      "  Batch 11,040  of  11,817.  Loss 0.0013  Elapsed: 0:01:47.604021.\n",
      "  Batch 11,080  of  11,817.  Loss 0.0013  Elapsed: 0:01:47.991073.\n",
      "  Batch 11,120  of  11,817.  Loss 0.0015  Elapsed: 0:01:48.378414.\n",
      "  Batch 11,160  of  11,817.  Loss 0.0085  Elapsed: 0:01:48.766271.\n",
      "  Batch 11,200  of  11,817.  Loss 0.0015  Elapsed: 0:01:49.154917.\n",
      "  Batch 11,240  of  11,817.  Loss 0.0013  Elapsed: 0:01:49.543089.\n",
      "  Batch 11,280  of  11,817.  Loss 0.0015  Elapsed: 0:01:49.931063.\n",
      "  Batch 11,320  of  11,817.  Loss 0.0018  Elapsed: 0:01:50.320852.\n",
      "  Batch 11,360  of  11,817.  Loss 0.0061  Elapsed: 0:01:50.709071.\n",
      "  Batch 11,400  of  11,817.  Loss 0.0394  Elapsed: 0:01:51.096875.\n",
      "  Batch 11,440  of  11,817.  Loss 0.0526  Elapsed: 0:01:51.483939.\n",
      "  Batch 11,480  of  11,817.  Loss 0.0105  Elapsed: 0:01:51.871785.\n",
      "  Batch 11,520  of  11,817.  Loss 0.0018  Elapsed: 0:01:52.260751.\n",
      "  Batch 11,560  of  11,817.  Loss 0.0015  Elapsed: 0:01:52.649593.\n",
      "  Batch 11,600  of  11,817.  Loss 0.0104  Elapsed: 0:01:53.035858.\n",
      "  Batch 11,640  of  11,817.  Loss 0.0021  Elapsed: 0:01:53.424111.\n",
      "  Batch 11,680  of  11,817.  Loss 0.0035  Elapsed: 0:01:53.811597.\n",
      "  Batch 11,720  of  11,817.  Loss 0.0013  Elapsed: 0:01:54.201254.\n",
      "  Batch 11,760  of  11,817.  Loss 0.0016  Elapsed: 0:01:54.588775.\n",
      "  Batch 11,800  of  11,817.  Loss 0.0047  Elapsed: 0:01:54.976205.\n",
      "Avg Validation Loss 0.0213, Completed in 0:01:55.131288 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.980330\n",
      "F1 Score (micro) =   0.980419\n",
      "F1 Score (macro) =   0.955783\n",
      "F1 Score (samples) =   0.976136\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0013  Elapsed: 0:00:00.402427.\n",
      "  Batch    80  of  1,477.  Loss 0.0018  Elapsed: 0:00:00.790304.\n",
      "  Batch   120  of  1,477.  Loss 1.4968  Elapsed: 0:00:01.179419.\n",
      "  Batch   160  of  1,477.  Loss 0.0078  Elapsed: 0:00:01.565373.\n",
      "  Batch   200  of  1,477.  Loss 0.0021  Elapsed: 0:00:01.954700.\n",
      "  Batch   240  of  1,477.  Loss 0.5190  Elapsed: 0:00:02.342952.\n",
      "  Batch   280  of  1,477.  Loss 0.0074  Elapsed: 0:00:02.731669.\n",
      "  Batch   320  of  1,477.  Loss 0.0157  Elapsed: 0:00:03.120824.\n",
      "  Batch   360  of  1,477.  Loss 0.9394  Elapsed: 0:00:03.507154.\n",
      "  Batch   400  of  1,477.  Loss 0.0209  Elapsed: 0:00:03.892217.\n",
      "  Batch   440  of  1,477.  Loss 0.0078  Elapsed: 0:00:04.279711.\n",
      "  Batch   480  of  1,477.  Loss 0.0031  Elapsed: 0:00:04.668798.\n",
      "  Batch   520  of  1,477.  Loss 0.0769  Elapsed: 0:00:05.056926.\n",
      "  Batch   560  of  1,477.  Loss 0.7829  Elapsed: 0:00:05.445318.\n",
      "  Batch   600  of  1,477.  Loss 0.0143  Elapsed: 0:00:05.834386.\n",
      "  Batch   640  of  1,477.  Loss 0.0095  Elapsed: 0:00:06.220806.\n",
      "  Batch   680  of  1,477.  Loss 0.3049  Elapsed: 0:00:06.606596.\n",
      "  Batch   720  of  1,477.  Loss 0.0028  Elapsed: 0:00:06.997095.\n",
      "  Batch   760  of  1,477.  Loss 0.0016  Elapsed: 0:00:07.384481.\n",
      "  Batch   800  of  1,477.  Loss 1.2280  Elapsed: 0:00:07.773056.\n",
      "  Batch   840  of  1,477.  Loss 0.0020  Elapsed: 0:00:08.161871.\n",
      "  Batch   880  of  1,477.  Loss 0.0074  Elapsed: 0:00:08.549579.\n",
      "  Batch   920  of  1,477.  Loss 0.2194  Elapsed: 0:00:08.939195.\n",
      "  Batch   960  of  1,477.  Loss 0.2173  Elapsed: 0:00:09.328286.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0075  Elapsed: 0:00:09.717022.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0107  Elapsed: 0:00:10.108015.\n",
      "  Batch 1,080  of  1,477.  Loss 0.1568  Elapsed: 0:00:10.496176.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0032  Elapsed: 0:00:10.884413.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0214  Elapsed: 0:00:11.276222.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0038  Elapsed: 0:00:11.667297.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0017  Elapsed: 0:00:12.054810.\n",
      "  Batch 1,280  of  1,477.  Loss 0.1823  Elapsed: 0:00:12.443172.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0013  Elapsed: 0:00:12.831707.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0108  Elapsed: 0:00:13.219103.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0027  Elapsed: 0:00:13.608358.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0018  Elapsed: 0:00:13.994949.\n",
      "Avg Validation Loss 0.0920, Completed in 0:00:14.343703 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.901190\n",
      "F1 Score (micro) =   0.902606\n",
      "F1 Score (macro) =   0.832002\n",
      "F1 Score (samples) =   0.889641\n",
      "Running Experiment GEMB_SEM_FR_0_5_0\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.3528  Elapsed: 0:00:00.404420.\n",
      "  Batch    80  of  1,477.  Loss 0.3370  Elapsed: 0:00:00.792511.\n",
      "  Batch   120  of  1,477.  Loss 0.3900  Elapsed: 0:00:01.179084.\n",
      "  Batch   160  of  1,477.  Loss 0.1613  Elapsed: 0:00:01.567240.\n",
      "  Batch   200  of  1,477.  Loss 0.1086  Elapsed: 0:00:01.954691.\n",
      "  Batch   240  of  1,477.  Loss 0.4914  Elapsed: 0:00:02.342026.\n",
      "  Batch   280  of  1,477.  Loss 0.3277  Elapsed: 0:00:02.727134.\n",
      "  Batch   320  of  1,477.  Loss 0.2251  Elapsed: 0:00:03.114615.\n",
      "  Batch   360  of  1,477.  Loss 0.2255  Elapsed: 0:00:03.500830.\n",
      "  Batch   400  of  1,477.  Loss 0.2375  Elapsed: 0:00:03.887692.\n",
      "  Batch   440  of  1,477.  Loss 0.1929  Elapsed: 0:00:04.275879.\n",
      "  Batch   480  of  1,477.  Loss 0.2974  Elapsed: 0:00:04.663492.\n",
      "  Batch   520  of  1,477.  Loss 0.2168  Elapsed: 0:00:05.049036.\n",
      "  Batch   560  of  1,477.  Loss 0.3296  Elapsed: 0:00:05.435484.\n",
      "  Batch   600  of  1,477.  Loss 0.1437  Elapsed: 0:00:05.821317.\n",
      "  Batch   640  of  1,477.  Loss 0.5562  Elapsed: 0:00:06.209436.\n",
      "  Batch   680  of  1,477.  Loss 0.2718  Elapsed: 0:00:06.593434.\n",
      "  Batch   720  of  1,477.  Loss 0.3399  Elapsed: 0:00:06.978759.\n",
      "  Batch   760  of  1,477.  Loss 0.3140  Elapsed: 0:00:07.365349.\n",
      "  Batch   800  of  1,477.  Loss 0.1446  Elapsed: 0:00:07.752550.\n",
      "  Batch   840  of  1,477.  Loss 0.3184  Elapsed: 0:00:08.139244.\n",
      "  Batch   880  of  1,477.  Loss 0.6968  Elapsed: 0:00:08.525907.\n",
      "  Batch   920  of  1,477.  Loss 0.2788  Elapsed: 0:00:08.912867.\n",
      "  Batch   960  of  1,477.  Loss 0.2947  Elapsed: 0:00:09.301539.\n",
      "  Batch 1,000  of  1,477.  Loss 0.3223  Elapsed: 0:00:09.688927.\n",
      "  Batch 1,040  of  1,477.  Loss 0.2047  Elapsed: 0:00:10.076671.\n",
      "  Batch 1,080  of  1,477.  Loss 0.1164  Elapsed: 0:00:10.464599.\n",
      "  Batch 1,120  of  1,477.  Loss 0.3686  Elapsed: 0:00:10.851112.\n",
      "  Batch 1,160  of  1,477.  Loss 0.2626  Elapsed: 0:00:11.236512.\n",
      "  Batch 1,200  of  1,477.  Loss 0.6829  Elapsed: 0:00:11.621233.\n",
      "  Batch 1,240  of  1,477.  Loss 0.3961  Elapsed: 0:00:12.009341.\n",
      "  Batch 1,280  of  1,477.  Loss 0.2232  Elapsed: 0:00:12.395546.\n",
      "  Batch 1,320  of  1,477.  Loss 0.2587  Elapsed: 0:00:12.780978.\n",
      "  Batch 1,360  of  1,477.  Loss 0.2523  Elapsed: 0:00:13.169016.\n",
      "  Batch 1,400  of  1,477.  Loss 0.4257  Elapsed: 0:00:13.555772.\n",
      "  Batch 1,440  of  1,477.  Loss 0.2157  Elapsed: 0:00:13.945637.\n",
      "Avg Validation Loss 0.3452, Completed in 0:00:14.293885 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.282961\n",
      "F1 Score (micro) =   0.288722\n",
      "F1 Score (macro) =   0.220325\n",
      "F1 Score (samples) =   0.191605\n",
      "Running Experiment GEMB_SEM_FR_0_5_100\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.3471, Completed in 0:00:00.598564 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3195, Completed in 0:00:00.563005 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.390685\n",
      "F1 Score (micro) =   0.425000\n",
      "F1 Score (macro) =   0.279349\n",
      "F1 Score (samples) =   0.340000\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2724, Completed in 0:00:00.597641 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3515, Completed in 0:00:00.563378 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.399335\n",
      "F1 Score (micro) =   0.483146\n",
      "F1 Score (macro) =   0.236339\n",
      "F1 Score (samples) =   0.430000\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2703, Completed in 0:00:00.597372 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3625, Completed in 0:00:00.563914 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.355698\n",
      "F1 Score (micro) =   0.469945\n",
      "F1 Score (macro) =   0.175523\n",
      "F1 Score (samples) =   0.430000\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2670, Completed in 0:00:00.597993 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3872, Completed in 0:00:00.564661 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.341423\n",
      "F1 Score (micro) =   0.470588\n",
      "F1 Score (macro) =   0.163524\n",
      "F1 Score (samples) =   0.440000\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of    100.  Loss 0.7274  Elapsed: 0:00:00.400274.\n",
      "  Batch    80  of    100.  Loss 0.2069  Elapsed: 0:00:00.788059.\n",
      "Avg Validation Loss 0.2688, Completed in 0:00:00.971872 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.577658\n",
      "F1 Score (micro) =   0.648649\n",
      "F1 Score (macro) =   0.193146\n",
      "F1 Score (samples) =   0.600000\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0481  Elapsed: 0:00:00.405801.\n",
      "  Batch    80  of  1,477.  Loss 0.1642  Elapsed: 0:00:00.793514.\n",
      "  Batch   120  of  1,477.  Loss 0.0482  Elapsed: 0:00:01.181581.\n",
      "  Batch   160  of  1,477.  Loss 0.0970  Elapsed: 0:00:01.569400.\n",
      "  Batch   200  of  1,477.  Loss 0.1492  Elapsed: 0:00:01.957251.\n",
      "  Batch   240  of  1,477.  Loss 0.0510  Elapsed: 0:00:02.343414.\n",
      "  Batch   280  of  1,477.  Loss 0.7535  Elapsed: 0:00:02.732792.\n",
      "  Batch   320  of  1,477.  Loss 0.4749  Elapsed: 0:00:03.120484.\n",
      "  Batch   360  of  1,477.  Loss 0.0640  Elapsed: 0:00:03.507731.\n",
      "  Batch   400  of  1,477.  Loss 0.7696  Elapsed: 0:00:03.895112.\n",
      "  Batch   440  of  1,477.  Loss 0.5958  Elapsed: 0:00:04.284492.\n",
      "  Batch   480  of  1,477.  Loss 0.3597  Elapsed: 0:00:04.675513.\n",
      "  Batch   520  of  1,477.  Loss 0.0495  Elapsed: 0:00:05.062514.\n",
      "  Batch   560  of  1,477.  Loss 0.0920  Elapsed: 0:00:05.451194.\n",
      "  Batch   600  of  1,477.  Loss 0.0311  Elapsed: 0:00:05.838466.\n",
      "  Batch   640  of  1,477.  Loss 0.3762  Elapsed: 0:00:06.225657.\n",
      "  Batch   680  of  1,477.  Loss 0.1675  Elapsed: 0:00:06.611760.\n",
      "  Batch   720  of  1,477.  Loss 0.0443  Elapsed: 0:00:07.001703.\n",
      "  Batch   760  of  1,477.  Loss 0.2678  Elapsed: 0:00:07.387393.\n",
      "  Batch   800  of  1,477.  Loss 0.5573  Elapsed: 0:00:07.774870.\n",
      "  Batch   840  of  1,477.  Loss 0.0738  Elapsed: 0:00:08.164234.\n",
      "  Batch   880  of  1,477.  Loss 0.4872  Elapsed: 0:00:08.554961.\n",
      "  Batch   920  of  1,477.  Loss 0.1034  Elapsed: 0:00:08.945059.\n",
      "  Batch   960  of  1,477.  Loss 0.0635  Elapsed: 0:00:09.337373.\n",
      "  Batch 1,000  of  1,477.  Loss 0.4662  Elapsed: 0:00:09.730797.\n",
      "  Batch 1,040  of  1,477.  Loss 0.1280  Elapsed: 0:00:10.120928.\n",
      "  Batch 1,080  of  1,477.  Loss 0.8092  Elapsed: 0:00:10.518329.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0227  Elapsed: 0:00:10.908151.\n",
      "  Batch 1,160  of  1,477.  Loss 0.7833  Elapsed: 0:00:11.298947.\n",
      "  Batch 1,200  of  1,477.  Loss 0.3198  Elapsed: 0:00:11.689705.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0474  Elapsed: 0:00:12.078837.\n",
      "  Batch 1,280  of  1,477.  Loss 0.9790  Elapsed: 0:00:12.476657.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0899  Elapsed: 0:00:12.863524.\n",
      "  Batch 1,360  of  1,477.  Loss 0.1919  Elapsed: 0:00:13.250535.\n",
      "  Batch 1,400  of  1,477.  Loss 0.6097  Elapsed: 0:00:13.639969.\n",
      "  Batch 1,440  of  1,477.  Loss 0.5758  Elapsed: 0:00:14.026499.\n",
      "Avg Validation Loss 0.3073, Completed in 0:00:14.375473 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.454153\n",
      "F1 Score (micro) =   0.557887\n",
      "F1 Score (macro) =   0.210974\n",
      "F1 Score (samples) =   0.519522\n",
      "Running Experiment GEMB_SEM_FR_0_5_200\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.3038, Completed in 0:00:01.190628 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3274, Completed in 0:00:01.122835 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.387074\n",
      "F1 Score (micro) =   0.457971\n",
      "F1 Score (macro) =   0.211013\n",
      "F1 Score (samples) =   0.393333\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2517, Completed in 0:00:01.188191 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3397, Completed in 0:00:01.124135 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.361298\n",
      "F1 Score (micro) =   0.471545\n",
      "F1 Score (macro) =   0.179757\n",
      "F1 Score (samples) =   0.431667\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2453, Completed in 0:00:01.188354 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3476, Completed in 0:00:01.122797 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.354449\n",
      "F1 Score (micro) =   0.477454\n",
      "F1 Score (macro) =   0.167666\n",
      "F1 Score (samples) =   0.448333\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2494, Completed in 0:00:01.188498 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3552, Completed in 0:00:01.122553 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.353138\n",
      "F1 Score (micro) =   0.474667\n",
      "F1 Score (macro) =   0.167272\n",
      "F1 Score (samples) =   0.443333\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of    200.  Loss 0.2196  Elapsed: 0:00:00.395155.\n",
      "  Batch    80  of    200.  Loss 0.1137  Elapsed: 0:00:00.781768.\n",
      "  Batch   120  of    200.  Loss 0.5421  Elapsed: 0:00:01.166884.\n",
      "  Batch   160  of    200.  Loss 0.0329  Elapsed: 0:00:01.554357.\n",
      "Avg Validation Loss 0.2445, Completed in 0:00:01.931954 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.525647\n",
      "F1 Score (micro) =   0.622590\n",
      "F1 Score (macro) =   0.149060\n",
      "F1 Score (samples) =   0.565000\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.4127  Elapsed: 0:00:00.399040.\n",
      "  Batch    80  of  1,477.  Loss 0.0766  Elapsed: 0:00:00.787719.\n",
      "  Batch   120  of  1,477.  Loss 0.5894  Elapsed: 0:00:01.174067.\n",
      "  Batch   160  of  1,477.  Loss 0.2027  Elapsed: 0:00:01.559259.\n",
      "  Batch   200  of  1,477.  Loss 0.0169  Elapsed: 0:00:01.944852.\n",
      "  Batch   240  of  1,477.  Loss 0.4053  Elapsed: 0:00:02.330976.\n",
      "  Batch   280  of  1,477.  Loss 0.0587  Elapsed: 0:00:02.717657.\n",
      "  Batch   320  of  1,477.  Loss 0.0447  Elapsed: 0:00:03.106221.\n",
      "  Batch   360  of  1,477.  Loss 0.0521  Elapsed: 0:00:03.491730.\n",
      "  Batch   400  of  1,477.  Loss 0.4562  Elapsed: 0:00:03.878348.\n",
      "  Batch   440  of  1,477.  Loss 0.2337  Elapsed: 0:00:04.265651.\n",
      "  Batch   480  of  1,477.  Loss 0.0584  Elapsed: 0:00:04.652335.\n",
      "  Batch   520  of  1,477.  Loss 0.2486  Elapsed: 0:00:05.038554.\n",
      "  Batch   560  of  1,477.  Loss 1.0037  Elapsed: 0:00:05.424302.\n",
      "  Batch   600  of  1,477.  Loss 0.0444  Elapsed: 0:00:05.810041.\n",
      "  Batch   640  of  1,477.  Loss 0.1203  Elapsed: 0:00:06.196075.\n",
      "  Batch   680  of  1,477.  Loss 0.9318  Elapsed: 0:00:06.583323.\n",
      "  Batch   720  of  1,477.  Loss 0.0858  Elapsed: 0:00:06.970125.\n",
      "  Batch   760  of  1,477.  Loss 0.0906  Elapsed: 0:00:07.359478.\n",
      "  Batch   800  of  1,477.  Loss 0.4716  Elapsed: 0:00:07.746820.\n",
      "  Batch   840  of  1,477.  Loss 0.0170  Elapsed: 0:00:08.135854.\n",
      "  Batch   880  of  1,477.  Loss 0.4608  Elapsed: 0:00:08.523673.\n",
      "  Batch   920  of  1,477.  Loss 0.4225  Elapsed: 0:00:08.912215.\n",
      "  Batch   960  of  1,477.  Loss 0.1015  Elapsed: 0:00:09.299201.\n",
      "  Batch 1,000  of  1,477.  Loss 0.3940  Elapsed: 0:00:09.686223.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0668  Elapsed: 0:00:10.075413.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0429  Elapsed: 0:00:10.463438.\n",
      "  Batch 1,120  of  1,477.  Loss 0.2394  Elapsed: 0:00:10.849305.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0462  Elapsed: 0:00:11.236548.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0426  Elapsed: 0:00:11.623676.\n",
      "  Batch 1,240  of  1,477.  Loss 0.4260  Elapsed: 0:00:12.011160.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0379  Elapsed: 0:00:12.400154.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0335  Elapsed: 0:00:12.786065.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0567  Elapsed: 0:00:13.173365.\n",
      "  Batch 1,400  of  1,477.  Loss 0.1490  Elapsed: 0:00:13.561498.\n",
      "  Batch 1,440  of  1,477.  Loss 0.1819  Elapsed: 0:00:13.948475.\n",
      "Avg Validation Loss 0.3150, Completed in 0:00:14.297688 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.431549\n",
      "F1 Score (micro) =   0.549691\n",
      "F1 Score (macro) =   0.173395\n",
      "F1 Score (samples) =   0.510494\n",
      "Running Experiment GEMB_SEM_FR_0_5_300\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.3058, Completed in 0:00:01.787138 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3096, Completed in 0:00:01.690020 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.446838\n",
      "F1 Score (micro) =   0.530612\n",
      "F1 Score (macro) =   0.250377\n",
      "F1 Score (samples) =   0.474444\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2800, Completed in 0:00:01.783776 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3155, Completed in 0:00:01.690161 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.404398\n",
      "F1 Score (micro) =   0.510870\n",
      "F1 Score (macro) =   0.200206\n",
      "F1 Score (samples) =   0.468889\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2721, Completed in 0:00:01.784247 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3132, Completed in 0:00:01.691296 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.409743\n",
      "F1 Score (micro) =   0.515654\n",
      "F1 Score (macro) =   0.201797\n",
      "F1 Score (samples) =   0.465556\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2716, Completed in 0:00:01.782714 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3079, Completed in 0:00:01.689931 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.409965\n",
      "F1 Score (micro) =   0.513109\n",
      "F1 Score (macro) =   0.201863\n",
      "F1 Score (samples) =   0.456667\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of    300.  Loss 0.6925  Elapsed: 0:00:00.403967.\n",
      "  Batch    80  of    300.  Loss 0.0408  Elapsed: 0:00:00.793226.\n",
      "  Batch   120  of    300.  Loss 0.2341  Elapsed: 0:00:01.183736.\n",
      "  Batch   160  of    300.  Loss 0.0894  Elapsed: 0:00:01.575580.\n",
      "  Batch   200  of    300.  Loss 0.0478  Elapsed: 0:00:01.965710.\n",
      "  Batch   240  of    300.  Loss 0.7506  Elapsed: 0:00:02.354942.\n",
      "  Batch   280  of    300.  Loss 0.6117  Elapsed: 0:00:02.743232.\n",
      "Avg Validation Loss 0.2670, Completed in 0:00:02.928050 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.528653\n",
      "F1 Score (micro) =   0.599251\n",
      "F1 Score (macro) =   0.244811\n",
      "F1 Score (samples) =   0.532222\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.1003  Elapsed: 0:00:00.397198.\n",
      "  Batch    80  of  1,477.  Loss 0.2199  Elapsed: 0:00:00.785388.\n",
      "  Batch   120  of  1,477.  Loss 0.0827  Elapsed: 0:00:01.174611.\n",
      "  Batch   160  of  1,477.  Loss 0.7505  Elapsed: 0:00:01.565048.\n",
      "  Batch   200  of  1,477.  Loss 0.0619  Elapsed: 0:00:01.953194.\n",
      "  Batch   240  of  1,477.  Loss 0.6673  Elapsed: 0:00:02.341632.\n",
      "  Batch   280  of  1,477.  Loss 0.5564  Elapsed: 0:00:02.729190.\n",
      "  Batch   320  of  1,477.  Loss 0.1942  Elapsed: 0:00:03.118352.\n",
      "  Batch   360  of  1,477.  Loss 0.1152  Elapsed: 0:00:03.504278.\n",
      "  Batch   400  of  1,477.  Loss 0.4693  Elapsed: 0:00:03.891256.\n",
      "  Batch   440  of  1,477.  Loss 0.1104  Elapsed: 0:00:04.277371.\n",
      "  Batch   480  of  1,477.  Loss 0.1340  Elapsed: 0:00:04.661639.\n",
      "  Batch   520  of  1,477.  Loss 0.3025  Elapsed: 0:00:05.047681.\n",
      "  Batch   560  of  1,477.  Loss 0.0472  Elapsed: 0:00:05.435113.\n",
      "  Batch   600  of  1,477.  Loss 0.0346  Elapsed: 0:00:05.820992.\n",
      "  Batch   640  of  1,477.  Loss 0.2858  Elapsed: 0:00:06.206985.\n",
      "  Batch   680  of  1,477.  Loss 0.0563  Elapsed: 0:00:06.592533.\n",
      "  Batch   720  of  1,477.  Loss 0.0589  Elapsed: 0:00:06.979578.\n",
      "  Batch   760  of  1,477.  Loss 0.3034  Elapsed: 0:00:07.365912.\n",
      "  Batch   800  of  1,477.  Loss 0.1497  Elapsed: 0:00:07.752798.\n",
      "  Batch   840  of  1,477.  Loss 0.0424  Elapsed: 0:00:08.137076.\n",
      "  Batch   880  of  1,477.  Loss 0.1400  Elapsed: 0:00:08.524201.\n",
      "  Batch   920  of  1,477.  Loss 0.1849  Elapsed: 0:00:08.911881.\n",
      "  Batch   960  of  1,477.  Loss 0.1740  Elapsed: 0:00:09.299302.\n",
      "  Batch 1,000  of  1,477.  Loss 0.3321  Elapsed: 0:00:09.685886.\n",
      "  Batch 1,040  of  1,477.  Loss 0.1427  Elapsed: 0:00:10.073071.\n",
      "  Batch 1,080  of  1,477.  Loss 0.1842  Elapsed: 0:00:10.459083.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0384  Elapsed: 0:00:10.845008.\n",
      "  Batch 1,160  of  1,477.  Loss 0.1434  Elapsed: 0:00:11.232823.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0617  Elapsed: 0:00:11.616940.\n",
      "  Batch 1,240  of  1,477.  Loss 0.1095  Elapsed: 0:00:12.001909.\n",
      "  Batch 1,280  of  1,477.  Loss 0.6901  Elapsed: 0:00:12.387260.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0724  Elapsed: 0:00:12.772769.\n",
      "  Batch 1,360  of  1,477.  Loss 0.0481  Elapsed: 0:00:13.159755.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0390  Elapsed: 0:00:13.542426.\n",
      "  Batch 1,440  of  1,477.  Loss 0.3716  Elapsed: 0:00:13.927871.\n",
      "Avg Validation Loss 0.2987, Completed in 0:00:14.275007 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.434106\n",
      "F1 Score (micro) =   0.538490\n",
      "F1 Score (macro) =   0.181142\n",
      "F1 Score (samples) =   0.484541\n",
      "Running Experiment GEMB_SEM_FR_0_5_400\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.3101, Completed in 0:00:02.374979 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3151, Completed in 0:00:02.252020 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.435106\n",
      "F1 Score (micro) =   0.523416\n",
      "F1 Score (macro) =   0.242430\n",
      "F1 Score (samples) =   0.472500\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2837, Completed in 0:00:02.371597 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3149, Completed in 0:00:02.249574 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.419614\n",
      "F1 Score (micro) =   0.521262\n",
      "F1 Score (macro) =   0.214702\n",
      "F1 Score (samples) =   0.472500\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2706, Completed in 0:00:02.373070 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3109, Completed in 0:00:02.249234 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.417055\n",
      "F1 Score (micro) =   0.513475\n",
      "F1 Score (macro) =   0.214511\n",
      "F1 Score (samples) =   0.451667\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2696, Completed in 0:00:02.372715 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3101, Completed in 0:00:02.249088 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.413888\n",
      "F1 Score (micro) =   0.509194\n",
      "F1 Score (macro) =   0.213333\n",
      "F1 Score (samples) =   0.449167\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of    400.  Loss 0.6495  Elapsed: 0:00:00.395716.\n",
      "  Batch    80  of    400.  Loss 0.0499  Elapsed: 0:00:00.780513.\n",
      "  Batch   120  of    400.  Loss 0.0561  Elapsed: 0:00:01.166470.\n",
      "  Batch   160  of    400.  Loss 0.4505  Elapsed: 0:00:01.557152.\n",
      "  Batch   200  of    400.  Loss 0.4070  Elapsed: 0:00:01.952424.\n",
      "  Batch   240  of    400.  Loss 0.3378  Elapsed: 0:00:02.340800.\n",
      "  Batch   280  of    400.  Loss 0.3475  Elapsed: 0:00:02.726608.\n",
      "  Batch   320  of    400.  Loss 0.5752  Elapsed: 0:00:03.112136.\n",
      "  Batch   360  of    400.  Loss 0.0865  Elapsed: 0:00:03.495131.\n",
      "Avg Validation Loss 0.2653, Completed in 0:00:03.870799 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.493146\n",
      "F1 Score (micro) =   0.562942\n",
      "F1 Score (macro) =   0.217027\n",
      "F1 Score (samples) =   0.495833\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.8936  Elapsed: 0:00:00.394870.\n",
      "  Batch    80  of  1,477.  Loss 0.4545  Elapsed: 0:00:00.778351.\n",
      "  Batch   120  of  1,477.  Loss 0.3329  Elapsed: 0:00:01.162199.\n",
      "  Batch   160  of  1,477.  Loss 0.5625  Elapsed: 0:00:01.546733.\n",
      "  Batch   200  of  1,477.  Loss 0.3641  Elapsed: 0:00:01.929230.\n",
      "  Batch   240  of  1,477.  Loss 0.4447  Elapsed: 0:00:02.314571.\n",
      "  Batch   280  of  1,477.  Loss 0.0869  Elapsed: 0:00:02.699557.\n",
      "  Batch   320  of  1,477.  Loss 0.1307  Elapsed: 0:00:03.085732.\n",
      "  Batch   360  of  1,477.  Loss 0.0813  Elapsed: 0:00:03.471081.\n",
      "  Batch   400  of  1,477.  Loss 0.3313  Elapsed: 0:00:03.855252.\n",
      "  Batch   440  of  1,477.  Loss 0.2379  Elapsed: 0:00:04.239223.\n",
      "  Batch   480  of  1,477.  Loss 0.7747  Elapsed: 0:00:04.623601.\n",
      "  Batch   520  of  1,477.  Loss 0.4917  Elapsed: 0:00:05.011266.\n",
      "  Batch   560  of  1,477.  Loss 0.2687  Elapsed: 0:00:05.397537.\n",
      "  Batch   600  of  1,477.  Loss 0.9943  Elapsed: 0:00:05.781805.\n",
      "  Batch   640  of  1,477.  Loss 0.0675  Elapsed: 0:00:06.166465.\n",
      "  Batch   680  of  1,477.  Loss 0.3719  Elapsed: 0:00:06.551062.\n",
      "  Batch   720  of  1,477.  Loss 0.1253  Elapsed: 0:00:06.935241.\n",
      "  Batch   760  of  1,477.  Loss 0.8813  Elapsed: 0:00:07.320077.\n",
      "  Batch   800  of  1,477.  Loss 0.5521  Elapsed: 0:00:07.704875.\n",
      "  Batch   840  of  1,477.  Loss 0.2952  Elapsed: 0:00:08.091331.\n",
      "  Batch   880  of  1,477.  Loss 0.7977  Elapsed: 0:00:08.477749.\n",
      "  Batch   920  of  1,477.  Loss 0.1347  Elapsed: 0:00:08.861181.\n",
      "  Batch   960  of  1,477.  Loss 0.0413  Elapsed: 0:00:09.246072.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0383  Elapsed: 0:00:09.629228.\n",
      "  Batch 1,040  of  1,477.  Loss 0.5123  Elapsed: 0:00:10.012863.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0455  Elapsed: 0:00:10.399993.\n",
      "  Batch 1,120  of  1,477.  Loss 0.5280  Elapsed: 0:00:10.784362.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0567  Elapsed: 0:00:11.170601.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0836  Elapsed: 0:00:11.555789.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0537  Elapsed: 0:00:11.939804.\n",
      "  Batch 1,280  of  1,477.  Loss 0.5286  Elapsed: 0:00:12.325842.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0704  Elapsed: 0:00:12.711448.\n",
      "  Batch 1,360  of  1,477.  Loss 0.7388  Elapsed: 0:00:13.096980.\n",
      "  Batch 1,400  of  1,477.  Loss 0.7482  Elapsed: 0:00:13.479365.\n",
      "  Batch 1,440  of  1,477.  Loss 1.0923  Elapsed: 0:00:13.865326.\n",
      "Avg Validation Loss 0.3013, Completed in 0:00:14.212137 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.423890\n",
      "F1 Score (micro) =   0.534134\n",
      "F1 Score (macro) =   0.166188\n",
      "F1 Score (samples) =   0.481607\n",
      "Running Experiment GEMB_SEM_FR_0_5_500\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2939, Completed in 0:00:02.973677 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3250, Completed in 0:00:02.805516 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.432551\n",
      "F1 Score (micro) =   0.538049\n",
      "F1 Score (macro) =   0.233602\n",
      "F1 Score (samples) =   0.500000\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2655, Completed in 0:00:02.971735 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3186, Completed in 0:00:02.804342 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.408595\n",
      "F1 Score (micro) =   0.519912\n",
      "F1 Score (macro) =   0.212797\n",
      "F1 Score (samples) =   0.469333\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2636, Completed in 0:00:02.968780 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3128, Completed in 0:00:02.803681 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.399240\n",
      "F1 Score (micro) =   0.512821\n",
      "F1 Score (macro) =   0.201792\n",
      "F1 Score (samples) =   0.459333\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2566, Completed in 0:00:02.969351 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "Avg Validation Loss 0.3066, Completed in 0:00:02.804215 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.404477\n",
      "F1 Score (micro) =   0.515186\n",
      "F1 Score (macro) =   0.207212\n",
      "F1 Score (samples) =   0.456667\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of    500.  Loss 0.1446  Elapsed: 0:00:00.400644.\n",
      "  Batch    80  of    500.  Loss 0.1408  Elapsed: 0:00:00.788893.\n",
      "  Batch   120  of    500.  Loss 0.0707  Elapsed: 0:00:01.176158.\n",
      "  Batch   160  of    500.  Loss 0.0229  Elapsed: 0:00:01.565645.\n",
      "  Batch   200  of    500.  Loss 0.2351  Elapsed: 0:00:01.953956.\n",
      "  Batch   240  of    500.  Loss 0.1040  Elapsed: 0:00:02.340407.\n",
      "  Batch   280  of    500.  Loss 0.2268  Elapsed: 0:00:02.726452.\n",
      "  Batch   320  of    500.  Loss 0.8128  Elapsed: 0:00:03.116020.\n",
      "  Batch   360  of    500.  Loss 0.3897  Elapsed: 0:00:03.502532.\n",
      "  Batch   400  of    500.  Loss 0.6271  Elapsed: 0:00:03.890554.\n",
      "  Batch   440  of    500.  Loss 0.1281  Elapsed: 0:00:04.277968.\n",
      "  Batch   480  of    500.  Loss 0.6707  Elapsed: 0:00:04.666466.\n",
      "Avg Validation Loss 0.2517, Completed in 0:00:04.852530 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.529415\n",
      "F1 Score (micro) =   0.603816\n",
      "F1 Score (macro) =   0.238341\n",
      "F1 Score (samples) =   0.537333\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.0764  Elapsed: 0:00:00.398709.\n",
      "  Batch    80  of  1,477.  Loss 0.4224  Elapsed: 0:00:00.787446.\n",
      "  Batch   120  of  1,477.  Loss 0.0861  Elapsed: 0:00:01.178762.\n",
      "  Batch   160  of  1,477.  Loss 0.3880  Elapsed: 0:00:01.566707.\n",
      "  Batch   200  of  1,477.  Loss 0.0453  Elapsed: 0:00:01.956212.\n",
      "  Batch   240  of  1,477.  Loss 0.2556  Elapsed: 0:00:02.344771.\n",
      "  Batch   280  of  1,477.  Loss 0.2647  Elapsed: 0:00:02.732885.\n",
      "  Batch   320  of  1,477.  Loss 0.0431  Elapsed: 0:00:03.125374.\n",
      "  Batch   360  of  1,477.  Loss 0.0233  Elapsed: 0:00:03.514952.\n",
      "  Batch   400  of  1,477.  Loss 0.2346  Elapsed: 0:00:03.905498.\n",
      "  Batch   440  of  1,477.  Loss 0.3217  Elapsed: 0:00:04.294345.\n",
      "  Batch   480  of  1,477.  Loss 0.2391  Elapsed: 0:00:04.680922.\n",
      "  Batch   520  of  1,477.  Loss 0.0611  Elapsed: 0:00:05.070528.\n",
      "  Batch   560  of  1,477.  Loss 0.0915  Elapsed: 0:00:05.457013.\n",
      "  Batch   600  of  1,477.  Loss 0.1006  Elapsed: 0:00:05.850009.\n",
      "  Batch   640  of  1,477.  Loss 0.0599  Elapsed: 0:00:06.237626.\n",
      "  Batch   680  of  1,477.  Loss 0.3563  Elapsed: 0:00:06.627019.\n",
      "  Batch   720  of  1,477.  Loss 0.6167  Elapsed: 0:00:07.014933.\n",
      "  Batch   760  of  1,477.  Loss 0.2246  Elapsed: 0:00:07.404005.\n",
      "  Batch   800  of  1,477.  Loss 0.1347  Elapsed: 0:00:07.792802.\n",
      "  Batch   840  of  1,477.  Loss 0.1658  Elapsed: 0:00:08.182865.\n",
      "  Batch   880  of  1,477.  Loss 0.0360  Elapsed: 0:00:08.573856.\n",
      "  Batch   920  of  1,477.  Loss 0.4474  Elapsed: 0:00:08.963737.\n",
      "  Batch   960  of  1,477.  Loss 0.0414  Elapsed: 0:00:09.352498.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0505  Elapsed: 0:00:09.741858.\n",
      "  Batch 1,040  of  1,477.  Loss 0.3834  Elapsed: 0:00:10.132233.\n",
      "  Batch 1,080  of  1,477.  Loss 0.1347  Elapsed: 0:00:10.522223.\n",
      "  Batch 1,120  of  1,477.  Loss 0.5955  Elapsed: 0:00:10.909430.\n",
      "  Batch 1,160  of  1,477.  Loss 0.1295  Elapsed: 0:00:11.296848.\n",
      "  Batch 1,200  of  1,477.  Loss 0.8328  Elapsed: 0:00:11.685130.\n",
      "  Batch 1,240  of  1,477.  Loss 0.4280  Elapsed: 0:00:12.073472.\n",
      "  Batch 1,280  of  1,477.  Loss 0.2036  Elapsed: 0:00:12.462783.\n",
      "  Batch 1,320  of  1,477.  Loss 0.1391  Elapsed: 0:00:12.851029.\n",
      "  Batch 1,360  of  1,477.  Loss 0.5446  Elapsed: 0:00:13.240086.\n",
      "  Batch 1,400  of  1,477.  Loss 0.1010  Elapsed: 0:00:13.629670.\n",
      "  Batch 1,440  of  1,477.  Loss 0.9409  Elapsed: 0:00:14.015846.\n",
      "Avg Validation Loss 0.2980, Completed in 0:00:14.366907 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.429850\n",
      "F1 Score (micro) =   0.540313\n",
      "F1 Score (macro) =   0.174269\n",
      "F1 Score (samples) =   0.477996\n",
      "Running Experiment GEMB_SEM_FR_0_5_1000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2723, Completed in 0:00:05.927942 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     63.  Loss 0.2931  Elapsed: 0:00:03.682478.\n",
      "Avg Validation Loss 0.3333, Completed in 0:00:05.621629 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.384849\n",
      "F1 Score (micro) =   0.509444\n",
      "F1 Score (macro) =   0.180274\n",
      "F1 Score (samples) =   0.470667\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2546, Completed in 0:00:05.926395 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     63.  Loss 0.4400  Elapsed: 0:00:03.682340.\n",
      "Avg Validation Loss 0.3177, Completed in 0:00:05.620204 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.389865\n",
      "F1 Score (micro) =   0.507246\n",
      "F1 Score (macro) =   0.186150\n",
      "F1 Score (samples) =   0.454333\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2459, Completed in 0:00:05.921430 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     63.  Loss 0.2971  Elapsed: 0:00:03.685741.\n",
      "Avg Validation Loss 0.3178, Completed in 0:00:05.619470 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.387771\n",
      "F1 Score (micro) =   0.510708\n",
      "F1 Score (macro) =   0.179373\n",
      "F1 Score (samples) =   0.464667\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2427, Completed in 0:00:05.918206 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     63.  Loss 0.3584  Elapsed: 0:00:03.682073.\n",
      "Avg Validation Loss 0.3240, Completed in 0:00:05.620723 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.384386\n",
      "F1 Score (micro) =   0.516129\n",
      "F1 Score (macro) =   0.169751\n",
      "F1 Score (samples) =   0.479333\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,000.  Loss 0.1711  Elapsed: 0:00:00.397818.\n",
      "  Batch    80  of  1,000.  Loss 0.0277  Elapsed: 0:00:00.786944.\n",
      "  Batch   120  of  1,000.  Loss 0.0324  Elapsed: 0:00:01.174484.\n",
      "  Batch   160  of  1,000.  Loss 0.1804  Elapsed: 0:00:01.563696.\n",
      "  Batch   200  of  1,000.  Loss 0.0258  Elapsed: 0:00:01.952233.\n",
      "  Batch   240  of  1,000.  Loss 0.8117  Elapsed: 0:00:02.339727.\n",
      "  Batch   280  of  1,000.  Loss 0.1523  Elapsed: 0:00:02.726898.\n",
      "  Batch   320  of  1,000.  Loss 0.0654  Elapsed: 0:00:03.116322.\n",
      "  Batch   360  of  1,000.  Loss 0.1986  Elapsed: 0:00:03.506519.\n",
      "  Batch   400  of  1,000.  Loss 0.3819  Elapsed: 0:00:03.895250.\n",
      "  Batch   440  of  1,000.  Loss 0.6654  Elapsed: 0:00:04.281833.\n",
      "  Batch   480  of  1,000.  Loss 0.0310  Elapsed: 0:00:04.671492.\n",
      "  Batch   520  of  1,000.  Loss 0.6185  Elapsed: 0:00:05.062157.\n",
      "  Batch   560  of  1,000.  Loss 0.0247  Elapsed: 0:00:05.457788.\n",
      "  Batch   600  of  1,000.  Loss 0.0402  Elapsed: 0:00:05.850969.\n",
      "  Batch   640  of  1,000.  Loss 0.3989  Elapsed: 0:00:06.239122.\n",
      "  Batch   680  of  1,000.  Loss 0.9175  Elapsed: 0:00:06.627014.\n",
      "  Batch   720  of  1,000.  Loss 0.0710  Elapsed: 0:00:07.023698.\n",
      "  Batch   760  of  1,000.  Loss 0.0983  Elapsed: 0:00:07.414107.\n",
      "  Batch   800  of  1,000.  Loss 0.4812  Elapsed: 0:00:07.801639.\n",
      "  Batch   840  of  1,000.  Loss 0.1312  Elapsed: 0:00:08.191473.\n",
      "  Batch   880  of  1,000.  Loss 0.3919  Elapsed: 0:00:08.579953.\n",
      "  Batch   920  of  1,000.  Loss 0.7103  Elapsed: 0:00:08.965851.\n",
      "  Batch   960  of  1,000.  Loss 0.1587  Elapsed: 0:00:09.352673.\n",
      "Avg Validation Loss 0.2397, Completed in 0:00:09.731222 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.565599\n",
      "F1 Score (micro) =   0.661654\n",
      "F1 Score (macro) =   0.203193\n",
      "F1 Score (samples) =   0.614333\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.1451  Elapsed: 0:00:00.408642.\n",
      "  Batch    80  of  1,477.  Loss 0.0832  Elapsed: 0:00:00.796231.\n",
      "  Batch   120  of  1,477.  Loss 0.0335  Elapsed: 0:00:01.182885.\n",
      "  Batch   160  of  1,477.  Loss 0.7038  Elapsed: 0:00:01.567060.\n",
      "  Batch   200  of  1,477.  Loss 0.4247  Elapsed: 0:00:01.956001.\n",
      "  Batch   240  of  1,477.  Loss 0.0601  Elapsed: 0:00:02.339769.\n",
      "  Batch   280  of  1,477.  Loss 0.0330  Elapsed: 0:00:02.727473.\n",
      "  Batch   320  of  1,477.  Loss 0.6274  Elapsed: 0:00:03.117584.\n",
      "  Batch   360  of  1,477.  Loss 0.4904  Elapsed: 0:00:03.505047.\n",
      "  Batch   400  of  1,477.  Loss 0.3924  Elapsed: 0:00:03.893558.\n",
      "  Batch   440  of  1,477.  Loss 0.4650  Elapsed: 0:00:04.281770.\n",
      "  Batch   480  of  1,477.  Loss 0.5215  Elapsed: 0:00:04.669781.\n",
      "  Batch   520  of  1,477.  Loss 0.6800  Elapsed: 0:00:05.058729.\n",
      "  Batch   560  of  1,477.  Loss 0.0677  Elapsed: 0:00:05.448539.\n",
      "  Batch   600  of  1,477.  Loss 0.0249  Elapsed: 0:00:05.837280.\n",
      "  Batch   640  of  1,477.  Loss 0.0357  Elapsed: 0:00:06.227115.\n",
      "  Batch   680  of  1,477.  Loss 0.0917  Elapsed: 0:00:06.615336.\n",
      "  Batch   720  of  1,477.  Loss 0.0700  Elapsed: 0:00:07.003970.\n",
      "  Batch   760  of  1,477.  Loss 0.0741  Elapsed: 0:00:07.393412.\n",
      "  Batch   800  of  1,477.  Loss 0.1209  Elapsed: 0:00:07.781061.\n",
      "  Batch   840  of  1,477.  Loss 0.0641  Elapsed: 0:00:08.168877.\n",
      "  Batch   880  of  1,477.  Loss 0.0378  Elapsed: 0:00:08.556387.\n",
      "  Batch   920  of  1,477.  Loss 0.3627  Elapsed: 0:00:08.948649.\n",
      "  Batch   960  of  1,477.  Loss 0.8777  Elapsed: 0:00:09.336587.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0589  Elapsed: 0:00:09.724173.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0427  Elapsed: 0:00:10.113072.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0565  Elapsed: 0:00:10.500044.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0783  Elapsed: 0:00:10.887454.\n",
      "  Batch 1,160  of  1,477.  Loss 0.1297  Elapsed: 0:00:11.274501.\n",
      "  Batch 1,200  of  1,477.  Loss 0.1249  Elapsed: 0:00:11.663623.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0369  Elapsed: 0:00:12.051775.\n",
      "  Batch 1,280  of  1,477.  Loss 0.5044  Elapsed: 0:00:12.438234.\n",
      "  Batch 1,320  of  1,477.  Loss 0.1024  Elapsed: 0:00:12.825496.\n",
      "  Batch 1,360  of  1,477.  Loss 0.1416  Elapsed: 0:00:13.213164.\n",
      "  Batch 1,400  of  1,477.  Loss 0.7296  Elapsed: 0:00:13.601472.\n",
      "  Batch 1,440  of  1,477.  Loss 0.8872  Elapsed: 0:00:13.990365.\n",
      "Avg Validation Loss 0.3083, Completed in 0:00:14.339296 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.418417\n",
      "F1 Score (micro) =   0.550872\n",
      "F1 Score (macro) =   0.150625\n",
      "F1 Score (samples) =   0.512751\n",
      "Running Experiment GEMB_SEM_FR_0_5_1500\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2805, Completed in 0:00:08.893989 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3177  Elapsed: 0:00:03.687169.\n",
      "  Batch    80  of     93.  Loss 0.2796  Elapsed: 0:00:07.286786.\n",
      "Avg Validation Loss 0.3271, Completed in 0:00:08.301104 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.378522\n",
      "F1 Score (micro) =   0.508511\n",
      "F1 Score (macro) =   0.164930\n",
      "F1 Score (samples) =   0.474611\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2613, Completed in 0:00:08.893447 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2053  Elapsed: 0:00:03.687135.\n",
      "  Batch    80  of     93.  Loss 0.2829  Elapsed: 0:00:07.280579.\n",
      "Avg Validation Loss 0.3186, Completed in 0:00:08.300359 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.376527\n",
      "F1 Score (micro) =   0.504604\n",
      "F1 Score (macro) =   0.162654\n",
      "F1 Score (samples) =   0.463327\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2559, Completed in 0:00:08.896788 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3081  Elapsed: 0:00:03.688194.\n",
      "  Batch    80  of     93.  Loss 0.2005  Elapsed: 0:00:07.287309.\n",
      "Avg Validation Loss 0.3111, Completed in 0:00:08.306147 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.391383\n",
      "F1 Score (micro) =   0.508321\n",
      "F1 Score (macro) =   0.174598\n",
      "F1 Score (samples) =   0.454751\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Avg Training Loss 0.2541, Completed in 0:00:08.894871 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2964  Elapsed: 0:00:03.686496.\n",
      "  Batch    80  of     93.  Loss 0.2138  Elapsed: 0:00:07.290032.\n",
      "Avg Validation Loss 0.3142, Completed in 0:00:08.304688 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.383550\n",
      "F1 Score (micro) =   0.513028\n",
      "F1 Score (macro) =   0.166467\n",
      "F1 Score (samples) =   0.472354\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,500.  Loss 0.8812  Elapsed: 0:00:00.395054.\n",
      "  Batch    80  of  1,500.  Loss 0.4568  Elapsed: 0:00:00.780998.\n",
      "  Batch   120  of  1,500.  Loss 0.4328  Elapsed: 0:00:01.167046.\n",
      "  Batch   160  of  1,500.  Loss 0.4293  Elapsed: 0:00:01.551565.\n",
      "  Batch   200  of  1,500.  Loss 0.1408  Elapsed: 0:00:01.937047.\n",
      "  Batch   240  of  1,500.  Loss 0.0452  Elapsed: 0:00:02.319660.\n",
      "  Batch   280  of  1,500.  Loss 0.6389  Elapsed: 0:00:02.703981.\n",
      "  Batch   320  of  1,500.  Loss 0.4045  Elapsed: 0:00:03.089834.\n",
      "  Batch   360  of  1,500.  Loss 0.0897  Elapsed: 0:00:03.476810.\n",
      "  Batch   400  of  1,500.  Loss 0.0474  Elapsed: 0:00:03.861394.\n",
      "  Batch   440  of  1,500.  Loss 0.0295  Elapsed: 0:00:04.247410.\n",
      "  Batch   480  of  1,500.  Loss 0.5000  Elapsed: 0:00:04.633042.\n",
      "  Batch   520  of  1,500.  Loss 0.5150  Elapsed: 0:00:05.019353.\n",
      "  Batch   560  of  1,500.  Loss 0.0571  Elapsed: 0:00:05.405502.\n",
      "  Batch   600  of  1,500.  Loss 0.1903  Elapsed: 0:00:05.789946.\n",
      "  Batch   640  of  1,500.  Loss 0.2608  Elapsed: 0:00:06.175232.\n",
      "  Batch   680  of  1,500.  Loss 0.4893  Elapsed: 0:00:06.557344.\n",
      "  Batch   720  of  1,500.  Loss 0.5295  Elapsed: 0:00:06.940688.\n",
      "  Batch   760  of  1,500.  Loss 0.7281  Elapsed: 0:00:07.324166.\n",
      "  Batch   800  of  1,500.  Loss 0.3909  Elapsed: 0:00:07.711722.\n",
      "  Batch   840  of  1,500.  Loss 0.1195  Elapsed: 0:00:08.101776.\n",
      "  Batch   880  of  1,500.  Loss 0.0770  Elapsed: 0:00:08.489564.\n",
      "  Batch   920  of  1,500.  Loss 0.7756  Elapsed: 0:00:08.874928.\n",
      "  Batch   960  of  1,500.  Loss 0.0290  Elapsed: 0:00:09.260035.\n",
      "  Batch 1,000  of  1,500.  Loss 0.1053  Elapsed: 0:00:09.646490.\n",
      "  Batch 1,040  of  1,500.  Loss 0.0414  Elapsed: 0:00:10.030611.\n",
      "  Batch 1,080  of  1,500.  Loss 0.0251  Elapsed: 0:00:10.416267.\n",
      "  Batch 1,120  of  1,500.  Loss 0.2165  Elapsed: 0:00:10.801213.\n",
      "  Batch 1,160  of  1,500.  Loss 0.0436  Elapsed: 0:00:11.188253.\n",
      "  Batch 1,200  of  1,500.  Loss 0.0286  Elapsed: 0:00:11.573082.\n",
      "  Batch 1,240  of  1,500.  Loss 0.0962  Elapsed: 0:00:11.958118.\n",
      "  Batch 1,280  of  1,500.  Loss 0.6157  Elapsed: 0:00:12.342742.\n",
      "  Batch 1,320  of  1,500.  Loss 0.0408  Elapsed: 0:00:12.728109.\n",
      "  Batch 1,360  of  1,500.  Loss 0.0527  Elapsed: 0:00:13.112650.\n",
      "  Batch 1,400  of  1,500.  Loss 0.6548  Elapsed: 0:00:13.496078.\n",
      "  Batch 1,440  of  1,500.  Loss 0.6322  Elapsed: 0:00:13.880975.\n",
      "  Batch 1,480  of  1,500.  Loss 0.6727  Elapsed: 0:00:14.265261.\n",
      "Avg Validation Loss 0.2489, Completed in 0:00:14.448169 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.538963\n",
      "F1 Score (micro) =   0.636200\n",
      "F1 Score (macro) =   0.216740\n",
      "F1 Score (samples) =   0.588444\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.9176  Elapsed: 0:00:00.394151.\n",
      "  Batch    80  of  1,477.  Loss 0.0595  Elapsed: 0:00:00.779668.\n",
      "  Batch   120  of  1,477.  Loss 0.1436  Elapsed: 0:00:01.164651.\n",
      "  Batch   160  of  1,477.  Loss 0.0597  Elapsed: 0:00:01.550823.\n",
      "  Batch   200  of  1,477.  Loss 0.4123  Elapsed: 0:00:01.936968.\n",
      "  Batch   240  of  1,477.  Loss 0.0649  Elapsed: 0:00:02.320963.\n",
      "  Batch   280  of  1,477.  Loss 0.1875  Elapsed: 0:00:02.705897.\n",
      "  Batch   320  of  1,477.  Loss 0.5378  Elapsed: 0:00:03.095139.\n",
      "  Batch   360  of  1,477.  Loss 0.8513  Elapsed: 0:00:03.480073.\n",
      "  Batch   400  of  1,477.  Loss 0.4625  Elapsed: 0:00:03.866280.\n",
      "  Batch   440  of  1,477.  Loss 0.5439  Elapsed: 0:00:04.252449.\n",
      "  Batch   480  of  1,477.  Loss 0.6436  Elapsed: 0:00:04.635831.\n",
      "  Batch   520  of  1,477.  Loss 0.5384  Elapsed: 0:00:05.022084.\n",
      "  Batch   560  of  1,477.  Loss 0.5440  Elapsed: 0:00:05.406323.\n",
      "  Batch   600  of  1,477.  Loss 0.0236  Elapsed: 0:00:05.792809.\n",
      "  Batch   640  of  1,477.  Loss 0.8422  Elapsed: 0:00:06.180057.\n",
      "  Batch   680  of  1,477.  Loss 0.0432  Elapsed: 0:00:06.563318.\n",
      "  Batch   720  of  1,477.  Loss 0.2063  Elapsed: 0:00:06.947050.\n",
      "  Batch   760  of  1,477.  Loss 0.4882  Elapsed: 0:00:07.332851.\n",
      "  Batch   800  of  1,477.  Loss 0.1026  Elapsed: 0:00:07.719644.\n",
      "  Batch   840  of  1,477.  Loss 0.7581  Elapsed: 0:00:08.104313.\n",
      "  Batch   880  of  1,477.  Loss 0.0604  Elapsed: 0:00:08.492208.\n",
      "  Batch   920  of  1,477.  Loss 0.0428  Elapsed: 0:00:08.878972.\n",
      "  Batch   960  of  1,477.  Loss 0.0796  Elapsed: 0:00:09.264362.\n",
      "  Batch 1,000  of  1,477.  Loss 0.2118  Elapsed: 0:00:09.651040.\n",
      "  Batch 1,040  of  1,477.  Loss 0.1194  Elapsed: 0:00:10.034699.\n",
      "  Batch 1,080  of  1,477.  Loss 0.2385  Elapsed: 0:00:10.418919.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0485  Elapsed: 0:00:10.804196.\n",
      "  Batch 1,160  of  1,477.  Loss 0.8264  Elapsed: 0:00:11.187685.\n",
      "  Batch 1,200  of  1,477.  Loss 0.6518  Elapsed: 0:00:11.572227.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0380  Elapsed: 0:00:11.956167.\n",
      "  Batch 1,280  of  1,477.  Loss 0.1179  Elapsed: 0:00:12.342590.\n",
      "  Batch 1,320  of  1,477.  Loss 0.1130  Elapsed: 0:00:12.725744.\n",
      "  Batch 1,360  of  1,477.  Loss 0.4895  Elapsed: 0:00:13.110621.\n",
      "  Batch 1,400  of  1,477.  Loss 0.4933  Elapsed: 0:00:13.498089.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0765  Elapsed: 0:00:13.881661.\n",
      "Avg Validation Loss 0.2996, Completed in 0:00:14.230213 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.419732\n",
      "F1 Score (micro) =   0.551095\n",
      "F1 Score (macro) =   0.151622\n",
      "F1 Score (samples) =   0.510720\n",
      "Running Experiment GEMB_SEM_FR_0_5_2000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    125.  Loss 0.2663  Elapsed: 0:00:09.588453.\n",
      "Avg Training Loss 0.2627, Completed in 0:00:11.865793 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2612  Elapsed: 0:00:03.686139.\n",
      "  Batch    80  of     93.  Loss 0.3529  Elapsed: 0:00:07.289479.\n",
      "Avg Validation Loss 0.3140, Completed in 0:00:08.309170 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.406506\n",
      "F1 Score (micro) =   0.509789\n",
      "F1 Score (macro) =   0.199340\n",
      "F1 Score (samples) =   0.457007\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    125.  Loss 0.2994  Elapsed: 0:00:09.572852.\n",
      "Avg Training Loss 0.2487, Completed in 0:00:11.854034 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.4667  Elapsed: 0:00:03.694396.\n",
      "  Batch    80  of     93.  Loss 0.3351  Elapsed: 0:00:07.287170.\n",
      "Avg Validation Loss 0.3103, Completed in 0:00:08.302881 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.403331\n",
      "F1 Score (micro) =   0.510381\n",
      "F1 Score (macro) =   0.197310\n",
      "F1 Score (samples) =   0.455879\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    125.  Loss 0.3545  Elapsed: 0:00:09.575893.\n",
      "Avg Training Loss 0.2442, Completed in 0:00:11.849525 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2734  Elapsed: 0:00:03.685741.\n",
      "  Batch    80  of     93.  Loss 0.2692  Elapsed: 0:00:07.288096.\n",
      "Avg Validation Loss 0.3131, Completed in 0:00:08.301465 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.400656\n",
      "F1 Score (micro) =   0.518871\n",
      "F1 Score (macro) =   0.188866\n",
      "F1 Score (samples) =   0.477545\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    125.  Loss 0.1735  Elapsed: 0:00:09.586761.\n",
      "Avg Training Loss 0.2417, Completed in 0:00:11.862847 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3075  Elapsed: 0:00:03.688883.\n",
      "  Batch    80  of     93.  Loss 0.2868  Elapsed: 0:00:07.286639.\n",
      "Avg Validation Loss 0.3050, Completed in 0:00:08.333857 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.405993\n",
      "F1 Score (micro) =   0.508621\n",
      "F1 Score (macro) =   0.196988\n",
      "F1 Score (samples) =   0.438501\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  2,000.  Loss 0.1166  Elapsed: 0:00:00.396874.\n",
      "  Batch    80  of  2,000.  Loss 0.3521  Elapsed: 0:00:00.780818.\n",
      "  Batch   120  of  2,000.  Loss 0.5068  Elapsed: 0:00:01.165323.\n",
      "  Batch   160  of  2,000.  Loss 0.2102  Elapsed: 0:00:01.550046.\n",
      "  Batch   200  of  2,000.  Loss 0.1097  Elapsed: 0:00:01.936224.\n",
      "  Batch   240  of  2,000.  Loss 0.0626  Elapsed: 0:00:02.322653.\n",
      "  Batch   280  of  2,000.  Loss 0.5701  Elapsed: 0:00:02.708564.\n",
      "  Batch   320  of  2,000.  Loss 0.0690  Elapsed: 0:00:03.094971.\n",
      "  Batch   360  of  2,000.  Loss 0.5919  Elapsed: 0:00:03.479278.\n",
      "  Batch   400  of  2,000.  Loss 0.0622  Elapsed: 0:00:03.864556.\n",
      "  Batch   440  of  2,000.  Loss 0.1723  Elapsed: 0:00:04.251708.\n",
      "  Batch   480  of  2,000.  Loss 0.0969  Elapsed: 0:00:04.636235.\n",
      "  Batch   520  of  2,000.  Loss 0.0657  Elapsed: 0:00:05.022070.\n",
      "  Batch   560  of  2,000.  Loss 0.0256  Elapsed: 0:00:05.406281.\n",
      "  Batch   600  of  2,000.  Loss 0.7510  Elapsed: 0:00:05.790103.\n",
      "  Batch   640  of  2,000.  Loss 0.1595  Elapsed: 0:00:06.176402.\n",
      "  Batch   680  of  2,000.  Loss 0.1437  Elapsed: 0:00:06.560474.\n",
      "  Batch   720  of  2,000.  Loss 0.0421  Elapsed: 0:00:06.945467.\n",
      "  Batch   760  of  2,000.  Loss 0.0988  Elapsed: 0:00:07.331634.\n",
      "  Batch   800  of  2,000.  Loss 0.1599  Elapsed: 0:00:07.716340.\n",
      "  Batch   840  of  2,000.  Loss 0.1796  Elapsed: 0:00:08.101106.\n",
      "  Batch   880  of  2,000.  Loss 0.5666  Elapsed: 0:00:08.486312.\n",
      "  Batch   920  of  2,000.  Loss 0.1504  Elapsed: 0:00:08.872063.\n",
      "  Batch   960  of  2,000.  Loss 0.0727  Elapsed: 0:00:09.255645.\n",
      "  Batch 1,000  of  2,000.  Loss 0.5192  Elapsed: 0:00:09.642222.\n",
      "  Batch 1,040  of  2,000.  Loss 0.6037  Elapsed: 0:00:10.028662.\n",
      "  Batch 1,080  of  2,000.  Loss 0.0941  Elapsed: 0:00:10.413547.\n",
      "  Batch 1,120  of  2,000.  Loss 0.0718  Elapsed: 0:00:10.797994.\n",
      "  Batch 1,160  of  2,000.  Loss 0.1263  Elapsed: 0:00:11.182064.\n",
      "  Batch 1,200  of  2,000.  Loss 0.1114  Elapsed: 0:00:11.566588.\n",
      "  Batch 1,240  of  2,000.  Loss 0.1181  Elapsed: 0:00:11.952017.\n",
      "  Batch 1,280  of  2,000.  Loss 0.0738  Elapsed: 0:00:12.335611.\n",
      "  Batch 1,320  of  2,000.  Loss 0.5288  Elapsed: 0:00:12.721189.\n",
      "  Batch 1,360  of  2,000.  Loss 0.1537  Elapsed: 0:00:13.105537.\n",
      "  Batch 1,400  of  2,000.  Loss 0.0730  Elapsed: 0:00:13.490963.\n",
      "  Batch 1,440  of  2,000.  Loss 0.0743  Elapsed: 0:00:13.878065.\n",
      "  Batch 1,480  of  2,000.  Loss 0.0231  Elapsed: 0:00:14.264543.\n",
      "  Batch 1,520  of  2,000.  Loss 0.1494  Elapsed: 0:00:14.649039.\n",
      "  Batch 1,560  of  2,000.  Loss 0.4413  Elapsed: 0:00:15.033636.\n",
      "  Batch 1,600  of  2,000.  Loss 0.1909  Elapsed: 0:00:15.418888.\n",
      "  Batch 1,640  of  2,000.  Loss 0.1105  Elapsed: 0:00:15.803197.\n",
      "  Batch 1,680  of  2,000.  Loss 0.1162  Elapsed: 0:00:16.187354.\n",
      "  Batch 1,720  of  2,000.  Loss 0.0787  Elapsed: 0:00:16.570731.\n",
      "  Batch 1,760  of  2,000.  Loss 0.0819  Elapsed: 0:00:16.955730.\n",
      "  Batch 1,800  of  2,000.  Loss 0.1467  Elapsed: 0:00:17.342731.\n",
      "  Batch 1,840  of  2,000.  Loss 0.4509  Elapsed: 0:00:17.728698.\n",
      "  Batch 1,880  of  2,000.  Loss 0.0893  Elapsed: 0:00:18.115734.\n",
      "  Batch 1,920  of  2,000.  Loss 0.1484  Elapsed: 0:00:18.501976.\n",
      "  Batch 1,960  of  2,000.  Loss 0.1371  Elapsed: 0:00:18.886893.\n",
      "Avg Validation Loss 0.2368, Completed in 0:00:19.262773 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.568966\n",
      "F1 Score (micro) =   0.646811\n",
      "F1 Score (macro) =   0.251428\n",
      "F1 Score (samples) =   0.574333\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.1115  Elapsed: 0:00:00.396752.\n",
      "  Batch    80  of  1,477.  Loss 0.0929  Elapsed: 0:00:00.782871.\n",
      "  Batch   120  of  1,477.  Loss 0.3524  Elapsed: 0:00:01.168354.\n",
      "  Batch   160  of  1,477.  Loss 0.5831  Elapsed: 0:00:01.557050.\n",
      "  Batch   200  of  1,477.  Loss 0.1809  Elapsed: 0:00:01.943226.\n",
      "  Batch   240  of  1,477.  Loss 0.1246  Elapsed: 0:00:02.327778.\n",
      "  Batch   280  of  1,477.  Loss 0.5718  Elapsed: 0:00:02.715944.\n",
      "  Batch   320  of  1,477.  Loss 0.0622  Elapsed: 0:00:03.104575.\n",
      "  Batch   360  of  1,477.  Loss 0.1146  Elapsed: 0:00:03.491015.\n",
      "  Batch   400  of  1,477.  Loss 0.0308  Elapsed: 0:00:03.875324.\n",
      "  Batch   440  of  1,477.  Loss 0.1466  Elapsed: 0:00:04.261311.\n",
      "  Batch   480  of  1,477.  Loss 0.2597  Elapsed: 0:00:04.647994.\n",
      "  Batch   520  of  1,477.  Loss 0.5125  Elapsed: 0:00:05.034422.\n",
      "  Batch   560  of  1,477.  Loss 0.8388  Elapsed: 0:00:05.420444.\n",
      "  Batch   600  of  1,477.  Loss 0.1324  Elapsed: 0:00:05.803946.\n",
      "  Batch   640  of  1,477.  Loss 0.0769  Elapsed: 0:00:06.189032.\n",
      "  Batch   680  of  1,477.  Loss 0.5751  Elapsed: 0:00:06.575293.\n",
      "  Batch   720  of  1,477.  Loss 0.4607  Elapsed: 0:00:06.962187.\n",
      "  Batch   760  of  1,477.  Loss 0.0897  Elapsed: 0:00:07.347599.\n",
      "  Batch   800  of  1,477.  Loss 0.3921  Elapsed: 0:00:07.733864.\n",
      "  Batch   840  of  1,477.  Loss 0.0698  Elapsed: 0:00:08.117842.\n",
      "  Batch   880  of  1,477.  Loss 0.6209  Elapsed: 0:00:08.508302.\n",
      "  Batch   920  of  1,477.  Loss 0.1027  Elapsed: 0:00:08.894565.\n",
      "  Batch   960  of  1,477.  Loss 0.3985  Elapsed: 0:00:09.279093.\n",
      "  Batch 1,000  of  1,477.  Loss 0.1398  Elapsed: 0:00:09.666780.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0766  Elapsed: 0:00:10.053663.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0841  Elapsed: 0:00:10.440247.\n",
      "  Batch 1,120  of  1,477.  Loss 0.0723  Elapsed: 0:00:10.825489.\n",
      "  Batch 1,160  of  1,477.  Loss 0.5260  Elapsed: 0:00:11.210181.\n",
      "  Batch 1,200  of  1,477.  Loss 0.1489  Elapsed: 0:00:11.595610.\n",
      "  Batch 1,240  of  1,477.  Loss 0.3633  Elapsed: 0:00:11.978822.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0549  Elapsed: 0:00:12.361489.\n",
      "  Batch 1,320  of  1,477.  Loss 0.6611  Elapsed: 0:00:12.748358.\n",
      "  Batch 1,360  of  1,477.  Loss 0.2171  Elapsed: 0:00:13.132584.\n",
      "  Batch 1,400  of  1,477.  Loss 0.3607  Elapsed: 0:00:13.516405.\n",
      "  Batch 1,440  of  1,477.  Loss 0.7073  Elapsed: 0:00:13.900002.\n",
      "Avg Validation Loss 0.2916, Completed in 0:00:14.247510 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.433225\n",
      "F1 Score (micro) =   0.541441\n",
      "F1 Score (macro) =   0.174031\n",
      "F1 Score (samples) =   0.472805\n",
      "Running Experiment GEMB_SEM_FR_0_5_2500\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    157.  Loss 0.2923  Elapsed: 0:00:09.590078.\n",
      "Avg Training Loss 0.2656, Completed in 0:00:14.832796 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3695  Elapsed: 0:00:03.679349.\n",
      "  Batch    80  of     93.  Loss 0.2864  Elapsed: 0:00:07.277862.\n",
      "Avg Validation Loss 0.3238, Completed in 0:00:08.299542 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.382048\n",
      "F1 Score (micro) =   0.511289\n",
      "F1 Score (macro) =   0.165636\n",
      "F1 Score (samples) =   0.474385\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    157.  Loss 0.2137  Elapsed: 0:00:09.599068.\n",
      "Avg Training Loss 0.2526, Completed in 0:00:14.858438 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3923  Elapsed: 0:00:03.682828.\n",
      "  Batch    80  of     93.  Loss 0.2233  Elapsed: 0:00:07.280377.\n",
      "Avg Validation Loss 0.3172, Completed in 0:00:08.299170 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.396026\n",
      "F1 Score (micro) =   0.521550\n",
      "F1 Score (macro) =   0.179585\n",
      "F1 Score (samples) =   0.485669\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    157.  Loss 0.3144  Elapsed: 0:00:09.586854.\n",
      "Avg Training Loss 0.2474, Completed in 0:00:14.837836 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2663  Elapsed: 0:00:03.682205.\n",
      "  Batch    80  of     93.  Loss 0.4943  Elapsed: 0:00:07.285642.\n",
      "Avg Validation Loss 0.3094, Completed in 0:00:08.301057 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.391922\n",
      "F1 Score (micro) =   0.512012\n",
      "F1 Score (macro) =   0.176182\n",
      "F1 Score (samples) =   0.460844\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    157.  Loss 0.2397  Elapsed: 0:00:09.580362.\n",
      "Avg Training Loss 0.2462, Completed in 0:00:14.836770 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2817  Elapsed: 0:00:03.685502.\n",
      "  Batch    80  of     93.  Loss 0.3345  Elapsed: 0:00:07.282224.\n",
      "Avg Validation Loss 0.3064, Completed in 0:00:08.299644 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.396011\n",
      "F1 Score (micro) =   0.517744\n",
      "F1 Score (macro) =   0.181795\n",
      "F1 Score (samples) =   0.468743\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  2,500.  Loss 0.2092  Elapsed: 0:00:00.396621.\n",
      "  Batch    80  of  2,500.  Loss 1.0074  Elapsed: 0:00:00.783751.\n",
      "  Batch   120  of  2,500.  Loss 0.5325  Elapsed: 0:00:01.166725.\n",
      "  Batch   160  of  2,500.  Loss 0.1069  Elapsed: 0:00:01.551462.\n",
      "  Batch   200  of  2,500.  Loss 0.0086  Elapsed: 0:00:01.934756.\n",
      "  Batch   240  of  2,500.  Loss 0.0335  Elapsed: 0:00:02.321146.\n",
      "  Batch   280  of  2,500.  Loss 0.6052  Elapsed: 0:00:02.706381.\n",
      "  Batch   320  of  2,500.  Loss 0.4012  Elapsed: 0:00:03.093425.\n",
      "  Batch   360  of  2,500.  Loss 0.6695  Elapsed: 0:00:03.478677.\n",
      "  Batch   400  of  2,500.  Loss 0.0337  Elapsed: 0:00:03.865036.\n",
      "  Batch   440  of  2,500.  Loss 0.0517  Elapsed: 0:00:04.252140.\n",
      "  Batch   480  of  2,500.  Loss 0.7252  Elapsed: 0:00:04.635338.\n",
      "  Batch   520  of  2,500.  Loss 0.1758  Elapsed: 0:00:05.020403.\n",
      "  Batch   560  of  2,500.  Loss 0.0361  Elapsed: 0:00:05.405864.\n",
      "  Batch   600  of  2,500.  Loss 0.0695  Elapsed: 0:00:05.794807.\n",
      "  Batch   640  of  2,500.  Loss 0.6970  Elapsed: 0:00:06.179089.\n",
      "  Batch   680  of  2,500.  Loss 0.0394  Elapsed: 0:00:06.566917.\n",
      "  Batch   720  of  2,500.  Loss 0.5692  Elapsed: 0:00:06.952752.\n",
      "  Batch   760  of  2,500.  Loss 0.8755  Elapsed: 0:00:07.340261.\n",
      "  Batch   800  of  2,500.  Loss 0.5451  Elapsed: 0:00:07.728695.\n",
      "  Batch   840  of  2,500.  Loss 0.1066  Elapsed: 0:00:08.116446.\n",
      "  Batch   880  of  2,500.  Loss 0.0961  Elapsed: 0:00:08.501400.\n",
      "  Batch   920  of  2,500.  Loss 0.1291  Elapsed: 0:00:08.889660.\n",
      "  Batch   960  of  2,500.  Loss 0.9219  Elapsed: 0:00:09.274678.\n",
      "  Batch 1,000  of  2,500.  Loss 0.0781  Elapsed: 0:00:09.662331.\n",
      "  Batch 1,040  of  2,500.  Loss 0.0536  Elapsed: 0:00:10.047534.\n",
      "  Batch 1,080  of  2,500.  Loss 0.5042  Elapsed: 0:00:10.432996.\n",
      "  Batch 1,120  of  2,500.  Loss 0.4204  Elapsed: 0:00:10.818631.\n",
      "  Batch 1,160  of  2,500.  Loss 0.5888  Elapsed: 0:00:11.203488.\n",
      "  Batch 1,200  of  2,500.  Loss 0.0537  Elapsed: 0:00:11.588951.\n",
      "  Batch 1,240  of  2,500.  Loss 0.2804  Elapsed: 0:00:11.973534.\n",
      "  Batch 1,280  of  2,500.  Loss 0.2794  Elapsed: 0:00:12.358915.\n",
      "  Batch 1,320  of  2,500.  Loss 0.4126  Elapsed: 0:00:12.744808.\n",
      "  Batch 1,360  of  2,500.  Loss 0.3201  Elapsed: 0:00:13.130987.\n",
      "  Batch 1,400  of  2,500.  Loss 0.2419  Elapsed: 0:00:13.518291.\n",
      "  Batch 1,440  of  2,500.  Loss 0.5263  Elapsed: 0:00:13.904042.\n",
      "  Batch 1,480  of  2,500.  Loss 0.5200  Elapsed: 0:00:14.290879.\n",
      "  Batch 1,520  of  2,500.  Loss 0.1241  Elapsed: 0:00:14.678267.\n",
      "  Batch 1,560  of  2,500.  Loss 0.0751  Elapsed: 0:00:15.065342.\n",
      "  Batch 1,600  of  2,500.  Loss 1.0006  Elapsed: 0:00:15.450013.\n",
      "  Batch 1,640  of  2,500.  Loss 0.0303  Elapsed: 0:00:15.834778.\n",
      "  Batch 1,680  of  2,500.  Loss 0.0772  Elapsed: 0:00:16.223450.\n",
      "  Batch 1,720  of  2,500.  Loss 0.3445  Elapsed: 0:00:16.609230.\n",
      "  Batch 1,760  of  2,500.  Loss 0.1745  Elapsed: 0:00:16.996157.\n",
      "  Batch 1,800  of  2,500.  Loss 0.4938  Elapsed: 0:00:17.384057.\n",
      "  Batch 1,840  of  2,500.  Loss 0.7817  Elapsed: 0:00:17.768574.\n",
      "  Batch 1,880  of  2,500.  Loss 0.0159  Elapsed: 0:00:18.155967.\n",
      "  Batch 1,920  of  2,500.  Loss 0.1773  Elapsed: 0:00:18.542107.\n",
      "  Batch 1,960  of  2,500.  Loss 0.2506  Elapsed: 0:00:18.927412.\n",
      "  Batch 2,000  of  2,500.  Loss 0.1728  Elapsed: 0:00:19.313392.\n",
      "  Batch 2,040  of  2,500.  Loss 0.0493  Elapsed: 0:00:19.697275.\n",
      "  Batch 2,080  of  2,500.  Loss 0.1774  Elapsed: 0:00:20.081767.\n",
      "  Batch 2,120  of  2,500.  Loss 0.2700  Elapsed: 0:00:20.468271.\n",
      "  Batch 2,160  of  2,500.  Loss 0.2719  Elapsed: 0:00:20.853161.\n",
      "  Batch 2,200  of  2,500.  Loss 0.0383  Elapsed: 0:00:21.238634.\n",
      "  Batch 2,240  of  2,500.  Loss 0.3024  Elapsed: 0:00:21.624212.\n",
      "  Batch 2,280  of  2,500.  Loss 0.0572  Elapsed: 0:00:22.009731.\n",
      "  Batch 2,320  of  2,500.  Loss 0.1950  Elapsed: 0:00:22.394418.\n",
      "  Batch 2,360  of  2,500.  Loss 0.0919  Elapsed: 0:00:22.779326.\n",
      "  Batch 2,400  of  2,500.  Loss 0.0555  Elapsed: 0:00:23.165629.\n",
      "  Batch 2,440  of  2,500.  Loss 0.6592  Elapsed: 0:00:23.550842.\n",
      "  Batch 2,480  of  2,500.  Loss 0.7450  Elapsed: 0:00:23.934538.\n",
      "Avg Validation Loss 0.2410, Completed in 0:00:24.118473 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.553793\n",
      "F1 Score (micro) =   0.646879\n",
      "F1 Score (macro) =   0.224096\n",
      "F1 Score (samples) =   0.591733\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.1331  Elapsed: 0:00:00.400230.\n",
      "  Batch    80  of  1,477.  Loss 0.3692  Elapsed: 0:00:00.790181.\n",
      "  Batch   120  of  1,477.  Loss 0.0973  Elapsed: 0:00:01.180593.\n",
      "  Batch   160  of  1,477.  Loss 0.0288  Elapsed: 0:00:01.569334.\n",
      "  Batch   200  of  1,477.  Loss 0.0661  Elapsed: 0:00:01.958073.\n",
      "  Batch   240  of  1,477.  Loss 0.0503  Elapsed: 0:00:02.346493.\n",
      "  Batch   280  of  1,477.  Loss 0.1678  Elapsed: 0:00:02.732586.\n",
      "  Batch   320  of  1,477.  Loss 0.0406  Elapsed: 0:00:03.120322.\n",
      "  Batch   360  of  1,477.  Loss 0.3536  Elapsed: 0:00:03.507910.\n",
      "  Batch   400  of  1,477.  Loss 0.2871  Elapsed: 0:00:03.894671.\n",
      "  Batch   440  of  1,477.  Loss 0.6822  Elapsed: 0:00:04.284728.\n",
      "  Batch   480  of  1,477.  Loss 0.0575  Elapsed: 0:00:04.671132.\n",
      "  Batch   520  of  1,477.  Loss 0.6288  Elapsed: 0:00:05.056983.\n",
      "  Batch   560  of  1,477.  Loss 0.2182  Elapsed: 0:00:05.446372.\n",
      "  Batch   600  of  1,477.  Loss 0.8820  Elapsed: 0:00:05.834946.\n",
      "  Batch   640  of  1,477.  Loss 0.5541  Elapsed: 0:00:06.220498.\n",
      "  Batch   680  of  1,477.  Loss 0.0782  Elapsed: 0:00:06.610419.\n",
      "  Batch   720  of  1,477.  Loss 0.0637  Elapsed: 0:00:06.999057.\n",
      "  Batch   760  of  1,477.  Loss 0.1469  Elapsed: 0:00:07.388607.\n",
      "  Batch   800  of  1,477.  Loss 0.0482  Elapsed: 0:00:07.778355.\n",
      "  Batch   840  of  1,477.  Loss 0.5912  Elapsed: 0:00:08.167660.\n",
      "  Batch   880  of  1,477.  Loss 0.1288  Elapsed: 0:00:08.556193.\n",
      "  Batch   920  of  1,477.  Loss 0.0863  Elapsed: 0:00:08.943230.\n",
      "  Batch   960  of  1,477.  Loss 0.0452  Elapsed: 0:00:09.330716.\n",
      "  Batch 1,000  of  1,477.  Loss 0.1381  Elapsed: 0:00:09.719668.\n",
      "  Batch 1,040  of  1,477.  Loss 0.1454  Elapsed: 0:00:10.110024.\n",
      "  Batch 1,080  of  1,477.  Loss 0.1122  Elapsed: 0:00:10.496403.\n",
      "  Batch 1,120  of  1,477.  Loss 0.3895  Elapsed: 0:00:10.883707.\n",
      "  Batch 1,160  of  1,477.  Loss 0.1519  Elapsed: 0:00:11.275108.\n",
      "  Batch 1,200  of  1,477.  Loss 0.1338  Elapsed: 0:00:11.666191.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0408  Elapsed: 0:00:12.058674.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0977  Elapsed: 0:00:12.451382.\n",
      "  Batch 1,320  of  1,477.  Loss 0.0965  Elapsed: 0:00:12.842885.\n",
      "  Batch 1,360  of  1,477.  Loss 0.5869  Elapsed: 0:00:13.233622.\n",
      "  Batch 1,400  of  1,477.  Loss 0.1161  Elapsed: 0:00:13.624053.\n",
      "  Batch 1,440  of  1,477.  Loss 0.1231  Elapsed: 0:00:14.018476.\n",
      "Avg Validation Loss 0.2936, Completed in 0:00:14.373610 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.428405\n",
      "F1 Score (micro) =   0.551955\n",
      "F1 Score (macro) =   0.164483\n",
      "F1 Score (samples) =   0.501016\n",
      "Running Experiment GEMB_SEM_FR_0_5_3000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    188.  Loss 0.4178  Elapsed: 0:00:09.604349.\n",
      "Avg Training Loss 0.2660, Completed in 0:00:17.821101 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2394  Elapsed: 0:00:03.695686.\n",
      "  Batch    80  of     93.  Loss 0.4566  Elapsed: 0:00:07.291873.\n",
      "Avg Validation Loss 0.3175, Completed in 0:00:08.308986 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.391062\n",
      "F1 Score (micro) =   0.511697\n",
      "F1 Score (macro) =   0.176609\n",
      "F1 Score (samples) =   0.465132\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    188.  Loss 0.2269  Elapsed: 0:00:09.602536.\n",
      "Avg Training Loss 0.2539, Completed in 0:00:17.823112 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2801  Elapsed: 0:00:03.692067.\n",
      "  Batch    80  of     93.  Loss 0.3567  Elapsed: 0:00:07.298026.\n",
      "Avg Validation Loss 0.3089, Completed in 0:00:08.320002 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.394940\n",
      "F1 Score (micro) =   0.511086\n",
      "F1 Score (macro) =   0.180311\n",
      "F1 Score (samples) =   0.459264\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    188.  Loss 0.3460  Elapsed: 0:00:09.590103.\n",
      "Avg Training Loss 0.2511, Completed in 0:00:17.820415 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3470  Elapsed: 0:00:03.691503.\n",
      "  Batch    80  of     93.  Loss 0.3948  Elapsed: 0:00:07.295572.\n",
      "Avg Validation Loss 0.3093, Completed in 0:00:08.312222 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.388794\n",
      "F1 Score (micro) =   0.506464\n",
      "F1 Score (macro) =   0.176264\n",
      "F1 Score (samples) =   0.450463\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    188.  Loss 0.3274  Elapsed: 0:00:09.601336.\n",
      "Avg Training Loss 0.2465, Completed in 0:00:17.823014 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3125  Elapsed: 0:00:03.693582.\n",
      "  Batch    80  of     93.  Loss 0.3000  Elapsed: 0:00:07.289109.\n",
      "Avg Validation Loss 0.3079, Completed in 0:00:08.311624 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.408607\n",
      "F1 Score (micro) =   0.520466\n",
      "F1 Score (macro) =   0.188906\n",
      "F1 Score (samples) =   0.468969\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  3,000.  Loss 0.1108  Elapsed: 0:00:00.402375.\n",
      "  Batch    80  of  3,000.  Loss 0.1028  Elapsed: 0:00:00.792738.\n",
      "  Batch   120  of  3,000.  Loss 0.2024  Elapsed: 0:00:01.184276.\n",
      "  Batch   160  of  3,000.  Loss 0.5366  Elapsed: 0:00:01.574162.\n",
      "  Batch   200  of  3,000.  Loss 0.5993  Elapsed: 0:00:01.963723.\n",
      "  Batch   240  of  3,000.  Loss 0.2091  Elapsed: 0:00:02.351699.\n",
      "  Batch   280  of  3,000.  Loss 0.0471  Elapsed: 0:00:02.740646.\n",
      "  Batch   320  of  3,000.  Loss 0.0344  Elapsed: 0:00:03.131822.\n",
      "  Batch   360  of  3,000.  Loss 0.1723  Elapsed: 0:00:03.522056.\n",
      "  Batch   400  of  3,000.  Loss 0.1604  Elapsed: 0:00:03.911887.\n",
      "  Batch   440  of  3,000.  Loss 0.1480  Elapsed: 0:00:04.303208.\n",
      "  Batch   480  of  3,000.  Loss 0.0706  Elapsed: 0:00:04.692899.\n",
      "  Batch   520  of  3,000.  Loss 0.0910  Elapsed: 0:00:05.084004.\n",
      "  Batch   560  of  3,000.  Loss 0.2352  Elapsed: 0:00:05.477105.\n",
      "  Batch   600  of  3,000.  Loss 0.1999  Elapsed: 0:00:05.866829.\n",
      "  Batch   640  of  3,000.  Loss 0.2803  Elapsed: 0:00:06.257384.\n",
      "  Batch   680  of  3,000.  Loss 0.6035  Elapsed: 0:00:06.645405.\n",
      "  Batch   720  of  3,000.  Loss 0.5886  Elapsed: 0:00:07.035712.\n",
      "  Batch   760  of  3,000.  Loss 0.3996  Elapsed: 0:00:07.426213.\n",
      "  Batch   800  of  3,000.  Loss 0.5313  Elapsed: 0:00:07.816647.\n",
      "  Batch   840  of  3,000.  Loss 0.0540  Elapsed: 0:00:08.206849.\n",
      "  Batch   880  of  3,000.  Loss 0.5813  Elapsed: 0:00:08.596235.\n",
      "  Batch   920  of  3,000.  Loss 0.0977  Elapsed: 0:00:08.985231.\n",
      "  Batch   960  of  3,000.  Loss 0.2042  Elapsed: 0:00:09.377544.\n",
      "  Batch 1,000  of  3,000.  Loss 0.0291  Elapsed: 0:00:09.768959.\n",
      "  Batch 1,040  of  3,000.  Loss 0.3426  Elapsed: 0:00:10.160714.\n",
      "  Batch 1,080  of  3,000.  Loss 0.3965  Elapsed: 0:00:10.550083.\n",
      "  Batch 1,120  of  3,000.  Loss 0.3876  Elapsed: 0:00:10.939331.\n",
      "  Batch 1,160  of  3,000.  Loss 0.1284  Elapsed: 0:00:11.329663.\n",
      "  Batch 1,200  of  3,000.  Loss 0.0295  Elapsed: 0:00:11.725167.\n",
      "  Batch 1,240  of  3,000.  Loss 0.2313  Elapsed: 0:00:12.115559.\n",
      "  Batch 1,280  of  3,000.  Loss 0.5051  Elapsed: 0:00:12.511092.\n",
      "  Batch 1,320  of  3,000.  Loss 0.1637  Elapsed: 0:00:12.906868.\n",
      "  Batch 1,360  of  3,000.  Loss 0.0649  Elapsed: 0:00:13.298614.\n",
      "  Batch 1,400  of  3,000.  Loss 0.6463  Elapsed: 0:00:13.691083.\n",
      "  Batch 1,440  of  3,000.  Loss 0.1007  Elapsed: 0:00:14.081403.\n",
      "  Batch 1,480  of  3,000.  Loss 0.6564  Elapsed: 0:00:14.469763.\n",
      "  Batch 1,520  of  3,000.  Loss 0.3376  Elapsed: 0:00:14.860757.\n",
      "  Batch 1,560  of  3,000.  Loss 0.2003  Elapsed: 0:00:15.251543.\n",
      "  Batch 1,600  of  3,000.  Loss 0.9571  Elapsed: 0:00:15.641917.\n",
      "  Batch 1,640  of  3,000.  Loss 0.6600  Elapsed: 0:00:16.035527.\n",
      "  Batch 1,680  of  3,000.  Loss 0.1919  Elapsed: 0:00:16.426422.\n",
      "  Batch 1,720  of  3,000.  Loss 0.1816  Elapsed: 0:00:16.817382.\n",
      "  Batch 1,760  of  3,000.  Loss 0.5953  Elapsed: 0:00:17.206899.\n",
      "  Batch 1,800  of  3,000.  Loss 0.1391  Elapsed: 0:00:17.594594.\n",
      "  Batch 1,840  of  3,000.  Loss 0.0745  Elapsed: 0:00:17.983225.\n",
      "  Batch 1,880  of  3,000.  Loss 0.1232  Elapsed: 0:00:18.374657.\n",
      "  Batch 1,920  of  3,000.  Loss 0.2307  Elapsed: 0:00:18.764025.\n",
      "  Batch 1,960  of  3,000.  Loss 0.7917  Elapsed: 0:00:19.155365.\n",
      "  Batch 2,000  of  3,000.  Loss 0.1686  Elapsed: 0:00:19.545335.\n",
      "  Batch 2,040  of  3,000.  Loss 0.5149  Elapsed: 0:00:19.936188.\n",
      "  Batch 2,080  of  3,000.  Loss 0.0935  Elapsed: 0:00:20.325476.\n",
      "  Batch 2,120  of  3,000.  Loss 0.3106  Elapsed: 0:00:20.716069.\n",
      "  Batch 2,160  of  3,000.  Loss 0.2220  Elapsed: 0:00:21.105443.\n",
      "  Batch 2,200  of  3,000.  Loss 0.4129  Elapsed: 0:00:21.496656.\n",
      "  Batch 2,240  of  3,000.  Loss 0.1121  Elapsed: 0:00:21.887311.\n",
      "  Batch 2,280  of  3,000.  Loss 0.1232  Elapsed: 0:00:22.275686.\n",
      "  Batch 2,320  of  3,000.  Loss 0.0334  Elapsed: 0:00:22.665770.\n",
      "  Batch 2,360  of  3,000.  Loss 0.0798  Elapsed: 0:00:23.057545.\n",
      "  Batch 2,400  of  3,000.  Loss 0.6621  Elapsed: 0:00:23.447787.\n",
      "  Batch 2,440  of  3,000.  Loss 0.2806  Elapsed: 0:00:23.838987.\n",
      "  Batch 2,480  of  3,000.  Loss 0.0741  Elapsed: 0:00:24.229378.\n",
      "  Batch 2,520  of  3,000.  Loss 0.1817  Elapsed: 0:00:24.620901.\n",
      "  Batch 2,560  of  3,000.  Loss 0.0482  Elapsed: 0:00:25.013228.\n",
      "  Batch 2,600  of  3,000.  Loss 0.1090  Elapsed: 0:00:25.402988.\n",
      "  Batch 2,640  of  3,000.  Loss 0.0415  Elapsed: 0:00:25.794743.\n",
      "  Batch 2,680  of  3,000.  Loss 0.5827  Elapsed: 0:00:26.184328.\n",
      "  Batch 2,720  of  3,000.  Loss 0.3112  Elapsed: 0:00:26.577159.\n",
      "  Batch 2,760  of  3,000.  Loss 0.1043  Elapsed: 0:00:26.966372.\n",
      "  Batch 2,800  of  3,000.  Loss 0.2188  Elapsed: 0:00:27.354648.\n",
      "  Batch 2,840  of  3,000.  Loss 0.0555  Elapsed: 0:00:27.743746.\n",
      "  Batch 2,880  of  3,000.  Loss 0.1431  Elapsed: 0:00:28.133772.\n",
      "  Batch 2,920  of  3,000.  Loss 0.1712  Elapsed: 0:00:28.522495.\n",
      "  Batch 2,960  of  3,000.  Loss 0.1030  Elapsed: 0:00:28.912229.\n",
      "Avg Validation Loss 0.2433, Completed in 0:00:29.294128 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.552796\n",
      "F1 Score (micro) =   0.639138\n",
      "F1 Score (macro) =   0.215338\n",
      "F1 Score (samples) =   0.581778\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.2351  Elapsed: 0:00:00.409887.\n",
      "  Batch    80  of  1,477.  Loss 0.6858  Elapsed: 0:00:00.801937.\n",
      "  Batch   120  of  1,477.  Loss 0.1165  Elapsed: 0:00:01.192231.\n",
      "  Batch   160  of  1,477.  Loss 0.1052  Elapsed: 0:00:01.584118.\n",
      "  Batch   200  of  1,477.  Loss 0.1341  Elapsed: 0:00:01.976011.\n",
      "  Batch   240  of  1,477.  Loss 0.0343  Elapsed: 0:00:02.367492.\n",
      "  Batch   280  of  1,477.  Loss 0.5059  Elapsed: 0:00:02.757048.\n",
      "  Batch   320  of  1,477.  Loss 0.1484  Elapsed: 0:00:03.150671.\n",
      "  Batch   360  of  1,477.  Loss 0.2978  Elapsed: 0:00:03.538660.\n",
      "  Batch   400  of  1,477.  Loss 0.0939  Elapsed: 0:00:03.929084.\n",
      "  Batch   440  of  1,477.  Loss 0.5698  Elapsed: 0:00:04.319857.\n",
      "  Batch   480  of  1,477.  Loss 0.3311  Elapsed: 0:00:04.710707.\n",
      "  Batch   520  of  1,477.  Loss 0.0837  Elapsed: 0:00:05.100837.\n",
      "  Batch   560  of  1,477.  Loss 0.0418  Elapsed: 0:00:05.493011.\n",
      "  Batch   600  of  1,477.  Loss 0.4304  Elapsed: 0:00:05.884615.\n",
      "  Batch   640  of  1,477.  Loss 0.0475  Elapsed: 0:00:06.274765.\n",
      "  Batch   680  of  1,477.  Loss 0.1477  Elapsed: 0:00:06.663582.\n",
      "  Batch   720  of  1,477.  Loss 0.0914  Elapsed: 0:00:07.052115.\n",
      "  Batch   760  of  1,477.  Loss 0.1368  Elapsed: 0:00:07.442866.\n",
      "  Batch   800  of  1,477.  Loss 0.0170  Elapsed: 0:00:07.835406.\n",
      "  Batch   840  of  1,477.  Loss 0.0539  Elapsed: 0:00:08.226604.\n",
      "  Batch   880  of  1,477.  Loss 0.2543  Elapsed: 0:00:08.617921.\n",
      "  Batch   920  of  1,477.  Loss 0.3830  Elapsed: 0:00:09.009131.\n",
      "  Batch   960  of  1,477.  Loss 0.7582  Elapsed: 0:00:09.403210.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0322  Elapsed: 0:00:09.794600.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0370  Elapsed: 0:00:10.183435.\n",
      "  Batch 1,080  of  1,477.  Loss 0.1567  Elapsed: 0:00:10.576653.\n",
      "  Batch 1,120  of  1,477.  Loss 0.5827  Elapsed: 0:00:10.967791.\n",
      "  Batch 1,160  of  1,477.  Loss 0.3005  Elapsed: 0:00:11.360779.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0546  Elapsed: 0:00:11.750223.\n",
      "  Batch 1,240  of  1,477.  Loss 0.6168  Elapsed: 0:00:12.140638.\n",
      "  Batch 1,280  of  1,477.  Loss 0.1075  Elapsed: 0:00:12.532333.\n",
      "  Batch 1,320  of  1,477.  Loss 0.8455  Elapsed: 0:00:12.921432.\n",
      "  Batch 1,360  of  1,477.  Loss 0.5998  Elapsed: 0:00:13.311781.\n",
      "  Batch 1,400  of  1,477.  Loss 0.1898  Elapsed: 0:00:13.702765.\n",
      "  Batch 1,440  of  1,477.  Loss 0.0831  Elapsed: 0:00:14.094283.\n",
      "Avg Validation Loss 0.2940, Completed in 0:00:14.447664 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.436670\n",
      "F1 Score (micro) =   0.554352\n",
      "F1 Score (macro) =   0.169025\n",
      "F1 Score (samples) =   0.502144\n",
      "Running Experiment GEMB_SEM_FR_0_5_3500\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    219.  Loss 0.2584  Elapsed: 0:00:09.589090.\n",
      "Epoch  0  Batch   200  of    219.  Loss 0.1374  Elapsed: 0:00:19.088006.\n",
      "Avg Training Loss 0.2666, Completed in 0:00:20.779569 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3404  Elapsed: 0:00:03.694647.\n",
      "  Batch    80  of     93.  Loss 0.2830  Elapsed: 0:00:07.283682.\n",
      "Avg Validation Loss 0.3085, Completed in 0:00:08.305641 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.411056\n",
      "F1 Score (micro) =   0.507213\n",
      "F1 Score (macro) =   0.202261\n",
      "F1 Score (samples) =   0.451365\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    219.  Loss 0.2284  Elapsed: 0:00:09.590070.\n",
      "Epoch  1  Batch   200  of    219.  Loss 0.4535  Elapsed: 0:00:19.095516.\n",
      "Avg Training Loss 0.2532, Completed in 0:00:20.780656 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2465  Elapsed: 0:00:03.688330.\n",
      "  Batch    80  of     93.  Loss 0.3353  Elapsed: 0:00:07.286695.\n",
      "Avg Validation Loss 0.3084, Completed in 0:00:08.304322 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.407002\n",
      "F1 Score (micro) =   0.517319\n",
      "F1 Score (macro) =   0.190360\n",
      "F1 Score (samples) =   0.463552\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    219.  Loss 0.3277  Elapsed: 0:00:09.590552.\n",
      "Epoch  2  Batch   200  of    219.  Loss 0.3108  Elapsed: 0:00:19.106539.\n",
      "Avg Training Loss 0.2501, Completed in 0:00:20.795983 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2669  Elapsed: 0:00:03.688252.\n",
      "  Batch    80  of     93.  Loss 0.3063  Elapsed: 0:00:07.291871.\n",
      "Avg Validation Loss 0.3000, Completed in 0:00:08.313393 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.418332\n",
      "F1 Score (micro) =   0.518151\n",
      "F1 Score (macro) =   0.204785\n",
      "F1 Score (samples) =   0.458136\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    219.  Loss 0.1964  Elapsed: 0:00:09.608079.\n",
      "Epoch  3  Batch   200  of    219.  Loss 0.3159  Elapsed: 0:00:19.100755.\n",
      "Avg Training Loss 0.2473, Completed in 0:00:20.790820 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2066  Elapsed: 0:00:03.687046.\n",
      "  Batch    80  of     93.  Loss 0.2585  Elapsed: 0:00:07.287096.\n",
      "Avg Validation Loss 0.2982, Completed in 0:00:08.309572 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.410503\n",
      "F1 Score (micro) =   0.514397\n",
      "F1 Score (macro) =   0.193356\n",
      "F1 Score (samples) =   0.447077\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  3,500.  Loss 0.0248  Elapsed: 0:00:00.401147.\n",
      "  Batch    80  of  3,500.  Loss 0.0687  Elapsed: 0:00:00.791961.\n",
      "  Batch   120  of  3,500.  Loss 0.6067  Elapsed: 0:00:01.182688.\n",
      "  Batch   160  of  3,500.  Loss 0.1618  Elapsed: 0:00:01.575598.\n",
      "  Batch   200  of  3,500.  Loss 0.0723  Elapsed: 0:00:01.967030.\n",
      "  Batch   240  of  3,500.  Loss 0.1544  Elapsed: 0:00:02.358716.\n",
      "  Batch   280  of  3,500.  Loss 0.2245  Elapsed: 0:00:02.750131.\n",
      "  Batch   320  of  3,500.  Loss 0.4433  Elapsed: 0:00:03.142776.\n",
      "  Batch   360  of  3,500.  Loss 0.0864  Elapsed: 0:00:03.536948.\n",
      "  Batch   400  of  3,500.  Loss 0.2256  Elapsed: 0:00:03.925981.\n",
      "  Batch   440  of  3,500.  Loss 0.1057  Elapsed: 0:00:04.320251.\n",
      "  Batch   480  of  3,500.  Loss 0.2121  Elapsed: 0:00:04.711998.\n",
      "  Batch   520  of  3,500.  Loss 0.0433  Elapsed: 0:00:05.103714.\n",
      "  Batch   560  of  3,500.  Loss 0.2730  Elapsed: 0:00:05.492543.\n",
      "  Batch   600  of  3,500.  Loss 0.1154  Elapsed: 0:00:05.883642.\n",
      "  Batch   640  of  3,500.  Loss 0.8922  Elapsed: 0:00:06.274853.\n",
      "  Batch   680  of  3,500.  Loss 0.1229  Elapsed: 0:00:06.668489.\n",
      "  Batch   720  of  3,500.  Loss 0.6701  Elapsed: 0:00:07.057840.\n",
      "  Batch   760  of  3,500.  Loss 0.1110  Elapsed: 0:00:07.449712.\n",
      "  Batch   800  of  3,500.  Loss 0.1294  Elapsed: 0:00:07.838954.\n",
      "  Batch   840  of  3,500.  Loss 0.2511  Elapsed: 0:00:08.229095.\n",
      "  Batch   880  of  3,500.  Loss 0.9007  Elapsed: 0:00:08.619808.\n",
      "  Batch   920  of  3,500.  Loss 0.0309  Elapsed: 0:00:09.008619.\n",
      "  Batch   960  of  3,500.  Loss 0.1431  Elapsed: 0:00:09.397631.\n",
      "  Batch 1,000  of  3,500.  Loss 0.0981  Elapsed: 0:00:09.788332.\n",
      "  Batch 1,040  of  3,500.  Loss 0.8395  Elapsed: 0:00:10.178379.\n",
      "  Batch 1,080  of  3,500.  Loss 0.0547  Elapsed: 0:00:10.569302.\n",
      "  Batch 1,120  of  3,500.  Loss 0.7898  Elapsed: 0:00:10.962198.\n",
      "  Batch 1,160  of  3,500.  Loss 0.4370  Elapsed: 0:00:11.354589.\n",
      "  Batch 1,200  of  3,500.  Loss 0.1375  Elapsed: 0:00:11.746276.\n",
      "  Batch 1,240  of  3,500.  Loss 0.1525  Elapsed: 0:00:12.137505.\n",
      "  Batch 1,280  of  3,500.  Loss 0.0355  Elapsed: 0:00:12.528173.\n",
      "  Batch 1,320  of  3,500.  Loss 0.3261  Elapsed: 0:00:12.921664.\n",
      "  Batch 1,360  of  3,500.  Loss 0.0401  Elapsed: 0:00:13.314180.\n",
      "  Batch 1,400  of  3,500.  Loss 0.5922  Elapsed: 0:00:13.702257.\n",
      "  Batch 1,440  of  3,500.  Loss 0.4708  Elapsed: 0:00:14.093479.\n",
      "  Batch 1,480  of  3,500.  Loss 0.7210  Elapsed: 0:00:14.485579.\n",
      "  Batch 1,520  of  3,500.  Loss 0.0297  Elapsed: 0:00:14.877379.\n",
      "  Batch 1,560  of  3,500.  Loss 0.0550  Elapsed: 0:00:15.267549.\n",
      "  Batch 1,600  of  3,500.  Loss 0.0757  Elapsed: 0:00:15.658720.\n",
      "  Batch 1,640  of  3,500.  Loss 0.4089  Elapsed: 0:00:16.049206.\n",
      "  Batch 1,680  of  3,500.  Loss 0.7121  Elapsed: 0:00:16.439873.\n",
      "  Batch 1,720  of  3,500.  Loss 0.1331  Elapsed: 0:00:16.832231.\n",
      "  Batch 1,760  of  3,500.  Loss 0.3025  Elapsed: 0:00:17.226326.\n",
      "  Batch 1,800  of  3,500.  Loss 0.5899  Elapsed: 0:00:17.616596.\n",
      "  Batch 1,840  of  3,500.  Loss 0.1021  Elapsed: 0:00:18.009178.\n",
      "  Batch 1,880  of  3,500.  Loss 0.0340  Elapsed: 0:00:18.400873.\n",
      "  Batch 1,920  of  3,500.  Loss 0.0735  Elapsed: 0:00:18.792091.\n",
      "  Batch 1,960  of  3,500.  Loss 0.1749  Elapsed: 0:00:19.181375.\n",
      "  Batch 2,000  of  3,500.  Loss 0.1380  Elapsed: 0:00:19.572907.\n",
      "  Batch 2,040  of  3,500.  Loss 0.1521  Elapsed: 0:00:19.964033.\n",
      "  Batch 2,080  of  3,500.  Loss 0.4308  Elapsed: 0:00:20.352844.\n",
      "  Batch 2,120  of  3,500.  Loss 0.0752  Elapsed: 0:00:20.743631.\n",
      "  Batch 2,160  of  3,500.  Loss 0.1445  Elapsed: 0:00:21.133335.\n",
      "  Batch 2,200  of  3,500.  Loss 0.1387  Elapsed: 0:00:21.526448.\n",
      "  Batch 2,240  of  3,500.  Loss 0.0924  Elapsed: 0:00:21.918079.\n",
      "  Batch 2,280  of  3,500.  Loss 0.5936  Elapsed: 0:00:22.308978.\n",
      "  Batch 2,320  of  3,500.  Loss 0.6582  Elapsed: 0:00:22.702163.\n",
      "  Batch 2,360  of  3,500.  Loss 0.1131  Elapsed: 0:00:23.092964.\n",
      "  Batch 2,400  of  3,500.  Loss 0.6166  Elapsed: 0:00:23.486317.\n",
      "  Batch 2,440  of  3,500.  Loss 0.1290  Elapsed: 0:00:23.878645.\n",
      "  Batch 2,480  of  3,500.  Loss 0.1091  Elapsed: 0:00:24.270300.\n",
      "  Batch 2,520  of  3,500.  Loss 0.0522  Elapsed: 0:00:24.662878.\n",
      "  Batch 2,560  of  3,500.  Loss 0.7973  Elapsed: 0:00:25.055647.\n",
      "  Batch 2,600  of  3,500.  Loss 0.3662  Elapsed: 0:00:25.447752.\n",
      "  Batch 2,640  of  3,500.  Loss 0.0320  Elapsed: 0:00:25.838198.\n",
      "  Batch 2,680  of  3,500.  Loss 0.1138  Elapsed: 0:00:26.227526.\n",
      "  Batch 2,720  of  3,500.  Loss 0.5812  Elapsed: 0:00:26.617645.\n",
      "  Batch 2,760  of  3,500.  Loss 0.1381  Elapsed: 0:00:27.008079.\n",
      "  Batch 2,800  of  3,500.  Loss 0.1911  Elapsed: 0:00:27.401993.\n",
      "  Batch 2,840  of  3,500.  Loss 0.1822  Elapsed: 0:00:27.791827.\n",
      "  Batch 2,880  of  3,500.  Loss 0.3794  Elapsed: 0:00:28.181255.\n",
      "  Batch 2,920  of  3,500.  Loss 0.4040  Elapsed: 0:00:28.571172.\n",
      "  Batch 2,960  of  3,500.  Loss 0.1275  Elapsed: 0:00:28.961533.\n",
      "  Batch 3,000  of  3,500.  Loss 0.1473  Elapsed: 0:00:29.353850.\n",
      "  Batch 3,040  of  3,500.  Loss 0.0873  Elapsed: 0:00:29.744888.\n",
      "  Batch 3,080  of  3,500.  Loss 0.1576  Elapsed: 0:00:30.134811.\n",
      "  Batch 3,120  of  3,500.  Loss 0.1397  Elapsed: 0:00:30.525034.\n",
      "  Batch 3,160  of  3,500.  Loss 0.2518  Elapsed: 0:00:30.914366.\n",
      "  Batch 3,200  of  3,500.  Loss 0.1183  Elapsed: 0:00:31.303198.\n",
      "  Batch 3,240  of  3,500.  Loss 0.0946  Elapsed: 0:00:31.696659.\n",
      "  Batch 3,280  of  3,500.  Loss 0.0496  Elapsed: 0:00:32.089241.\n",
      "  Batch 3,320  of  3,500.  Loss 0.6769  Elapsed: 0:00:32.483419.\n",
      "  Batch 3,360  of  3,500.  Loss 0.4139  Elapsed: 0:00:32.874459.\n",
      "  Batch 3,400  of  3,500.  Loss 0.0804  Elapsed: 0:00:33.265960.\n",
      "  Batch 3,440  of  3,500.  Loss 0.1729  Elapsed: 0:00:33.656590.\n",
      "  Batch 3,480  of  3,500.  Loss 0.1003  Elapsed: 0:00:34.049194.\n",
      "Avg Validation Loss 0.2415, Completed in 0:00:34.234306 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.539369\n",
      "F1 Score (micro) =   0.625624\n",
      "F1 Score (macro) =   0.204082\n",
      "F1 Score (samples) =   0.553429\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.9566  Elapsed: 0:00:00.407598.\n",
      "  Batch    80  of  1,477.  Loss 0.7664  Elapsed: 0:00:00.799047.\n",
      "  Batch   120  of  1,477.  Loss 0.1028  Elapsed: 0:00:01.191677.\n",
      "  Batch   160  of  1,477.  Loss 0.0764  Elapsed: 0:00:01.584105.\n",
      "  Batch   200  of  1,477.  Loss 0.5145  Elapsed: 0:00:01.972971.\n",
      "  Batch   240  of  1,477.  Loss 0.0554  Elapsed: 0:00:02.363999.\n",
      "  Batch   280  of  1,477.  Loss 0.5043  Elapsed: 0:00:02.753242.\n",
      "  Batch   320  of  1,477.  Loss 0.0396  Elapsed: 0:00:03.144496.\n",
      "  Batch   360  of  1,477.  Loss 0.1714  Elapsed: 0:00:03.531321.\n",
      "  Batch   400  of  1,477.  Loss 0.1840  Elapsed: 0:00:03.921800.\n",
      "  Batch   440  of  1,477.  Loss 0.1465  Elapsed: 0:00:04.312701.\n",
      "  Batch   480  of  1,477.  Loss 0.5310  Elapsed: 0:00:04.702591.\n",
      "  Batch   520  of  1,477.  Loss 0.4278  Elapsed: 0:00:05.090489.\n",
      "  Batch   560  of  1,477.  Loss 0.2022  Elapsed: 0:00:05.480269.\n",
      "  Batch   600  of  1,477.  Loss 0.6340  Elapsed: 0:00:05.870902.\n",
      "  Batch   640  of  1,477.  Loss 0.5766  Elapsed: 0:00:06.262374.\n",
      "  Batch   680  of  1,477.  Loss 0.3227  Elapsed: 0:00:06.653598.\n",
      "  Batch   720  of  1,477.  Loss 0.3041  Elapsed: 0:00:07.042308.\n",
      "  Batch   760  of  1,477.  Loss 0.3221  Elapsed: 0:00:07.433460.\n",
      "  Batch   800  of  1,477.  Loss 0.0835  Elapsed: 0:00:07.824134.\n",
      "  Batch   840  of  1,477.  Loss 0.1094  Elapsed: 0:00:08.212845.\n",
      "  Batch   880  of  1,477.  Loss 0.1885  Elapsed: 0:00:08.603379.\n",
      "  Batch   920  of  1,477.  Loss 0.0840  Elapsed: 0:00:08.990694.\n",
      "  Batch   960  of  1,477.  Loss 0.0601  Elapsed: 0:00:09.379431.\n",
      "  Batch 1,000  of  1,477.  Loss 0.0377  Elapsed: 0:00:09.769045.\n",
      "  Batch 1,040  of  1,477.  Loss 0.2570  Elapsed: 0:00:10.158378.\n",
      "  Batch 1,080  of  1,477.  Loss 0.0869  Elapsed: 0:00:10.546938.\n",
      "  Batch 1,120  of  1,477.  Loss 0.2186  Elapsed: 0:00:10.937854.\n",
      "  Batch 1,160  of  1,477.  Loss 0.0984  Elapsed: 0:00:11.328805.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0829  Elapsed: 0:00:11.719726.\n",
      "  Batch 1,240  of  1,477.  Loss 0.2019  Elapsed: 0:00:12.109983.\n",
      "  Batch 1,280  of  1,477.  Loss 0.7766  Elapsed: 0:00:12.498331.\n",
      "  Batch 1,320  of  1,477.  Loss 0.7940  Elapsed: 0:00:12.888453.\n",
      "  Batch 1,360  of  1,477.  Loss 0.2798  Elapsed: 0:00:13.280015.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0731  Elapsed: 0:00:13.669109.\n",
      "  Batch 1,440  of  1,477.  Loss 0.1735  Elapsed: 0:00:14.061170.\n",
      "Avg Validation Loss 0.2865, Completed in 0:00:14.411108 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.436027\n",
      "F1 Score (micro) =   0.544402\n",
      "F1 Score (macro) =   0.171586\n",
      "F1 Score (samples) =   0.477093\n",
      "Running Experiment GEMB_SEM_FR_0_5_4000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    250.  Loss 0.1477  Elapsed: 0:00:09.603716.\n",
      "Epoch  0  Batch   200  of    250.  Loss 0.1936  Elapsed: 0:00:19.097946.\n",
      "Avg Training Loss 0.2622, Completed in 0:00:23.752194 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3216  Elapsed: 0:00:03.692430.\n",
      "  Batch    80  of     93.  Loss 0.3413  Elapsed: 0:00:07.298239.\n",
      "Avg Validation Loss 0.3072, Completed in 0:00:08.316919 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.395179\n",
      "F1 Score (micro) =   0.505352\n",
      "F1 Score (macro) =   0.184005\n",
      "F1 Score (samples) =   0.445949\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    250.  Loss 0.1695  Elapsed: 0:00:09.591072.\n",
      "Epoch  1  Batch   200  of    250.  Loss 0.2950  Elapsed: 0:00:19.097054.\n",
      "Avg Training Loss 0.2498, Completed in 0:00:23.744012 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3305  Elapsed: 0:00:03.687076.\n",
      "  Batch    80  of     93.  Loss 0.2354  Elapsed: 0:00:07.294340.\n",
      "Avg Validation Loss 0.3054, Completed in 0:00:08.309705 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.403616\n",
      "F1 Score (micro) =   0.513073\n",
      "F1 Score (macro) =   0.195217\n",
      "F1 Score (samples) =   0.457007\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    250.  Loss 0.2948  Elapsed: 0:00:09.597368.\n",
      "Epoch  2  Batch   200  of    250.  Loss 0.2194  Elapsed: 0:00:19.100611.\n",
      "Avg Training Loss 0.2459, Completed in 0:00:23.757621 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2585  Elapsed: 0:00:03.695580.\n",
      "  Batch    80  of     93.  Loss 0.2273  Elapsed: 0:00:07.297208.\n",
      "Avg Validation Loss 0.3098, Completed in 0:00:08.315316 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.411086\n",
      "F1 Score (micro) =   0.528524\n",
      "F1 Score (macro) =   0.190683\n",
      "F1 Score (samples) =   0.484766\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    250.  Loss 0.2077  Elapsed: 0:00:09.598136.\n",
      "Epoch  3  Batch   200  of    250.  Loss 0.1824  Elapsed: 0:00:19.096128.\n",
      "Avg Training Loss 0.2442, Completed in 0:00:23.747482 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.2910  Elapsed: 0:00:03.692252.\n",
      "  Batch    80  of     93.  Loss 0.2363  Elapsed: 0:00:07.292331.\n",
      "Avg Validation Loss 0.2970, Completed in 0:00:08.309912 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.417354\n",
      "F1 Score (micro) =   0.514914\n",
      "F1 Score (macro) =   0.203723\n",
      "F1 Score (samples) =   0.443466\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  4,000.  Loss 0.0403  Elapsed: 0:00:00.408540.\n",
      "  Batch    80  of  4,000.  Loss 0.4842  Elapsed: 0:00:00.799864.\n",
      "  Batch   120  of  4,000.  Loss 0.2096  Elapsed: 0:00:01.190868.\n",
      "  Batch   160  of  4,000.  Loss 0.0440  Elapsed: 0:00:01.584379.\n",
      "  Batch   200  of  4,000.  Loss 0.6442  Elapsed: 0:00:01.975324.\n",
      "  Batch   240  of  4,000.  Loss 0.6170  Elapsed: 0:00:02.366152.\n",
      "  Batch   280  of  4,000.  Loss 0.0784  Elapsed: 0:00:02.757871.\n",
      "  Batch   320  of  4,000.  Loss 0.4935  Elapsed: 0:00:03.153085.\n",
      "  Batch   360  of  4,000.  Loss 0.6939  Elapsed: 0:00:03.543124.\n",
      "  Batch   400  of  4,000.  Loss 0.0957  Elapsed: 0:00:03.933021.\n",
      "  Batch   440  of  4,000.  Loss 0.0670  Elapsed: 0:00:04.324099.\n",
      "  Batch   480  of  4,000.  Loss 0.0559  Elapsed: 0:00:04.713800.\n",
      "  Batch   520  of  4,000.  Loss 0.0779  Elapsed: 0:00:05.104525.\n",
      "  Batch   560  of  4,000.  Loss 0.1748  Elapsed: 0:00:05.495546.\n",
      "  Batch   600  of  4,000.  Loss 0.1072  Elapsed: 0:00:05.888945.\n",
      "  Batch   640  of  4,000.  Loss 0.2695  Elapsed: 0:00:06.279181.\n",
      "  Batch   680  of  4,000.  Loss 0.0648  Elapsed: 0:00:06.670702.\n",
      "  Batch   720  of  4,000.  Loss 0.4066  Elapsed: 0:00:07.058885.\n",
      "  Batch   760  of  4,000.  Loss 0.0586  Elapsed: 0:00:07.450519.\n",
      "  Batch   800  of  4,000.  Loss 0.0372  Elapsed: 0:00:07.839960.\n",
      "  Batch   840  of  4,000.  Loss 0.0827  Elapsed: 0:00:08.232237.\n",
      "  Batch   880  of  4,000.  Loss 0.1249  Elapsed: 0:00:08.622972.\n",
      "  Batch   920  of  4,000.  Loss 0.6375  Elapsed: 0:00:09.013743.\n",
      "  Batch   960  of  4,000.  Loss 0.0788  Elapsed: 0:00:09.404178.\n",
      "  Batch 1,000  of  4,000.  Loss 0.1248  Elapsed: 0:00:09.795022.\n",
      "  Batch 1,040  of  4,000.  Loss 0.0520  Elapsed: 0:00:10.182604.\n",
      "  Batch 1,080  of  4,000.  Loss 0.6137  Elapsed: 0:00:10.574228.\n",
      "  Batch 1,120  of  4,000.  Loss 0.1095  Elapsed: 0:00:10.965959.\n",
      "  Batch 1,160  of  4,000.  Loss 0.1463  Elapsed: 0:00:11.358071.\n",
      "  Batch 1,200  of  4,000.  Loss 0.1736  Elapsed: 0:00:11.749116.\n",
      "  Batch 1,240  of  4,000.  Loss 0.5503  Elapsed: 0:00:12.139804.\n",
      "  Batch 1,280  of  4,000.  Loss 0.1757  Elapsed: 0:00:12.531745.\n",
      "  Batch 1,320  of  4,000.  Loss 0.1177  Elapsed: 0:00:12.920433.\n",
      "  Batch 1,360  of  4,000.  Loss 0.3387  Elapsed: 0:00:13.311541.\n",
      "  Batch 1,400  of  4,000.  Loss 0.1956  Elapsed: 0:00:13.700930.\n",
      "  Batch 1,440  of  4,000.  Loss 0.1023  Elapsed: 0:00:14.091886.\n",
      "  Batch 1,480  of  4,000.  Loss 0.1915  Elapsed: 0:00:14.480855.\n",
      "  Batch 1,520  of  4,000.  Loss 0.1805  Elapsed: 0:00:14.872138.\n",
      "  Batch 1,560  of  4,000.  Loss 0.7063  Elapsed: 0:00:15.261844.\n",
      "  Batch 1,600  of  4,000.  Loss 0.3947  Elapsed: 0:00:15.653328.\n",
      "  Batch 1,640  of  4,000.  Loss 0.6856  Elapsed: 0:00:16.046082.\n",
      "  Batch 1,680  of  4,000.  Loss 0.5128  Elapsed: 0:00:16.434366.\n",
      "  Batch 1,720  of  4,000.  Loss 0.0576  Elapsed: 0:00:16.827145.\n",
      "  Batch 1,760  of  4,000.  Loss 0.0314  Elapsed: 0:00:17.217129.\n",
      "  Batch 1,800  of  4,000.  Loss 0.1250  Elapsed: 0:00:17.607614.\n",
      "  Batch 1,840  of  4,000.  Loss 0.1925  Elapsed: 0:00:18.000858.\n",
      "  Batch 1,880  of  4,000.  Loss 0.0673  Elapsed: 0:00:18.394808.\n",
      "  Batch 1,920  of  4,000.  Loss 0.0310  Elapsed: 0:00:18.786357.\n",
      "  Batch 1,960  of  4,000.  Loss 0.5917  Elapsed: 0:00:19.176505.\n",
      "  Batch 2,000  of  4,000.  Loss 0.3092  Elapsed: 0:00:19.568719.\n",
      "  Batch 2,040  of  4,000.  Loss 0.0322  Elapsed: 0:00:19.960487.\n",
      "  Batch 2,080  of  4,000.  Loss 0.0812  Elapsed: 0:00:20.351324.\n",
      "  Batch 2,120  of  4,000.  Loss 0.4030  Elapsed: 0:00:20.741820.\n",
      "  Batch 2,160  of  4,000.  Loss 0.3523  Elapsed: 0:00:21.131976.\n",
      "  Batch 2,200  of  4,000.  Loss 0.7176  Elapsed: 0:00:21.524499.\n",
      "  Batch 2,240  of  4,000.  Loss 0.1360  Elapsed: 0:00:21.916256.\n",
      "  Batch 2,280  of  4,000.  Loss 0.5305  Elapsed: 0:00:22.308425.\n",
      "  Batch 2,320  of  4,000.  Loss 0.2624  Elapsed: 0:00:22.698852.\n",
      "  Batch 2,360  of  4,000.  Loss 0.1656  Elapsed: 0:00:23.090459.\n",
      "  Batch 2,400  of  4,000.  Loss 0.1078  Elapsed: 0:00:23.482362.\n",
      "  Batch 2,440  of  4,000.  Loss 0.0462  Elapsed: 0:00:23.872306.\n",
      "  Batch 2,480  of  4,000.  Loss 0.0645  Elapsed: 0:00:24.264255.\n",
      "  Batch 2,520  of  4,000.  Loss 0.2508  Elapsed: 0:00:24.657471.\n",
      "  Batch 2,560  of  4,000.  Loss 0.1132  Elapsed: 0:00:25.048019.\n",
      "  Batch 2,600  of  4,000.  Loss 0.1052  Elapsed: 0:00:25.438999.\n",
      "  Batch 2,640  of  4,000.  Loss 0.1591  Elapsed: 0:00:25.831451.\n",
      "  Batch 2,680  of  4,000.  Loss 0.7461  Elapsed: 0:00:26.223998.\n",
      "  Batch 2,720  of  4,000.  Loss 0.0489  Elapsed: 0:00:26.614932.\n",
      "  Batch 2,760  of  4,000.  Loss 0.1486  Elapsed: 0:00:27.002779.\n",
      "  Batch 2,800  of  4,000.  Loss 0.0636  Elapsed: 0:00:27.395435.\n",
      "  Batch 2,840  of  4,000.  Loss 0.3512  Elapsed: 0:00:27.784965.\n",
      "  Batch 2,880  of  4,000.  Loss 0.1584  Elapsed: 0:00:28.174922.\n",
      "  Batch 2,920  of  4,000.  Loss 0.5320  Elapsed: 0:00:28.565357.\n",
      "  Batch 2,960  of  4,000.  Loss 0.0621  Elapsed: 0:00:28.957459.\n",
      "  Batch 3,000  of  4,000.  Loss 0.2863  Elapsed: 0:00:29.347255.\n",
      "  Batch 3,040  of  4,000.  Loss 0.5065  Elapsed: 0:00:29.738214.\n",
      "  Batch 3,080  of  4,000.  Loss 0.0853  Elapsed: 0:00:30.128616.\n",
      "  Batch 3,120  of  4,000.  Loss 0.1091  Elapsed: 0:00:30.518518.\n",
      "  Batch 3,160  of  4,000.  Loss 0.0334  Elapsed: 0:00:30.911378.\n",
      "  Batch 3,200  of  4,000.  Loss 0.2807  Elapsed: 0:00:31.302510.\n",
      "  Batch 3,240  of  4,000.  Loss 0.0376  Elapsed: 0:00:31.695683.\n",
      "  Batch 3,280  of  4,000.  Loss 0.0858  Elapsed: 0:00:32.086148.\n",
      "  Batch 3,320  of  4,000.  Loss 0.4995  Elapsed: 0:00:32.477900.\n",
      "  Batch 3,360  of  4,000.  Loss 0.0647  Elapsed: 0:00:32.869221.\n",
      "  Batch 3,400  of  4,000.  Loss 0.0989  Elapsed: 0:00:33.259428.\n",
      "  Batch 3,440  of  4,000.  Loss 0.0987  Elapsed: 0:00:33.652913.\n",
      "  Batch 3,480  of  4,000.  Loss 0.2337  Elapsed: 0:00:34.043996.\n",
      "  Batch 3,520  of  4,000.  Loss 0.2333  Elapsed: 0:00:34.436958.\n",
      "  Batch 3,560  of  4,000.  Loss 0.1435  Elapsed: 0:00:34.827873.\n",
      "  Batch 3,600  of  4,000.  Loss 0.1688  Elapsed: 0:00:35.220863.\n",
      "  Batch 3,640  of  4,000.  Loss 0.2801  Elapsed: 0:00:35.611588.\n",
      "  Batch 3,680  of  4,000.  Loss 0.1510  Elapsed: 0:00:36.002362.\n",
      "  Batch 3,720  of  4,000.  Loss 0.0522  Elapsed: 0:00:36.392287.\n",
      "  Batch 3,760  of  4,000.  Loss 0.1957  Elapsed: 0:00:36.786761.\n",
      "  Batch 3,800  of  4,000.  Loss 0.1307  Elapsed: 0:00:37.180053.\n",
      "  Batch 3,840  of  4,000.  Loss 0.1225  Elapsed: 0:00:37.569946.\n",
      "  Batch 3,880  of  4,000.  Loss 0.1620  Elapsed: 0:00:37.959669.\n",
      "  Batch 3,920  of  4,000.  Loss 0.0744  Elapsed: 0:00:38.349557.\n",
      "  Batch 3,960  of  4,000.  Loss 0.1063  Elapsed: 0:00:38.737442.\n",
      "Avg Validation Loss 0.2389, Completed in 0:00:39.118585 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.558599\n",
      "F1 Score (micro) =   0.638172\n",
      "F1 Score (macro) =   0.234349\n",
      "F1 Score (samples) =   0.564500\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.1541  Elapsed: 0:00:00.398970.\n",
      "  Batch    80  of  1,477.  Loss 0.8960  Elapsed: 0:00:00.790463.\n",
      "  Batch   120  of  1,477.  Loss 0.1170  Elapsed: 0:00:01.179013.\n",
      "  Batch   160  of  1,477.  Loss 0.1176  Elapsed: 0:00:01.569626.\n",
      "  Batch   200  of  1,477.  Loss 0.1370  Elapsed: 0:00:01.959206.\n",
      "  Batch   240  of  1,477.  Loss 0.5212  Elapsed: 0:00:02.348330.\n",
      "  Batch   280  of  1,477.  Loss 0.6702  Elapsed: 0:00:02.741169.\n",
      "  Batch   320  of  1,477.  Loss 0.3176  Elapsed: 0:00:03.130836.\n",
      "  Batch   360  of  1,477.  Loss 0.1265  Elapsed: 0:00:03.519711.\n",
      "  Batch   400  of  1,477.  Loss 0.8313  Elapsed: 0:00:03.911625.\n",
      "  Batch   440  of  1,477.  Loss 0.2676  Elapsed: 0:00:04.310669.\n",
      "  Batch   480  of  1,477.  Loss 0.8037  Elapsed: 0:00:04.700357.\n",
      "  Batch   520  of  1,477.  Loss 0.7974  Elapsed: 0:00:05.091048.\n",
      "  Batch   560  of  1,477.  Loss 0.2851  Elapsed: 0:00:05.483484.\n",
      "  Batch   600  of  1,477.  Loss 0.1978  Elapsed: 0:00:05.875781.\n",
      "  Batch   640  of  1,477.  Loss 0.1128  Elapsed: 0:00:06.265940.\n",
      "  Batch   680  of  1,477.  Loss 0.1468  Elapsed: 0:00:06.655683.\n",
      "  Batch   720  of  1,477.  Loss 0.0789  Elapsed: 0:00:07.046506.\n",
      "  Batch   760  of  1,477.  Loss 0.1271  Elapsed: 0:00:07.436378.\n",
      "  Batch   800  of  1,477.  Loss 0.0458  Elapsed: 0:00:07.827155.\n",
      "  Batch   840  of  1,477.  Loss 0.2895  Elapsed: 0:00:08.217363.\n",
      "  Batch   880  of  1,477.  Loss 0.1897  Elapsed: 0:00:08.608094.\n",
      "  Batch   920  of  1,477.  Loss 0.1827  Elapsed: 0:00:08.999962.\n",
      "  Batch   960  of  1,477.  Loss 0.5466  Elapsed: 0:00:09.392022.\n",
      "  Batch 1,000  of  1,477.  Loss 0.6067  Elapsed: 0:00:09.785656.\n",
      "  Batch 1,040  of  1,477.  Loss 0.0733  Elapsed: 0:00:10.175750.\n",
      "  Batch 1,080  of  1,477.  Loss 0.1485  Elapsed: 0:00:10.565967.\n",
      "  Batch 1,120  of  1,477.  Loss 0.5848  Elapsed: 0:00:10.955647.\n",
      "  Batch 1,160  of  1,477.  Loss 0.5233  Elapsed: 0:00:11.344913.\n",
      "  Batch 1,200  of  1,477.  Loss 0.1479  Elapsed: 0:00:11.735041.\n",
      "  Batch 1,240  of  1,477.  Loss 0.0730  Elapsed: 0:00:12.126105.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0791  Elapsed: 0:00:12.515098.\n",
      "  Batch 1,320  of  1,477.  Loss 0.1964  Elapsed: 0:00:12.908718.\n",
      "  Batch 1,360  of  1,477.  Loss 0.5778  Elapsed: 0:00:13.300410.\n",
      "  Batch 1,400  of  1,477.  Loss 0.6140  Elapsed: 0:00:13.690291.\n",
      "  Batch 1,440  of  1,477.  Loss 0.2327  Elapsed: 0:00:14.083399.\n",
      "Avg Validation Loss 0.2872, Completed in 0:00:14.434398 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.441800\n",
      "F1 Score (micro) =   0.543538\n",
      "F1 Score (macro) =   0.181490\n",
      "F1 Score (samples) =   0.471000\n",
      "Running Experiment GEMB_SEM_FR_0_5_4500\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    282.  Loss 0.4059  Elapsed: 0:00:09.597106.\n",
      "Epoch  0  Batch   200  of    282.  Loss 0.2743  Elapsed: 0:00:19.112257.\n",
      "Avg Training Loss 0.2672, Completed in 0:00:26.732303 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3704  Elapsed: 0:00:03.696788.\n",
      "  Batch    80  of     93.  Loss 0.3009  Elapsed: 0:00:07.298197.\n",
      "Avg Validation Loss 0.3088, Completed in 0:00:08.313132 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.405140\n",
      "F1 Score (micro) =   0.510816\n",
      "F1 Score (macro) =   0.194850\n",
      "F1 Score (samples) =   0.454525\n",
      "*****  Epoch 1 *****\n",
      "-----  Training -----\n",
      "Epoch  1  Batch   100  of    282.  Loss 0.1485  Elapsed: 0:00:09.602002.\n",
      "Epoch  1  Batch   200  of    282.  Loss 0.3025  Elapsed: 0:00:19.103336.\n",
      "Avg Training Loss 0.2529, Completed in 0:00:26.745274 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.4070  Elapsed: 0:00:03.696015.\n",
      "  Batch    80  of     93.  Loss 0.4334  Elapsed: 0:00:07.301390.\n",
      "Avg Validation Loss 0.3078, Completed in 0:00:08.317603 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.399282\n",
      "F1 Score (micro) =   0.518381\n",
      "F1 Score (macro) =   0.186028\n",
      "F1 Score (samples) =   0.471225\n",
      "*****  Epoch 2 *****\n",
      "-----  Training -----\n",
      "Epoch  2  Batch   100  of    282.  Loss 0.3256  Elapsed: 0:00:09.611229.\n",
      "Epoch  2  Batch   200  of    282.  Loss 0.2568  Elapsed: 0:00:19.098951.\n",
      "Avg Training Loss 0.2492, Completed in 0:00:26.728952 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.4167  Elapsed: 0:00:03.690109.\n",
      "  Batch    80  of     93.  Loss 0.2620  Elapsed: 0:00:07.288798.\n",
      "Avg Validation Loss 0.3031, Completed in 0:00:08.309560 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.419877\n",
      "F1 Score (micro) =   0.526596\n",
      "F1 Score (macro) =   0.199807\n",
      "F1 Score (samples) =   0.468292\n",
      "*****  Epoch 3 *****\n",
      "-----  Training -----\n",
      "Epoch  3  Batch   100  of    282.  Loss 0.2876  Elapsed: 0:00:09.593409.\n",
      "Epoch  3  Batch   200  of    282.  Loss 0.3072  Elapsed: 0:00:19.090565.\n",
      "Avg Training Loss 0.2482, Completed in 0:00:26.720550 \n",
      "-----  Validation -----\n",
      "**** Loss ******\n",
      "  Batch    40  of     93.  Loss 0.3357  Elapsed: 0:00:03.695153.\n",
      "  Batch    80  of     93.  Loss 0.2650  Elapsed: 0:00:07.292310.\n",
      "Avg Validation Loss 0.3010, Completed in 0:00:08.307400 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.421449\n",
      "F1 Score (micro) =   0.524027\n",
      "F1 Score (macro) =   0.205846\n",
      "F1 Score (samples) =   0.464004\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  4,500.  Loss 0.6881  Elapsed: 0:00:00.398549.\n",
      "  Batch    80  of  4,500.  Loss 0.0887  Elapsed: 0:00:00.786693.\n",
      "  Batch   120  of  4,500.  Loss 0.0520  Elapsed: 0:00:01.173715.\n",
      "  Batch   160  of  4,500.  Loss 0.6486  Elapsed: 0:00:01.561963.\n",
      "  Batch   200  of  4,500.  Loss 0.1676  Elapsed: 0:00:01.950220.\n",
      "  Batch   240  of  4,500.  Loss 0.0778  Elapsed: 0:00:02.336338.\n",
      "  Batch   280  of  4,500.  Loss 0.3321  Elapsed: 0:00:02.725333.\n",
      "  Batch   320  of  4,500.  Loss 0.0551  Elapsed: 0:00:03.114058.\n",
      "  Batch   360  of  4,500.  Loss 0.0814  Elapsed: 0:00:03.501980.\n",
      "  Batch   400  of  4,500.  Loss 0.3025  Elapsed: 0:00:03.887835.\n",
      "  Batch   440  of  4,500.  Loss 0.2406  Elapsed: 0:00:04.273945.\n",
      "  Batch   480  of  4,500.  Loss 0.1717  Elapsed: 0:00:04.662335.\n",
      "  Batch   520  of  4,500.  Loss 0.5519  Elapsed: 0:00:05.050320.\n",
      "  Batch   560  of  4,500.  Loss 0.1516  Elapsed: 0:00:05.437363.\n",
      "  Batch   600  of  4,500.  Loss 0.2000  Elapsed: 0:00:05.825955.\n",
      "  Batch   640  of  4,500.  Loss 0.1866  Elapsed: 0:00:06.213453.\n",
      "  Batch   680  of  4,500.  Loss 0.5412  Elapsed: 0:00:06.599895.\n",
      "  Batch   720  of  4,500.  Loss 0.4177  Elapsed: 0:00:06.987885.\n",
      "  Batch   760  of  4,500.  Loss 0.0461  Elapsed: 0:00:07.375480.\n",
      "  Batch   800  of  4,500.  Loss 0.0368  Elapsed: 0:00:07.761687.\n",
      "  Batch   840  of  4,500.  Loss 0.2930  Elapsed: 0:00:08.148005.\n",
      "  Batch   880  of  4,500.  Loss 0.4535  Elapsed: 0:00:08.533956.\n",
      "  Batch   920  of  4,500.  Loss 0.0637  Elapsed: 0:00:08.921061.\n",
      "  Batch   960  of  4,500.  Loss 0.8458  Elapsed: 0:00:09.306938.\n",
      "  Batch 1,000  of  4,500.  Loss 0.2705  Elapsed: 0:00:09.693621.\n",
      "  Batch 1,040  of  4,500.  Loss 0.3397  Elapsed: 0:00:10.081613.\n",
      "  Batch 1,080  of  4,500.  Loss 0.1329  Elapsed: 0:00:10.468727.\n",
      "  Batch 1,120  of  4,500.  Loss 0.0609  Elapsed: 0:00:10.856080.\n",
      "  Batch 1,160  of  4,500.  Loss 0.0947  Elapsed: 0:00:11.244619.\n",
      "  Batch 1,200  of  4,500.  Loss 0.1925  Elapsed: 0:00:11.633093.\n",
      "  Batch 1,240  of  4,500.  Loss 0.0683  Elapsed: 0:00:12.020043.\n",
      "  Batch 1,280  of  4,500.  Loss 0.4726  Elapsed: 0:00:12.408499.\n",
      "  Batch 1,320  of  4,500.  Loss 0.1023  Elapsed: 0:00:12.797697.\n",
      "  Batch 1,360  of  4,500.  Loss 0.0513  Elapsed: 0:00:13.184085.\n",
      "  Batch 1,400  of  4,500.  Loss 0.0758  Elapsed: 0:00:13.570851.\n",
      "  Batch 1,440  of  4,500.  Loss 0.0561  Elapsed: 0:00:13.958233.\n",
      "  Batch 1,480  of  4,500.  Loss 0.1115  Elapsed: 0:00:14.345865.\n",
      "  Batch 1,520  of  4,500.  Loss 0.1600  Elapsed: 0:00:14.733371.\n",
      "  Batch 1,560  of  4,500.  Loss 0.0761  Elapsed: 0:00:15.121562.\n",
      "  Batch 1,600  of  4,500.  Loss 0.1513  Elapsed: 0:00:15.507252.\n",
      "  Batch 1,640  of  4,500.  Loss 0.4765  Elapsed: 0:00:15.894687.\n",
      "  Batch 1,680  of  4,500.  Loss 0.1450  Elapsed: 0:00:16.283627.\n",
      "  Batch 1,720  of  4,500.  Loss 0.1059  Elapsed: 0:00:16.669295.\n",
      "  Batch 1,760  of  4,500.  Loss 0.0775  Elapsed: 0:00:17.055853.\n",
      "  Batch 1,800  of  4,500.  Loss 0.1976  Elapsed: 0:00:17.441720.\n",
      "  Batch 1,840  of  4,500.  Loss 0.2786  Elapsed: 0:00:17.826970.\n",
      "  Batch 1,880  of  4,500.  Loss 0.0437  Elapsed: 0:00:18.213954.\n",
      "  Batch 1,920  of  4,500.  Loss 0.0535  Elapsed: 0:00:18.601598.\n",
      "  Batch 1,960  of  4,500.  Loss 0.0935  Elapsed: 0:00:18.988673.\n",
      "  Batch 2,000  of  4,500.  Loss 0.0573  Elapsed: 0:00:19.376195.\n",
      "  Batch 2,040  of  4,500.  Loss 0.2132  Elapsed: 0:00:19.765314.\n",
      "  Batch 2,080  of  4,500.  Loss 0.6227  Elapsed: 0:00:20.151855.\n",
      "  Batch 2,120  of  4,500.  Loss 0.0562  Elapsed: 0:00:20.541718.\n",
      "  Batch 2,160  of  4,500.  Loss 0.0406  Elapsed: 0:00:20.930069.\n",
      "  Batch 2,200  of  4,500.  Loss 0.1361  Elapsed: 0:00:21.316943.\n",
      "  Batch 2,240  of  4,500.  Loss 0.0309  Elapsed: 0:00:21.704248.\n",
      "  Batch 2,280  of  4,500.  Loss 0.4544  Elapsed: 0:00:22.089958.\n",
      "  Batch 2,320  of  4,500.  Loss 0.7138  Elapsed: 0:00:22.476768.\n",
      "  Batch 2,360  of  4,500.  Loss 0.0543  Elapsed: 0:00:22.861901.\n",
      "  Batch 2,400  of  4,500.  Loss 0.3528  Elapsed: 0:00:23.249492.\n",
      "  Batch 2,440  of  4,500.  Loss 0.1199  Elapsed: 0:00:23.634526.\n",
      "  Batch 2,480  of  4,500.  Loss 0.1133  Elapsed: 0:00:24.020958.\n",
      "  Batch 2,520  of  4,500.  Loss 0.1332  Elapsed: 0:00:24.410211.\n",
      "  Batch 2,560  of  4,500.  Loss 0.3373  Elapsed: 0:00:24.797870.\n",
      "  Batch 2,600  of  4,500.  Loss 0.0642  Elapsed: 0:00:25.185361.\n",
      "  Batch 2,640  of  4,500.  Loss 0.0774  Elapsed: 0:00:25.573919.\n",
      "  Batch 2,680  of  4,500.  Loss 0.0982  Elapsed: 0:00:25.961088.\n",
      "  Batch 2,720  of  4,500.  Loss 0.0999  Elapsed: 0:00:26.347357.\n",
      "  Batch 2,760  of  4,500.  Loss 0.1318  Elapsed: 0:00:26.734986.\n",
      "  Batch 2,800  of  4,500.  Loss 0.0421  Elapsed: 0:00:27.121519.\n",
      "  Batch 2,840  of  4,500.  Loss 0.1586  Elapsed: 0:00:27.509830.\n",
      "  Batch 2,880  of  4,500.  Loss 0.0788  Elapsed: 0:00:27.896849.\n",
      "  Batch 2,920  of  4,500.  Loss 0.1407  Elapsed: 0:00:28.286356.\n",
      "  Batch 2,960  of  4,500.  Loss 0.1124  Elapsed: 0:00:28.673240.\n",
      "  Batch 3,000  of  4,500.  Loss 0.2416  Elapsed: 0:00:29.058861.\n",
      "  Batch 3,040  of  4,500.  Loss 0.2022  Elapsed: 0:00:29.446641.\n",
      "  Batch 3,080  of  4,500.  Loss 0.1040  Elapsed: 0:00:29.834744.\n",
      "  Batch 3,120  of  4,500.  Loss 0.1401  Elapsed: 0:00:30.222246.\n",
      "  Batch 3,160  of  4,500.  Loss 0.4887  Elapsed: 0:00:30.613604.\n",
      "  Batch 3,200  of  4,500.  Loss 0.3974  Elapsed: 0:00:31.001326.\n",
      "  Batch 3,240  of  4,500.  Loss 0.8234  Elapsed: 0:00:31.387159.\n",
      "  Batch 3,280  of  4,500.  Loss 0.0710  Elapsed: 0:00:31.776359.\n",
      "  Batch 3,320  of  4,500.  Loss 0.1751  Elapsed: 0:00:32.164794.\n",
      "  Batch 3,360  of  4,500.  Loss 0.0246  Elapsed: 0:00:32.551631.\n",
      "  Batch 3,400  of  4,500.  Loss 0.0411  Elapsed: 0:00:32.938187.\n",
      "  Batch 3,440  of  4,500.  Loss 0.1211  Elapsed: 0:00:33.325984.\n",
      "  Batch 3,480  of  4,500.  Loss 0.1031  Elapsed: 0:00:33.712494.\n",
      "  Batch 3,520  of  4,500.  Loss 0.1031  Elapsed: 0:00:34.099572.\n",
      "  Batch 3,560  of  4,500.  Loss 0.0695  Elapsed: 0:00:34.484757.\n",
      "  Batch 3,600  of  4,500.  Loss 0.1080  Elapsed: 0:00:34.873291.\n",
      "  Batch 3,640  of  4,500.  Loss 0.5340  Elapsed: 0:00:35.262721.\n",
      "  Batch 3,680  of  4,500.  Loss 0.1808  Elapsed: 0:00:35.653464.\n",
      "  Batch 3,720  of  4,500.  Loss 0.8484  Elapsed: 0:00:36.040937.\n",
      "  Batch 3,760  of  4,500.  Loss 0.0620  Elapsed: 0:00:36.427148.\n",
      "  Batch 3,800  of  4,500.  Loss 0.0707  Elapsed: 0:00:36.814443.\n",
      "  Batch 3,840  of  4,500.  Loss 0.0783  Elapsed: 0:00:37.201349.\n",
      "  Batch 3,880  of  4,500.  Loss 0.1151  Elapsed: 0:00:37.591208.\n",
      "  Batch 3,920  of  4,500.  Loss 0.0286  Elapsed: 0:00:37.978225.\n",
      "  Batch 3,960  of  4,500.  Loss 0.0876  Elapsed: 0:00:38.365503.\n",
      "  Batch 4,000  of  4,500.  Loss 0.1072  Elapsed: 0:00:38.754866.\n",
      "  Batch 4,040  of  4,500.  Loss 0.0911  Elapsed: 0:00:39.141641.\n",
      "  Batch 4,080  of  4,500.  Loss 0.1475  Elapsed: 0:00:39.529829.\n",
      "  Batch 4,120  of  4,500.  Loss 0.0489  Elapsed: 0:00:39.918386.\n",
      "  Batch 4,160  of  4,500.  Loss 0.3974  Elapsed: 0:00:40.305159.\n",
      "  Batch 4,200  of  4,500.  Loss 0.3066  Elapsed: 0:00:40.693650.\n",
      "  Batch 4,240  of  4,500.  Loss 0.5259  Elapsed: 0:00:41.082594.\n",
      "  Batch 4,280  of  4,500.  Loss 0.0266  Elapsed: 0:00:41.469526.\n",
      "  Batch 4,320  of  4,500.  Loss 0.0857  Elapsed: 0:00:41.857759.\n",
      "  Batch 4,360  of  4,500.  Loss 0.0384  Elapsed: 0:00:42.244536.\n",
      "  Batch 4,400  of  4,500.  Loss 0.0944  Elapsed: 0:00:42.629279.\n",
      "  Batch 4,440  of  4,500.  Loss 0.1316  Elapsed: 0:00:43.019297.\n",
      "  Batch 4,480  of  4,500.  Loss 0.1883  Elapsed: 0:00:43.410373.\n",
      "Avg Validation Loss 0.2409, Completed in 0:00:43.596093 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.561780\n",
      "F1 Score (micro) =   0.644006\n",
      "F1 Score (macro) =   0.237773\n",
      "F1 Score (samples) =   0.586296\n",
      "-----  Testing -----\n",
      "**** Loss ******\n",
      "  Batch    40  of  1,477.  Loss 0.3090  Elapsed: 0:00:00.401352.\n",
      "  Batch    80  of  1,477.  Loss 0.1205  Elapsed: 0:00:00.787957.\n",
      "  Batch   120  of  1,477.  Loss 0.0679  Elapsed: 0:00:01.173960.\n",
      "  Batch   160  of  1,477.  Loss 0.5058  Elapsed: 0:00:01.561882.\n",
      "  Batch   200  of  1,477.  Loss 0.4968  Elapsed: 0:00:01.947350.\n",
      "  Batch   240  of  1,477.  Loss 0.4031  Elapsed: 0:00:02.333984.\n",
      "  Batch   280  of  1,477.  Loss 0.1671  Elapsed: 0:00:02.720396.\n",
      "  Batch   320  of  1,477.  Loss 0.0870  Elapsed: 0:00:03.108308.\n",
      "  Batch   360  of  1,477.  Loss 0.0962  Elapsed: 0:00:03.493800.\n",
      "  Batch   400  of  1,477.  Loss 0.0853  Elapsed: 0:00:03.880584.\n",
      "  Batch   440  of  1,477.  Loss 0.8155  Elapsed: 0:00:04.267275.\n",
      "  Batch   480  of  1,477.  Loss 0.6289  Elapsed: 0:00:04.653389.\n",
      "  Batch   520  of  1,477.  Loss 0.0315  Elapsed: 0:00:05.039238.\n",
      "  Batch   560  of  1,477.  Loss 0.0413  Elapsed: 0:00:05.425241.\n",
      "  Batch   600  of  1,477.  Loss 0.4335  Elapsed: 0:00:05.810466.\n",
      "  Batch   640  of  1,477.  Loss 0.4042  Elapsed: 0:00:06.197525.\n",
      "  Batch   680  of  1,477.  Loss 0.5391  Elapsed: 0:00:06.585440.\n",
      "  Batch   720  of  1,477.  Loss 0.4797  Elapsed: 0:00:06.971538.\n",
      "  Batch   760  of  1,477.  Loss 0.0419  Elapsed: 0:00:07.359487.\n",
      "  Batch   800  of  1,477.  Loss 0.0955  Elapsed: 0:00:07.747628.\n",
      "  Batch   840  of  1,477.  Loss 0.7059  Elapsed: 0:00:08.134418.\n",
      "  Batch   880  of  1,477.  Loss 0.3501  Elapsed: 0:00:08.518657.\n",
      "  Batch   920  of  1,477.  Loss 0.7926  Elapsed: 0:00:08.905113.\n",
      "  Batch   960  of  1,477.  Loss 0.0480  Elapsed: 0:00:09.290393.\n",
      "  Batch 1,000  of  1,477.  Loss 0.5437  Elapsed: 0:00:09.678796.\n",
      "  Batch 1,040  of  1,477.  Loss 0.7161  Elapsed: 0:00:10.067603.\n",
      "  Batch 1,080  of  1,477.  Loss 0.1328  Elapsed: 0:00:10.457819.\n",
      "  Batch 1,120  of  1,477.  Loss 0.6286  Elapsed: 0:00:10.845715.\n",
      "  Batch 1,160  of  1,477.  Loss 0.4260  Elapsed: 0:00:11.233266.\n",
      "  Batch 1,200  of  1,477.  Loss 0.0851  Elapsed: 0:00:11.618730.\n",
      "  Batch 1,240  of  1,477.  Loss 0.9683  Elapsed: 0:00:12.005458.\n",
      "  Batch 1,280  of  1,477.  Loss 0.0284  Elapsed: 0:00:12.393514.\n",
      "  Batch 1,320  of  1,477.  Loss 0.3833  Elapsed: 0:00:12.779340.\n",
      "  Batch 1,360  of  1,477.  Loss 0.3678  Elapsed: 0:00:13.164473.\n",
      "  Batch 1,400  of  1,477.  Loss 0.0468  Elapsed: 0:00:13.554512.\n",
      "  Batch 1,440  of  1,477.  Loss 0.8360  Elapsed: 0:00:13.942672.\n",
      "Avg Validation Loss 0.2896, Completed in 0:00:14.291144 \n",
      "**** Results ******\n",
      "F1 Score (weighted) =   0.450052\n",
      "F1 Score (micro) =   0.556105\n",
      "F1 Score (macro) =   0.186508\n",
      "F1 Score (samples) =   0.494245\n",
      "Running Experiment GEMB_SEM_FR_0_5_5000\n",
      "*****  Epoch 0 *****\n",
      "-----  Training -----\n",
      "Epoch  0  Batch   100  of    313.  Loss 0.2446  Elapsed: 0:00:09.594379.\n",
      "Epoch  0  Batch   200  of    313.  Loss 0.2479  Elapsed: 0:00:19.087749.\n"
     ]
    }
   ],
   "source": [
    "run_stats = []\n",
    "\n",
    "if len(errors) == 0:\n",
    "    for e,c in all_exp.items():\n",
    "        ec = {}\n",
    "        ec.update({'experiment':e})        \n",
    "        ec.update(model_config)\n",
    "        ec.update(c)\n",
    "    \n",
    "        print ('Running Experiment',e)\n",
    "        model = None\n",
    "        train_stats = []\n",
    "        if ec.get('load_model',None):\n",
    "            model_name = ec['load_model']+metadata['model_ext']\n",
    "            out_model = ec['model_path'] + '/' + model_name\n",
    "            in_model = ec['input_model_path'] + '/' + model_name\n",
    "            if os.path.exists(out_model):\n",
    "                model = torch.load(out_model,map_location=device)\n",
    "            elif os.path.exists(in_model):\n",
    "                model = torch.load(in_model,map_location=device)\n",
    "            else:\n",
    "                model = torch.load(ec['dataset_path'] + '/' + model_name,map_location=device)\n",
    "        trained_model = model \n",
    "        if ec.get('train_corpus',None):\n",
    "            train_data = load_corpusdata(ec['dataset_path'],\n",
    "                                            corpus=ec['train_corpus'],\n",
    "                                            samples=ec.get('train_samples',None),\n",
    "                                            type='train')\n",
    "            dev_data = load_corpusdata(ec['dataset_path'],\n",
    "                                        corpus=ec['train_corpus'],\n",
    "                                        samples=ec.get('train_samples',None),\n",
    "                                        type='dev')\n",
    "            trained_model, train_stats = train_model(train_data,dev_data,model,\n",
    "                                    freeze_layers=ec.get('freeze_layers',None),\n",
    "                                        epochs=ec['epochs'],\n",
    "                                        batch_size=ec['batch_size'],\n",
    "                                        drop_out=ec['drop_out'],\n",
    "                                        lr=ec['lr'],\n",
    "                                        max_length=ec['max_length'])\n",
    "            train_predict_results = test_model(train_data,trained_model,epoch='tr_predict',\n",
    "                                        max_length=ec['max_length'])\n",
    "            train_stats.append(train_predict_results)\n",
    "\n",
    "        test_data = load_corpusdata(ec['dataset_path'],\n",
    "                                    corpus=ec['test_corpus'],\n",
    "                                    samples=ec.get('test_samples',None),\n",
    "                                    type='test')\n",
    "        test_results = test_model(test_data,trained_model,epoch='test',\n",
    "                                    max_length=ec['max_length'])\n",
    "        train_stats.append(test_results)\n",
    "        for d in train_stats:\n",
    "            d.update(ec)\n",
    "        if ec.get('save_model',None):\n",
    "            #https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch\n",
    "            torch.save(trained_model.state_dict(),\n",
    "                ec['model_path'] + '/' + ec['save_model']+metadata['dict_ext'])\n",
    "            torch.save(trained_model,\n",
    "                ec['model_path'] + '/' + ec['save_model']+metadata['model_ext'])\n",
    "        if ec.get('save_stats',None):\n",
    "            pdt = pd.DataFrame(train_stats)\n",
    "            pdt.to_csv(ec['output_path'] + '/' + ec['save_stats'] + '.csv')\n",
    "\n",
    "        run_stats.extend(train_stats)  \n",
    "\n",
    "    pd_results = pd.DataFrame(run_stats)\n",
    "    pd_results.to_csv(\n",
    "        os.path.join(OUTPUT_PATH,\n",
    "                    'results_{}_{}.csv'.format(metadata['run_type'],time.strftime('%Y%m%d_%H%M%S'))))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_basemodel(7,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    print (name,param.size(),'transformer.layer.0' in name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_layers = ['embeddings','transformer']\n",
    "for name,param in model.named_parameters():\n",
    "    if any([fl in name for fl in freeze_layers]):\n",
    "        param.requires_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
